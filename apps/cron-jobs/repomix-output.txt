This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-08T02:45:20.196Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
scripts/
  barrel.generator.ts
src/
  agents/
    clients/
      database.client.ts
      groq.client.ts
      index.ts
      openai.client.ts
    configs/
      config.ts
      summarizer.config.ts
    types/
      clients.ts
      index.ts
    utils/
      templating.ts
      validation.ts
    agent.ts
    index.ts
    manager.ts
  core/
    extractors/
      extractAuthor.ts
      extractDate.ts
      extractFeaturedImage.ts
      extractKeywords.ts
      extractor.module.ts
      extractTitle.ts
      index.ts
    services/
      circuit-breaker.service.ts
      event.service.ts
      logger.service.ts
      metrics.service.ts
      prisma.service.ts
      queue.service.ts
      scraper.service.ts
      workflow.service.ts
    index.ts
    shutdown.service.ts
  helpers/
    files.helpers.ts
    hashing.helpers.ts
    index.ts
    logging.helpers.ts
  jobs/
    config/
      news/
        news-links.config.ts
        news-page.scraper.ts
        news-pages.config.ts
        news-summary.config.ts
        news.module.ts
      test/
        test.config.ts
        test.module.ts
    utils/
      data/
        models/
          urlClassifier/
            v1/
              model.json
              training_history.json
        featuresConfig.json
        trainingData.json
      features/
        feature.factory.ts
        feature.pipeline.ts
        features.ts
      content-stash.ts
      database.utils.ts
      image-processor.ts
      link-extractor.ts
      schedule.utils.ts
      tlds.ts
      url-classifier.ts
      url-formatter.ts
      url-validator.ts
    job.events.ts
    job.factory.ts
    job.registry.ts
    job.runner.ts
    job.versioning.ts
  types/
    breaker.types.ts
    domain.types.ts
    index.ts
    job.types.ts
    logger.types.ts
    metric.types.ts
    module.types.ts
    news.types.ts
    queue.types.ts
    schedule.types.ts
    workflow.types.ts
  app.ts
  config.ts
  index.ts
.dockerignore
docker-compose.yml
Dockerfile
eslint.config.js
package.json
project.json
tsconfig.json
vite.config.ts

================================================================
Repository Files
================================================================

================
File: scripts/barrel.generator.ts
================
// scripts/barrel.generator.ts
import * as fs from 'fs'
import * as path from 'path'
import { fileURLToPath } from 'url'
import { CustomLogger } from '../src/core/services/logger.service'

interface BarrelConfig {
  // Root directory to start scanning from
  rootDir: string
  // Array of directory names to generate barrels for
  directories: string[]
  // Optional: File extensions to include (defaults to .ts and .tsx)
  extensions?: string[]
  // Optional: Files to ignore (defaults to .spec.ts, .test.ts, etc)
  ignorePatterns?: string[]
  // Optional: Whether to generate nested barrel files (defaults to true)
  generateNestedBarrels?: boolean

  excludeDirs?: string[]
}

export class BarrelGenerator {
  private readonly logger: CustomLogger
  private readonly defaultExtensions = ['.ts', '.tsx']
  private readonly defaultIgnorePatterns = [
    '.module',
    '.module.ts',
    '.spec.ts',
    '.test.ts',
    '.e2e-spec.ts',
    '.d.ts',
    'index.ts',
  ]

  private readonly defaultExcludeDirs = ['__tests__']

  constructor(logger: CustomLogger) {
    this.logger = logger
    this.logger.setDomain('jobs')
  }

  private shouldExcludeDir(dirPath: string, excludeDirs: string[]): boolean {
    return excludeDirs.some(
      (excludeDir) => dirPath.includes(`/${excludeDir}/`) || dirPath.endsWith(`/${excludeDir}`),
    )
  }

  async generateBarrels(config: BarrelConfig): Promise<void> {
    const {
      rootDir,
      directories,
      extensions = this.defaultExtensions,
      ignorePatterns = this.defaultIgnorePatterns,
      generateNestedBarrels = true,
    } = config

    this.logger.info(`Starting barrel file generation for directories: ${directories.join(', ')}`)

    for (const dir of directories) {
      const fullPath = path.join(rootDir, dir)

      if (!fs.existsSync(fullPath)) {
        this.logger.warn(`Directory not found: ${fullPath}`)
        continue
      }

      await this.processDirectory(fullPath, {
        extensions,
        ignorePatterns,
        generateNestedBarrels,
      })
    }

    this.logger.info('Barrel file generation completed')
  }

  private async processDirectory(
    dirPath: string,
    options: Pick<BarrelConfig, 'extensions' | 'ignorePatterns' | 'generateNestedBarrels'>,
    isRootDir = true, // Add flag to identify root level directories
  ): Promise<string[]> {
    // Return exports to collect from nested dirs
    try {
      const files = await fs.promises.readdir(dirPath, { withFileTypes: true })
      const exports: string[] = []
      const rootDir = isRootDir ? dirPath : path.dirname(dirPath)
      const excludeDirs = options.ignorePatterns || this.defaultExcludeDirs

      // Skip this directory if it's in the exclude list
      if (!isRootDir && this.shouldExcludeDir(dirPath, excludeDirs)) {
        return exports
      }

      // Process all files in current directory
      const validFiles = files
        .filter((file) => file.isFile())
        .map((file) => file.name)
        .filter((filename) => {
          const isValidExtension = options.extensions?.some((ext) => filename.endsWith(ext))
          const shouldInclude = !options.ignorePatterns?.some((pattern) =>
            filename.includes(pattern),
          )
          return isValidExtension && shouldInclude
        })

      // Generate exports for valid files
      validFiles.forEach((filename) => {
        const relativePath = path.relative(rootDir, path.join(dirPath, filename))
        exports.push(this.generateExportForFile(relativePath))
      })

      // Process subdirectories
      const subdirs = files.filter((file) => file.isDirectory())
      for (const subdir of subdirs) {
        const subdirPath = path.join(dirPath, subdir.name)
        // Skip excluded directories
        if (!this.shouldExcludeDir(subdirPath, excludeDirs)) {
          const nestedExports = await this.processDirectory(subdirPath, options, false)
          exports.push(...nestedExports)
        }
      }

      // Only write barrel file if this is a root directory
      if (isRootDir && exports.length > 0) {
        await this.writeBarrelFile(dirPath, exports)
      }

      return exports
    } catch (error: any) {
      this.logger.error(`Error processing directory ${dirPath}:`, error.stack)
      throw error
    }
  }

  private generateExportForFile(filename: string): string {
    // Remove .ts extension if it exists
    const withoutExt = filename.replace(/\.ts$/, '')
    return `export * from './${withoutExt}';`
  }

  private generateBarrelContent(exports: string[]): string {
    // Remove any duplicate exports
    const uniqueExports = [...new Set(exports)]

    return `// Auto-generated barrel file
  ${uniqueExports.join('\n')}
  `
  }

  private async writeBarrelFile(dirPath: string, exports: string[]): Promise<void> {
    const barrelPath = path.join(dirPath, 'index.ts')
    const content = this.generateBarrelContent(exports)

    try {
      await fs.promises.writeFile(barrelPath, content)
      this.logger.debug(`Generated barrel file: ${barrelPath}`)
    } catch (error: any) {
      this.logger.error(`Error writing barrel file ${barrelPath}:`, error.stack)
      throw error
    }
  }
}

const __dirname = path.dirname(fileURLToPath(import.meta.url))

// Example usage script
// scripts/barrel.generator.ts
export async function generateProjectBarrels() {
  const logger = new CustomLogger('BarrelGenerator')
  const generator = new BarrelGenerator(logger)

  await generator.generateBarrels({
    rootDir: path.resolve(__dirname, '../src'),
    directories: ['core', 'content', 'types', 'monitoring'],
    extensions: ['.ts', '.tsx'],
    ignorePatterns: [
      // NestJS Critical Files
      '.controller.ts', // Controllers
      '.service.ts', // Services
      '.module.ts', // Modules
      '.guard.ts', // Guards
      '.middleware.ts', // Middleware
      '.filter.ts', // Filters
      '.interceptor.ts', // Interceptors
      '.decorator.ts', // Custom decorators
      '.provider.ts', // Custom providers
      '.resolver.ts', // GraphQL resolvers

      // Test Files
      '.spec.ts',
      '.test.ts',
      '.e2e-spec.ts',

      // Other Common Ignores
      '.d.ts',
      'index.ts',
      '.mock.ts',
      '__tests__',
    ],
    generateNestedBarrels: false,
    excludeDirs: ['__tests__', 'dist', 'node_modules'],
  })
}

// If running directly
if (import.meta.url.endsWith(process.argv[1])) {
  generateProjectBarrels().catch((error: any) => {
    console.error('Error generating barrel files:', error)
    process.exit(1)
  })
}

================
File: src/agents/clients/database.client.ts
================
// clients/database.client.ts
import Pool from 'pg-pool';
import { defu } from 'defu';
import {
  DatabaseClient,
  DatabaseConfig,
  MergedConfig,
  NewsArticle,
} from '../types';

export const DEFAULT_DB_CONFIG: DatabaseConfig = {
  pool: {
    max: 20,
    idleTimeoutMillis: 30000,
    connectionTimeoutMillis: 2000,
  },
  ssl: process.env.NODE_ENV === 'production',
};

export class DatabaseClientImpl implements DatabaseClient {
  private pool: Pool;
  private config: MergedConfig<DatabaseConfig, typeof DEFAULT_DB_CONFIG>;

  constructor(config: Partial<DatabaseConfig> = {}) {
    this.config = defu(config, DEFAULT_DB_CONFIG);

    if (!this.config.connectionString && !process.env.DATABASE_URL) {
      throw new Error('Database connection string is required');
    }

    this.pool = new Pool(
      defu(
        {
          connectionString:
            this.config.connectionString || process.env.DATABASE_URL,
        },
        this.config.pool,
        { ssl: this.config.ssl }
      )
    );
  }

  async getArticlesWithoutSummary(limit: number): Promise<NewsArticle[]> {
    const query = `
      SELECT 
        id, title, body, author, url, description, 
        featured_image, published_at, keywords, 
        content_status, category_id, created_at, updated_at
      FROM news
      WHERE has_summary = false 
        AND content_status = 'published'
        AND body IS NOT NULL
      ORDER BY published_at DESC NULLS LAST
      LIMIT $1
    `;

    try {
      const result = await this.pool.query(query, [limit]);
      return result.rows.map((row) => ({
        ...row,
        published_at: row.published_at ? new Date(row.published_at) : null,
        created_at: new Date(row.created_at),
        updated_at: new Date(row.updated_at),
      }));
    } catch (error) {
      console.error('Database error in getArticlesWithoutSummary:', error);
      throw new Error('Failed to fetch articles without summary');
    }
  }

  async insertSummary(newsId: string, summary: string): Promise<void> {
    const query = `
      INSERT INTO news_summaries (news_id, summary)
      VALUES ($1, $2)
      ON CONFLICT (news_id) 
      DO UPDATE SET 
        summary = $2
    `;

    try {
      await this.pool.query(query, [newsId, summary]);
    } catch (error) {
      console.error('Database error in insertSummary:', error);
      throw new Error('Failed to insert summary');
    }
  }

  async markArticleHasSummary(newsId: string): Promise<void> {
    const query = `
      UPDATE news
      SET 
        has_summary = true,
        updated_at = CURRENT_TIMESTAMP
      WHERE id = $1
    `;

    try {
      await this.pool.query(query, [newsId]);
    } catch (error) {
      console.error('Database error in markArticleHasSummary:', error);
      throw new Error('Failed to mark article as summarized');
    }
  }

  async close(): Promise<void> {
    await this.pool.end();
  }
}

================
File: src/agents/clients/groq.client.ts
================
// clients/groq.client.ts
import { Groq } from 'groq-sdk';
import type { GroqClient, GroqConfig, MergedConfig } from '../types';
import { defu } from 'defu';

export const DEFAULT_GROQ_CONFIG: GroqConfig = {
  defaultModel: 'mixtral-8x7b-32768',
  maxRetries: 3,
  timeout: 30000,
  defaultParams: {
    temperature: 0.3,
    top_p: 0.95,
    stream: false,
  },
};

export class GroqClientImpl implements GroqClient {
  private client: Groq;
  private config: MergedConfig<GroqConfig, typeof DEFAULT_GROQ_CONFIG>;

  constructor(config: Partial<GroqConfig> = {}) {
    this.config = defu(config, DEFAULT_GROQ_CONFIG);

    if (!this.config.apiKey && !process.env.GROQ_API_KEY) {
      throw new Error('Groq API key is required');
    }

    this.client = new Groq({
      apiKey: this.config.apiKey || process.env.GROQ_API_KEY,
      baseURL: this.config.baseURL,
    });
  }

  async query<T = unknown>(
    queryStr: string,
    options: Partial<ChatCompletionCreateParamsBase> = {}
  ): Promise<T> {
    try {
      const completion = await this.client.chat.completions.create(
        defu(
          { messages: [{ role: 'user', content: queryStr }] },
          options,
          this.config.defaultParams,
          { model: this.config.defaultModel }
        )
      );

      return JSON.parse(completion.choices[0]?.message?.content || '{}') as T;
    } catch (error) {
      console.error('Groq API error:', error);
      throw new Error('Failed to get completion from Groq');
    }
  }
}

================
File: src/agents/clients/index.ts
================
import { OpenAIClient } from './openai.client';
import { GroqClientImpl } from './groq.client';
import { DatabaseClientImpl } from './database.client';
import type { OpenAIConfig, GroqConfig, DatabaseConfig } from '../types';

export const createClient = {
  openAI: (config?: Partial<OpenAIConfig>) => new OpenAIClient(config),
  groq: (config?: Partial<GroqConfig>) => new GroqClientImpl(config),
  database: (config?: Partial<DatabaseConfig>) =>
    new DatabaseClientImpl(config),
};

================
File: src/agents/clients/openai.client.ts
================
// clients/openai.client.ts
import OpenAI from 'openai';
import { defu } from 'defu';
import { env } from '../../../../config';
import type { OpenAIConfig } from '../types';
import { getLogger } from '@base';
import type {
  ChatCompletionCreateParamsBase,
  ChatCompletionMessageParam,
} from 'openai/resources/chat/completions';

const DEFAULT_CONFIG: OpenAIConfig = {
  apiKey: env.openaiApiKey,
  organization: env.openaiOrgId,
  defaultModel: 'gpt-4o-mini',
  maxRetries: 3,
  timeout: 30000,
  temperature: 0.3,
  frequencyPenalty: 0.0,
  presencePenalty: 0.0,
};

export type CompletePromptOptions = Omit<
  ChatCompletionCreateParamsBase,
  'messages' | 'model' | 'stream'
>;

export const testOpenAI = async () => {
  const client = new OpenAIClient({
    apiKey: env.openaiApiKey,
    organization: env.openaiOrgId,
    defaultModel: 'gpt-4o-mini',
  });

  try {
    const response = await client.completePrompt([
      {
        role: 'system',
        content: 'You are a helpful assistant.',
      },
      {
        role: 'user',
        content: 'Say hello!',
      },
    ]);

    console.log('Response:', response);
  } catch (error) {
    console.error('Test failed:', error);
  }
};

export class OpenAIClient {
  private client: OpenAI;
  private config: OpenAIConfig;
  private log = getLogger('openai-client');

  constructor(config: Partial<OpenAIConfig> = {}) {
    this.config = defu(config, DEFAULT_CONFIG);

    if (!this.config.apiKey) {
      throw new Error('OpenAI API key is required');
    }

    this.log.info('Creating OpenAI client with config:', {
      model: this.config.defaultModel,
      organization: this.config.organization,
    });

    this.client = new OpenAI({
      apiKey: this.config.apiKey,
      organization: this.config.organization,
      baseURL: this.config.baseURL,
      timeout: this.config.timeout,
      maxRetries: this.config.maxRetries,
    });
  }

  async completePrompt(
    prompt: string | ChatCompletionMessageParam[],
    options: Partial<CompletePromptOptions> = {}
  ): Promise<string> {
    try {
      // Ensure messages are properly formatted
      const messages = Array.isArray(prompt)
        ? prompt.map((msg) => ({
            role: msg.role,
            content: msg.content,
          }))
        : [
            {
              role: 'user',
              content: prompt,
            },
          ];

      const requestPayload = {
        model: this.config.defaultModel!,
        messages,
        temperature: this.config.temperature,
        frequency_penalty: this.config.frequencyPenalty,
        presence_penalty: this.config.presencePenalty,
        ...options,
      };

      this.log.info('Sending request to OpenAI:', {
        model: requestPayload.model,
        messages: requestPayload.messages,
      });

      const completion =
        await this.client.chat.completions.create(requestPayload);

      this.log.info('Received response from OpenAI');
      return completion.choices[0]?.message?.content || '';
    } catch (error) {
      if (error instanceof OpenAI.APIError) {
        this.log.error('OpenAI API error:', {
          status: error.status,
          message: error.message,
          code: error.code,
          type: error.type,
          requestId: error.request_id,
        });
        throw error;
      } else {
        this.log.error('Unexpected error:', error);
        throw new Error('Failed to get completion from OpenAI');
      }
    }
  }
}

export const createOpenAIClient = (config?: Partial<OpenAIConfig>) =>
  new OpenAIClient(config);

================
File: src/agents/configs/config.ts
================
// agents/configs/config.ts
import { z } from 'zod';
import { Tool } from '../types';

export interface FewShotExample {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface AgentConfig {
  name: string;
  taskDescription: string;

  // Prompting
  systemPrompt?: string;
  userPrompt?: string;
  assistantPrompt?: string;
  fewShotExamples?: FewShotExample[];
  openAIModel?: string;

  // Data fetching (e.g. GROQ)
  groqQuery?: string;

  // Template placeholders for dynamic replacement in userPrompt/assistantPrompt.
  placeholders?: Record<string, unknown>;

  // Tools dynamically selected by the LLM
  tools?: Tool[];

  // Structured output validation
  outputSchema?: z.ZodType<any>;

  // Validation and retry configuration
  maxRetries?: number;
}

================
File: src/agents/configs/summarizer.config.ts
================
export const summarizerConfig: AgentConfig = {
  name: 'astronomy-news-summarizer',
  taskDescription: 'Summarize astronomy and space science news articles',

  systemPrompt: `You are a specialized AI assistant that summarizes astronomy and space science news articles.
    Create clear, concise summaries that:
    1. Capture the main scientific findings or discoveries
    2. Maintain technical accuracy while being accessible
    3. Highlight the significance for astronomy/space exploration
    4. Include relevant mission names, spacecraft, or astronomical objects
    5. Preserve specific measurements, distances, or technical specifications

    Provide a clear and concise summary text between 40-80 words. Your response should be plain text with no additional formatting.`,

  openAIModel: 'gpt-4o-mini',
  maxRetries: 2,
};

================
File: src/agents/types/clients.ts
================
// clients/types.ts
import type { Groq } from 'groq-sdk';
import type { Defu } from 'defu';

export interface OpenAIConfig {
    apiKey?: string;
    organization?: string;
    baseURL?: string;
    defaultModel?: string;
    timeout?: number;
    maxRetries?: number;
    temperature?: number;
    frequencyPenalty?: number;
    presencePenalty?: number;
}

export type GroqConfig = {
  apiKey?: string;
  baseURL?: string;
  defaultModel?: string;
  timeout?: number;
  maxRetries?: number;
  defaultParams?: Partial<Groq.Chat.Completions['create']>;
};

export type DatabaseConfig = {
  connectionString?: string;
  pool?: {
    max?: number;
    idleTimeoutMillis?: number;
    connectionTimeoutMillis?: number;
  };
  ssl?: boolean;
};

export type ClientConfig = {
    openAI?: Partial<OpenAIConfig>;
    groq?: Partial<GroqConfig>;
    database?: Partial<DatabaseConfig>;
  };

export type MergedConfig<T, D extends > = Defu<Partial<T>, [D]>;

export interface OpenAIClient {
  completePrompt(prompt: string, model?: string): Promise<string>;
}

export interface GroqClient {
  query<T = unknown>(queryStr: string): Promise<T>;
}

export interface NewsArticle {
    id: string;
    title: string;
    body: string;
    author: string;
    url: string;
    description?: string;
    featured_image?: string;
    published_at: Date | null;
    keywords?: Record<string, unknown>;
    content_status: string;
    category_id: number;
    created_at: Date;
    updated_at: Date;
  }

export interface DatabaseClient {
  getArticlesWithoutSummary(limit: number): Promise<NewsArticle[]>;
  insertSummary(newsId: string, summary: string): Promise<void>;

  markArticleHasSummary(newsId: string): Promise<void>;
}

================
File: src/agents/types/index.ts
================
// types/index.ts
import { OpenAIClient, GroqClient, DatabaseClient } from './clients';

export * from './clients';

export interface AgentContext {
  openAI: OpenAIClient;
  groq: GroqClient;
  db: DatabaseClient;
  config?: Record<string, unknown>;
}

export interface ArticleInput {
  title: string;
  author: string;
  body: string;
  published_at?: string;
}

export interface AgentInput {
  data: ArticleInput;
}

export interface AgentOutput {
  result: string;
}

export interface Tool {
  name: string;
  description?: string;
  execute(input: unknown): Promise<unknown>;
}

================
File: src/agents/utils/templating.ts
================
// utils/templating.ts
export function applyPlaceholders(
  template: string,
  placeholders?: Record<string, unknown>
): string {
  if (!placeholders) return template;
  return Object.entries(placeholders).reduce((acc, [key, val]) => {
    const re = new RegExp(`{{${key}}}`, 'g');
    return acc.replace(re, String(val));
  }, template);
}

// Tool instruction parsing:
// Suppose the LLM can output a line like "#useTool:toolName:{"some":"input"}"
export function parseToolInstruction(
  output: string
): { toolName: string; toolInput: unknown } | null {
  const match = output.match(/#useTool:(\w+):({.*})/);
  if (!match) return null;
  const toolName = match[1];
  try {
    const toolInput = JSON.parse(match[2]);
    return { toolName, toolInput };
  } catch {
    return null;
  }
}

================
File: src/agents/utils/validation.ts
================
// utils/validation.ts
import { z } from 'zod';
import { OpenAIClient } from '../clients';

export async function validateOutputWithRetry(
  completion: string,
  schema: z.ZodType<any>,
  openAI: OpenAIClient,
  originalPrompt: string,
  maxRetries: number
): Promise<unknown> {
  for (let i = 0; i <= maxRetries; i++) {
    try {
      const parsed = schema.parse(JSON.parse(completion));
      return parsed;
    } catch (err) {
      if (i === maxRetries)
        throw new Error('Failed to validate output after retries.');

      // Retry: ask LLM to reformat the output
      const reformatPrompt = `${originalPrompt}\n\nThe above response was not valid. Please return a strictly valid JSON matching the required schema. Don't add extra commentary.`;
      completion = await openAI.completePrompt(reformatPrompt);
    }
  }
  return completion; // fallback, though we never reach here due to throw
}

================
File: src/agents/agent.ts
================
// agent.ts
import { AgentContext } from './types';
import { AgentConfig, FewShotExample } from './configs/config';
import { z } from 'zod';
import { AgentInput, AgentOutput } from './types';
import { applyPlaceholders, parseToolInstruction } from './utils/templating';
import { validateOutputWithRetry } from './utils/validation';
import { logFile } from '@helpers';

type ContentItem = {
  type: 'text';
  text: string;
};

type ChatMessageContent = ContentItem[];

interface ChatCompletionMessageParam {
  role: 'system' | 'user' | 'assistant';
  content: ChatMessageContent;
}

export class Agent {
  private context: AgentContext;
  private config: AgentConfig;

  constructor(context: AgentContext, config: AgentConfig) {
    this.context = context;
    this.config = config;
  }

  public async execute(input: AgentInput): Promise<AgentOutput> {
    try {
      const messages = this.formatMessages(input.data);

      const completion = await this.context.openAI.completePrompt(messages);

      logFile('agent_completion:', completion);

      return { result: completion };
    } catch (error) {
      console.error('Error in agent execution:', error);
      throw error;
    }
  }

  private formatMessages(article: ArticleInput): ChatCompletionMessageParam[] {
    return [
      {
        role: 'system',
        content: [
          {
            type: 'text',
            text: this.config.systemPrompt,
          },
        ],
      },
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: `Please summarize this article:
                Title: ${article.title}
                Author: ${article.author}
                Content: ${article.body}`,
          },
        ],
      },
    ];
  }
}

================
File: src/agents/index.ts
================
export * from './configs/summarizer.config'
export * from './agent'
export * from './manager'
export * from './clients'

================
File: src/agents/manager.ts
================
// agentManager.ts
import { Agent } from './agent';
import { AgentInput, AgentOutput } from './types';

export interface PipelineStep {
  agent: Agent;
  condition?: (data: unknown) => boolean; // optional condition to run this step
}

export class AgentManager {
  private pipeline: PipelineStep[] = [];

  addStep(agent: Agent, condition?: (data: unknown) => boolean): void {
    this.pipeline.push({ agent, condition });
  }

  // Run pipeline: each step checks condition, if defined
  async runPipeline(input: AgentInput): Promise<AgentOutput> {
    let currentData = input.data;
    for (const step of this.pipeline) {
      if (!step.condition || step.condition(currentData)) {
        const result = await step.agent.execute({ data: currentData });
        currentData = result.result;
      }
    }
    return { result: currentData };
  }
}

================
File: src/core/extractors/extractAuthor.ts
================
import * as cheerio from 'cheerio'

interface AuthorExtractor {
  name: string
  extract: ($: cheerio.CheerioAPI) => string | null
}

const extractors: AuthorExtractor[] = [
  {
    name: 'Schema.org JSON-LD',
    extract: ($) => {
      const jsonLd = $('script[type="application/ld+json"]').first().html()
      if (jsonLd) {
        try {
          const data = JSON.parse(jsonLd)
          if (data['@type'] === 'Article' || data['@type'] === 'BlogPosting') {
            if (typeof data.author === 'string') {
              return data.author
            } else if (typeof data.author === 'object' && data.author.name) {
              return data.author.name
            }
          }
        } catch (e) {
          console.error('Error parsing JSON-LD:', e)
        }
      }
      return null
    },
  },
  {
    name: 'Meta Author',
    extract: ($) => {
      return $('meta[name="author"]').attr('content') || null
    },
  },
  {
    name: 'Open Graph',
    extract: ($) => {
      return (
        $('meta[property="og:author"], meta[property="article:author"]').attr('content') || null
      )
    },
  },
  {
    name: 'Twitter Creator',
    extract: ($) => {
      return $('meta[name="twitter:creator"]').attr('content') || null
    },
  },
  {
    name: 'RelAuthor Link',
    extract: ($) => {
      return $('link[rel="author"]').attr('href') || null
    },
  },
  {
    name: 'Common Author Classes',
    extract: ($) => {
      return $('.author, .byline, .entry-author, .post-author').first().text().trim() || null
    },
  },
  {
    name: 'Author Element',
    extract: ($) => {
      return $('[itemprop="author"]').first().text().trim() || null
    },
  },
  {
    name: 'Regex Pattern',
    extract: ($) => {
      const bodyText = $('body').text()
      const authorPatterns = [
        /By\s+([\w\s]+)/i,
        /Author[:\s]+([\w\s]+)/i,
        /Written by\s+([\w\s]+)/i,
      ]
      for (const pattern of authorPatterns) {
        const match = bodyText.match(pattern)
        if (match && match[1]) {
          return match[1].trim()
        }
      }
      return null
    },
  },
]

export function extractAuthor(cleanHTML: string): string | null {
  const $ = cheerio.load(cleanHTML)
  for (const extractor of extractors) {
    const author = extractor.extract($)

    if (author) {
      console.log(`${extractor.name} extractor found author: "${author}"`)
      return author
    } else {
      console.log(`${extractor.name} extractor did not find an author.`)
    }
  }

  return null
}

================
File: src/core/extractors/extractDate.ts
================
import * as cheerio from 'cheerio';
import * as chrono from 'chrono-node';

export function extractPublishedDate(html: string): Date | null {
  const $ = cheerio.load(html);

  // 1. Check metadata
  const metaDate = checkMetadata($);
  if (metaDate) return metaDate;

  // 2. Check specific HTML elements
  const elementDate = checkElements($);
  if (elementDate) return elementDate;

  // 3. Parse content for date patterns
  const contentDate = parseContent($);
  if (contentDate) return contentDate;

  // 4. Use NLP as a last resort
  return useNLP($);
}

function checkMetadata($: cheerio.CheerioAPI): Date | null {
  const metaTags = [
    'article:published_time',
    'datePublished',
    'date',
    'publishdate',
    'pubdate',
    'og:published_time',
    'publication-date',
    'release_date',
  ];

  for (const tag of metaTags) {
    const metaElement = $(`meta[property="${tag}"], meta[name="${tag}"]`);
    if (metaElement.length) {
      const dateString = metaElement.attr('content');
      if (dateString) {
        const date = new Date(dateString);
        if (!isNaN(date.getTime())) return date;
      }
    }
  }

  return null;
}

function checkElements($: cheerio.CheerioAPI): Date | null {
  const dateElements = $(
    'time, [itemprop="datePublished"], .entry-meta-date, .published, .post-date, .entry-date, .date'
  );

  for (let i = 0; i < dateElements.length; i++) {
    const element = dateElements.eq(i);
    let dateString = element.attr('datetime') || element.text();
    if (dateString) {
      const date = new Date(dateString);
      if (!isNaN(date.getTime())) return date;
    }
  }

  return null;
}

function parseContent($: cheerio.CheerioAPI): Date | null {
  const bodyText = $('body').text();
  const dateRegex = /\d{1,4}[-./]\d{1,2}[-./]\d{2,4}/g;
  const matches = bodyText.match(dateRegex);

  if (matches) {
    for (const match of matches) {
      const date = new Date(match);
      if (!isNaN(date.getTime())) return date;
    }
  }

  return null;
}

function useNLP($: cheerio.CheerioAPI): Date | null {
  const bodyText = $('body').text();
  const now = new Date();
  const results = chrono.parse(bodyText, now, { forwardDate: false });

  if (results.length > 0) {
    const start = results.find((result) => {
      const startDate = result.start.get('year');
      if (startDate) {
        return startDate <= now.getFullYear();
      }
      return false;
    });
    return start?.date() || now;
  }

  return null;
}

================
File: src/core/extractors/extractFeaturedImage.ts
================
import * as cheerio from 'cheerio';

interface ImageExtractor {
  name: string;
  extract: ($: cheerio.CheerioAPI, url: string) => string | null;
}

const extractors: ImageExtractor[] = [
  {
    name: 'Open Graph Image',
    extract: ($) => {
      return $('meta[property="og:image"]').attr('content') || null;
    },
  },
  {
    name: 'Twitter Image',
    extract: ($) => {
      return $('meta[name="twitter:image"]').attr('content') || null;
    },
  },
  {
    name: 'Schema.org JSON-LD',
    extract: ($) => {
      const jsonLd = $('script[type="application/ld+json"]').first().html();
      if (jsonLd) {
        try {
          const data = JSON.parse(jsonLd);
          if (data['@type'] === 'Article' || data['@type'] === 'BlogPosting') {
            return data.image || null;
          }
        } catch (e) {
          console.error('Error parsing JSON-LD:', e);
        }
      }
      return null;
    },
  },
  {
    name: 'Article Image',
    extract: ($) => {
      return (
        $('article img, .post-content img, .entry-content img')
          .first()
          .attr('src') || null
      );
    },
  },
  {
    name: 'First Image',
    extract: ($) => {
      return $('img').first().attr('src') || null;
    },
  },
  {
    name: 'Background Image',
    extract: ($) => {
      let backgroundImage = null;
      $('*').each((_, element) => {
        const style = $(element).attr('style');
        if (style && style.includes('background-image')) {
          const match = style.match(/url\(['"]?(.*?)['"]?\)/);
          if (match && match[1]) {
            backgroundImage = match[1];
            return false; // Break the loop
          }
        }
      });
      return backgroundImage;
    },
  },
];

export function extractFeaturedImage(
  cleanHTML: string,
  url: string
): string | null {
  const $ = cheerio.load(cleanHTML);

  for (const extractor of extractors) {
    const imageUrl = extractor.extract($, url);
    if (imageUrl) {
      console.log(`Featured image extracted using ${extractor.name} method`);
      return resolveRelativeUrl(imageUrl, url);
    }
  }

  console.log('Could not extract featured image');
  return null;
}

function resolveRelativeUrl(imageUrl: string, baseUrl: string): string {
  try {
    return new URL(imageUrl, baseUrl).href;
  } catch (e) {
    console.error('Error resolving relative URL:', e);
    return imageUrl;
  }
}

================
File: src/core/extractors/extractKeywords.ts
================
import * as cheerio from 'cheerio';
import natural from 'natural';

function parseKeywords(keywords: string | string[] | null): string[] {
  if (!keywords) return [];

  if (typeof keywords === 'string') {
    try {
      const parsed = JSON.parse(keywords);
      if (Array.isArray(parsed)) {
        return parsed.map((k) => k.toString());
      } else if (typeof parsed === 'object') {
        return Object.values(parsed).map((k: any) => k.toString());
      }
    } catch (e) {
      return keywords.split(',').map((k) => k.trim());
    }
  }

  if (Array.isArray(keywords)) {
    return keywords.map((k) => k.toString());
  }

  return [];
}

interface KeywordExtractor {
  name: string;
  extract: ($: cheerio.Root, fullText: string) => string[] | null;
}

const extractors: KeywordExtractor[] = [
  {
    name: 'Meta Keywords',
    extract: ($) => {
      const metaKeywords = $('meta[name="keywords"]').attr('content');
      return metaKeywords ? parseKeywords(metaKeywords) : null;
    },
  },
  {
    name: 'Schema.org JSON-LD',
    extract: ($) => {
      const jsonLd = $('script[type="application/ld+json"]');
      if (jsonLd.length) {
        try {
          const data = JSON.parse(jsonLd.first().text());
          if (data['@type'] === 'Article' || data['@type'] === 'BlogPosting') {
            return parseKeywords(data.keywords);
          }
        } catch (e) {
          console.error('Error parsing JSON-LD:', e);
        }
      }
      return null;
    },
  },
  {
    name: 'Article Tags',
    extract: ($) => {
      const tags = $('.tags a, .keywords a, .topics a');
      return tags.length > 0
        ? tags.map((_, el) => $(el).text().trim()).get()
        : null;
    },
  },
  {
    name: 'TF-IDF Analysis',
    extract: (_, fullText) => {
      const tfidf = new natural.TfIdf();
      tfidf.addDocument(fullText);

      const stopwords = new Set(natural.stopwords);
      const tokenizer = new natural.WordTokenizer();
      const tokens = tokenizer.tokenize(fullText) || [];

      const wordScores = tokens
        .filter(
          (token) => token.length > 2 && !stopwords.has(token.toLowerCase())
        )
        .map((token) => ({
          word: token,
          score: tfidf.tfidf(token, 0),
        }));

      wordScores.sort((a, b) => b.score - a.score);
      return wordScores.slice(0, 10).map((item) => item.word);
    },
  },
];

function filterKeywords(keywords: string[]): string[] {
  const currentYear = new Date().getFullYear();
  const exceptions = new Set([
    'related',
    'articles',
    'inc',
    // Add more specific words to ignore here
  ]);

  return keywords.filter((keyword) => {
    const lowercaseKeyword = keyword.toLowerCase();

    // Check if it's not in the exceptions list
    if (exceptions.has(lowercaseKeyword)) return false;

    // Check if it's not a year between 1900 and current year
    const year = parseInt(keyword, 10);
    if (!isNaN(year) && year >= 1900 && year <= currentYear) return false;

    return true;
  });
}

export function extractKeywords(html: string): { values: string[] } {
  const $ = cheerio.load(html);
  const fullText = $('body').text();

  let keywords: string[] = [];

  for (const extractor of extractors) {
    const extractedKeywords = extractor.extract($, fullText);
    if (extractedKeywords && extractedKeywords.length > 0) {
      console.log(`Keywords extracted using ${extractor.name} method`);
      keywords = keywords.concat(extractedKeywords);
      if (keywords.length >= 10) break; // Stop if we have enough keywords
    }
  }

  keywords = filterKeywords(keywords);

  // Remove duplicates and limit to top 10
  keywords = Array.from(new Set(keywords)).slice(0, 10);

  if (keywords.length > 0) {
    console.log(`Extracted keywords: ${keywords.join(', ')}`);
  } else {
    console.log('No keywords found');
  }

  return { values: keywords };
}

================
File: src/core/extractors/extractor.module.ts
================
import type { Page } from 'playwright'
import type { CustomLogger } from '@core'
import {
  extractPublishedDate,
  extractTitle,
  extractAuthor,
  extractFeaturedImage,
  extractKeywords,
} from './index'

export class ExtractorModule {
  constructor(private logger: CustomLogger) {}

  extractPublishedDate(html: string): Date | null {
    try {
      return extractPublishedDate(html)
    } catch (error) {
      this.logger.warn('Failed to extract published date', { error })
      return null
    }
  }

  extractTitle(html: string): string {
    try {
      return extractTitle(html) || ''
    } catch (error) {
      this.logger.warn('Failed to extract title', { error })
      return ''
    }
  }

  extractAuthor(html: string): string | null {
    try {
      return extractAuthor(html)
    } catch (error) {
      this.logger.warn('Failed to extract author', { error })
      return null
    }
  }

  extractFeaturedImage(html: string, baseUrl: string): string | null {
    try {
      return extractFeaturedImage(html, baseUrl)
    } catch (error) {
      this.logger.warn('Failed to extract featured image', { error })
      return null
    }
  }

  extractKeywords(html: string): string[] {
    try {
      return extractKeywords(html)
    } catch (error) {
      this.logger.warn('Failed to extract keywords', { error })
      return []
    }
  }
}

================
File: src/core/extractors/extractTitle.ts
================
import * as cheerio from 'cheerio';

interface TitleExtractor {
  name: string;
  extract: ($: cheerio.CheerioAPI) => string | null;
}

const extractors: TitleExtractor[] = [
  {
    name: 'Open Graph',
    extract: ($) => $('meta[property="og:title"]').attr('content') || null,
  },
  {
    name: 'Twitter Card',
    extract: ($) => $('meta[name="twitter:title"]').attr('content') || null,
  },
  {
    name: 'Meta Title',
    extract: ($) => $('meta[name="title"]').attr('content') || null,
  },
  {
    name: 'H1',
    extract: ($) => $('h1').first().text().trim() || null,
  },
  {
    name: 'Title Tag',
    extract: ($) => $('title').text().trim() || null,
  },
  {
    name: 'Schema.org JSON-LD',
    extract: ($) => {
      const jsonLd = $('script[type="application/ld+json"]');
      if (jsonLd.length) {
        try {
          const data = JSON.parse(jsonLd.first().text());
          if (data['@type'] === 'Article' || data['@type'] === 'BlogPosting') {
            return data.headline || null;
          }
        } catch (e) {
          console.error('Error parsing JSON-LD:', e);
        }
      }
      return null;
    },
  },
  {
    name: 'Article Title Class',
    extract: ($) =>
      $('.article-title, .post-title, .entry-title').first().text().trim() ||
      null,
  },
];

export function extractTitle(html: string): string | null {
  const $ = cheerio.load(html);
  for (const extractor of extractors) {
    const title = extractor.extract($);
    if (title) {
      return title;
    }
  }

  console.log('Could not extract title');
  return null;
}

================
File: src/core/extractors/index.ts
================
export * from './extractAuthor'
export * from './extractDate'
export * from './extractFeaturedImage'
export * from './extractKeywords'
export * from './extractTitle'
export * from './extractor.module'

================
File: src/core/services/circuit-breaker.service.ts
================
// src/infrastructure/circuit-breaker/circuit-breaker.service.ts
import { CustomLogger, PrismaService, EventService } from '@core'
import { CircuitState, CircuitStatus } from '@types'

export class CircuitBreakerService {
  private readonly states = new Map<string, CircuitStatus>()
  private readonly failureThreshold = 5
  private readonly resetTimeout = 60000 // 1 minute

  constructor(
    private readonly logger: CustomLogger,
    private readonly prisma: PrismaService,
    private readonly events: EventService,
  ) {
    this.logger.setDomain('circuit_breaker')
  }

  private async loadCircuitState(jobName: string): Promise<CircuitStatus> {
    try {
      const state = await this.prisma.circuit_breaker_states.findUnique({
        where: { job_name: jobName },
      })

      if (!state) {
        return {
          state: 'closed',
          failures: 0,
          lastFailure: new Date(0),
        }
      }

      return {
        state: state.state as CircuitState,
        failures: state.failure_count ?? 0,
        lastFailure: state.last_failure ?? new Date(0),
      }
    } catch (error: any) {
      this.logger.error(`Failed to load circuit state for ${jobName}`, {
        error,
        context: { jobName },
      })
      return {
        state: 'closed',
        failures: 0,
        lastFailure: new Date(0),
      }
    }
  }

  private async saveCircuitState(
    jobName: string,
    status: CircuitStatus,
    previousState?: CircuitState,
  ): Promise<void> {
    try {
      await this.prisma.circuit_breaker_states.upsert({
        where: { job_name: jobName },
        create: {
          job_name: jobName,
          state: status.state,
          failure_count: status.failures,
          last_failure: status.lastFailure,
          updated_at: new Date(),
        },
        update: {
          state: status.state,
          failure_count: status.failures,
          last_failure: status.lastFailure,
          updated_at: new Date(),
        },
      })

      if (previousState && previousState !== status.state) {
        this.events.emit('circuit-breaker.state-changed', {
          jobName,
          previousState,
          newState: status.state,
          failures: status.failures,
        })

        this.logger.info(`Circuit state changed for ${jobName}`, {
          context: {
            jobName,
            previousState,
            newState: status.state,
            failures: status.failures,
          },
        })
      }
    } catch (error: any) {
      this.logger.error(`Failed to save circuit state for ${jobName}`, {
        error,
        context: { jobName, status },
      })
    }
  }

  async getCircuitState(jobName: string): Promise<CircuitStatus> {
    let status = this.states.get(jobName)

    if (!status) {
      status = await this.loadCircuitState(jobName)
      this.states.set(jobName, status)
    }

    if (status.state === 'open') {
      const now = new Date().getTime()
      const failureTime = status.lastFailure.getTime()

      if (now - failureTime > this.resetTimeout) {
        const previousState = status.state
        status.state = 'half-open'
        await this.saveCircuitState(jobName, status, previousState)
      }
    }

    return status
  }

  async recordSuccess(jobName: string): Promise<void> {
    const status = await this.getCircuitState(jobName)
    const previousState = status.state

    if (status.state === 'half-open') {
      status.state = 'closed'
      status.failures = 0
      await this.saveCircuitState(jobName, status, previousState)

      this.events.emit('circuit-breaker.success', {
        jobName,
        previousState,
      })
    }

    this.states.set(jobName, status)
  }

  async recordFailure(jobName: string, error: Error): Promise<void> {
    const status = await this.getCircuitState(jobName)
    const previousState = status.state

    status.failures++
    status.lastFailure = new Date()

    if (status.failures >= this.failureThreshold) {
      status.state = 'open'

      this.events.emit('circuit-breaker.failure', {
        jobName,
        error,
        failures: status.failures,
        threshold: this.failureThreshold,
      })

      this.logger.warn(`Circuit opened after ${status.failures} failures`, {
        context: {
          jobName,
          failures: status.failures,
          error: error.message,
        },
      })
    }

    await this.saveCircuitState(jobName, status, previousState)
    this.states.set(jobName, status)
  }

  async executeWithBreaker<T>(
    jobName: string,
    operation: () => Promise<T>,
    timeout: number = 30000,
  ): Promise<T> {
    const status = await this.getCircuitState(jobName)

    if (status.state === 'open') {
      this.logger.warn(`Circuit breaker is open`, {
        context: {
          jobName,
          failures: status.failures,
          lastFailure: status.lastFailure,
        },
      })
      throw new Error('Circuit breaker is open')
    }

    try {
      const result = (await Promise.race([
        operation(),
        new Promise((_, reject) => {
          setTimeout(() => {
            const timeoutError = new Error('Operation timed out')
            this.events.emit('circuit-breaker.timeout', {
              jobName,
              timeoutMs: timeout,
            })
            reject(timeoutError)
          }, timeout)
        }),
      ])) as T

      await this.recordSuccess(jobName)
      return result
    } catch (error: any) {
      await this.recordFailure(jobName, error)
      throw error
    }
  }

  async resetCircuit(jobName: string): Promise<void> {
    const currentStatus = await this.getCircuitState(jobName)
    const previousState = currentStatus.state

    const status = {
      state: 'closed' as CircuitState,
      failures: 0,
      lastFailure: new Date(0),
    }

    await this.saveCircuitState(jobName, status, previousState)
    this.states.set(jobName, status)

    this.logger.info(`Circuit manually reset`, {
      context: {
        jobName,
        previousState: currentStatus.state,
      },
    })
  }

  async getCircuitStats(jobName: string) {
    const status = await this.getCircuitState(jobName)
    return {
      jobName,
      state: status.state,
      failures: status.failures,
      lastFailure: status.lastFailure,
      isOpen: status.state === 'open',
      timeInCurrentState: Date.now() - status.lastFailure.getTime(),
    }
  }
}

================
File: src/core/services/event.service.ts
================
// src/infrastructure/events/event.service.ts
import { EventEmitter } from 'events'
import { CustomLogger } from './logger.service'

export class EventService {
  private emitter: EventEmitter

  constructor(private readonly logger: CustomLogger) {
    this.emitter = new EventEmitter()
    this.logger.setDomain('job_events')
  }

  on(event: string, listener: (...args: any[]) => void) {
    this.emitter.on(event, (...args) => {
      try {
        listener(...args)
      } catch (error: any) {
        this.logger.error(`Error handling event ${event}`, error)
      }
    })
  }

  emit(event: string, ...args: any[]) {
    this.emitter.emit(event, ...args)
    this.logger.debug(`Event emitted: ${event}`, {
      event,
      args: args.map((arg) =>
        arg instanceof Error ? { message: arg.message, stack: arg.stack } : arg,
      ),
    })
  }
}

================
File: src/core/services/logger.service.ts
================
// src/infrastructure/logger/logger.service.ts
import { createCentralizedLogger, Service, DomainsForService, LogContext } from '@ib/logger'

export class CustomLogger {
  private centralLogger
  private context?: string
  private domain?: DomainsForService<Service.JOBS>

  constructor(context?: string) {
    this.context = context
    this.centralLogger = createCentralizedLogger<Service>()
    this.centralLogger.setServiceName(Service.JOBS)
  }

  setDomain(domain: DomainsForService<Service.JOBS>) {
    this.domain = domain
    this.centralLogger.setDomain(domain)
  }

  async init() {
    const nodeTransport = this.centralLogger['transport']
    if (nodeTransport && typeof nodeTransport['init'] === 'function') {
      await nodeTransport.init()
    }
  }

  private formatMessage(message: string, context?: LogContext): string {
    const jobInfo = context?.jobId ? `[${context.jobName}:${context.jobId}] ` : ''
    return `${jobInfo}${message}`
  }

  private getLogContext(context?: LogContext) {
    return {
      context: this.context,
      domain: this.domain,
      ...context,
    }
  }

  error(message: string, { error, context }: { error?: Error; context?: LogContext }) {
    const formattedMessage = this.formatMessage(message, context)
    if (error instanceof Error) {
      void this.centralLogger.error(formattedMessage, {
        error: { message: error.message, stack: error.stack },
        ...this.getLogContext(context),
      })
    } else {
      void this.centralLogger.error(formattedMessage, {
        error,
        ...this.getLogContext(context),
      })
    }
  }

  warn(message: string, context?: LogContext) {
    void this.centralLogger.warn(this.formatMessage(message, context), this.getLogContext(context))
  }

  info(message: string, context?: LogContext) {
    void this.centralLogger.info(this.formatMessage(message, context), this.getLogContext(context))
  }

  debug(message: string, context?: LogContext) {
    void this.centralLogger.debug(this.formatMessage(message, context), this.getLogContext(context))
  }

  verbose(message: string, context?: LogContext) {
    void this.centralLogger.verbose(
      this.formatMessage(message, context),
      this.getLogContext(context),
    )
  }
}

================
File: src/core/services/metrics.service.ts
================
import { error_type, job_status } from '@prisma/client'
import { CustomLogger } from './logger.service'
import { PrismaService } from './prisma.service'
import { EventService } from './event.service'
import { JobExecutionMetrics, CircuitBreakerMetrics, JobMetrics } from '@types'

// src/infrastructure/services/metrics/metrics.service.ts

export class MetricsService {
  constructor(
    private readonly logger: CustomLogger,
    private readonly prisma: PrismaService,
    private readonly eventService: EventService,
  ) {
    this.logger.setDomain('metrics')
    this.setupEventListeners()
  }

  private setupEventListeners() {
    this.eventService.on('job.started', async (jobName: string, jobId: string, metadata?: any) => {
      await this.trackJobStart(jobName, jobId, metadata)
    })

    this.eventService.on('job.completed', async (jobName: string, jobId: string, result: any) => {
      const duration = await this.calculateJobDuration(jobId)
      await this.trackJobSuccess(jobName, jobId, duration, result)
    })

    this.eventService.on(
      'job.failed',
      async (jobName: string, jobId: string, error: Error, metadata?: any) => {
        const duration = await this.calculateJobDuration(jobId)
        await this.trackJobFailure(jobName, jobId, duration, error, metadata)
      },
    )

    this.eventService.on('circuit-breaker.state-changed', async (data) => {
      await this.trackCircuitBreakerChange(data.jobName, {
        jobName: data.jobName,
        state: data.newState,
        failures: data.failures,
        lastFailure: new Date(),
        timeInCurrentState: 0,
        lastSuccess: data.lastSuccess,
        recoveryAttempts: data.recoveryAttempts || 0,
      })
    })

    // Performance monitoring events
    this.eventService.on(
      'job.performance',
      async (jobName: string, jobId: string, metrics: any) => {
        await this.trackPerformanceMetrics(jobName, jobId, metrics)
      },
    )
  }

  async trackQueueStats(queueName: string, stats: QueueStats) {
    try {
      const data = {
        created_count: Math.max(0, Math.floor(Number(stats.created))),
        retry_count: Math.max(0, Math.floor(Number(stats.retry))),
        active_count: Math.max(0, Math.floor(Number(stats.active))),
        completed_count: Math.max(0, Math.floor(Number(stats.completed))),
        cancelled_count: Math.max(0, Math.floor(Number(stats.cancelled))),
        failed_count: Math.max(0, Math.floor(Number(stats.failed))),
        total_count: Math.max(0, Math.floor(Number(stats.all))),
        updated_at: new Date(),
      }

      await this.prisma.job_queue_stats.upsert({
        where: { queue_name: queueName },
        create: {
          queue_name: queueName,
          ...data,
        },
        update: data,
      })
    } catch (error: any) {
      this.logger.error('Failed to track queue stats', {
        error,
        context: { queueName, stats },
      })
    }
  }

  // Add method to get historical stats
  async getQueueStatsHistory(queueName: string, hours: number = 24): Promise<any[]> {
    return this.prisma.job_queue_stats.findMany({
      where: {
        queue_name: queueName,
        updated_at: {
          gte: new Date(Date.now() - hours * 60 * 60 * 1000),
        },
      },
      orderBy: {
        updated_at: 'desc',
      },
    })
  }

  async trackJobExecutionMetrics(metrics: JobExecutionMetrics) {
    try {
      const performanceData = metrics.performance
        ? {
            items_per_second: metrics.performance.itemsPerSecond,
            avg_processing_time: metrics.performance.avgProcessingTime,
            peak_memory_usage: metrics.performance.peakMemoryUsage,
          }
        : undefined

      await this.prisma.job_metrics.upsert({
        where: {
          id: metrics.jobId,
          job_name: metrics.jobName,
        },
        create: {
          job_id: metrics.jobId,
          job_name: metrics.jobName,
          status: metrics.status,
          started_at: metrics.startTime,
          duration_ms: metrics.duration,
          items_processed: metrics.itemsProcessed,
          error_message: metrics.error?.message,
          error_stack: metrics.error?.stack,
          metadata: {
            ...metrics.metadata,
            performance: performanceData,
          },
        },
        update: {
          status: metrics.status,
          duration_ms: metrics.duration,
          items_processed: metrics.itemsProcessed,
          error_message: metrics.error?.message,
          error_stack: metrics.error?.stack,
          metadata: {
            ...metrics.metadata,
            performance: performanceData,
          },
          updated_at: new Date(),
        },
      })
    } catch (error: any) {
      await this.logError('Failed to track job execution metrics', error, metrics)
    }
  }

  async trackCircuitBreakerChange(jobName: string, metrics: CircuitBreakerMetrics) {
    this.logger.debug(`Tracking circuit breaker change: ${jobName}`, metrics)
    try {
      await this.prisma.circuit_breaker_states.upsert({
        where: { job_name: jobName },
        create: {
          job_name: jobName,
          state: metrics.state,
          failure_count: metrics.failures,
          last_failure: metrics.lastFailure,
          last_success: metrics.lastSuccess,
          updated_at: new Date(),
        },
        update: {
          state: metrics.state,
          failure_count: metrics.failures,
          last_failure: metrics.lastFailure,
          last_success: metrics.lastSuccess,
          updated_at: new Date(),
        },
      })
    } catch (error: any) {
      await this.logError('Failed to track circuit breaker metrics', error, {
        jobName,
        metrics,
      })
    }
  }

  async trackPerformanceMetrics(jobName: string, jobId: string, metrics: any) {
    try {
      await this.prisma.job_metrics.update({
        where: {
          id: jobId,
        },
        data: {
          metadata: {
            performance: {
              itemsPerSecond: metrics.itemsPerSecond,
              avgProcessingTime: metrics.avgProcessingTime,
              peakMemoryUsage: metrics.peakMemoryUsage,
              batchMetrics: metrics.batchMetrics,
            },
          },
        },
      })
    } catch (error: any) {
      await this.logError('Failed to track performance metrics', error, { jobName, jobId, metrics })
    }
  }

  async getJobPerformanceStats(jobName: string, period: 'hour' | 'day' | 'week' | 'month') {
    const startDate = new Date()
    switch (period) {
      case 'hour':
        startDate.setHours(startDate.getHours() - 1)
        break
      case 'day':
        startDate.setDate(startDate.getDate() - 1)
        break
      case 'week':
        startDate.setDate(startDate.getDate() - 7)
        break
      case 'month':
        startDate.setMonth(startDate.getMonth() - 1)
        break
    }

    try {
      const metrics = await this.prisma.job_metrics.findMany({
        where: {
          job_name: jobName,
          started_at: { gte: startDate },
        },
        orderBy: { started_at: 'desc' },
      })

      const stats = {
        totalRuns: metrics.length,
        successRate: this.calculateSuccessRate(metrics),
        avgDuration: this.calculateAverageDuration(metrics),
        avgItemsProcessed: this.calculateAverageItemsProcessed(metrics),
        performanceMetrics: this.aggregatePerformanceMetrics(metrics),
        errorRate: this.calculateErrorRate(metrics),
        timeoutRate: this.calculateTimeoutRate(metrics),
      }

      return stats
    } catch (error: any) {
      await this.logError('Failed to get job performance stats', error, { jobName, period })
      throw error
    }
  }

  private calculateSuccessRate(metrics: any[]): number {
    const successful = metrics.filter((m) => m.status === 'completed').length
    return (successful / metrics.length) * 100
  }

  private calculateAverageDuration(metrics: any[]): number {
    const durations = metrics.map((m) => m.duration_ms).filter(Boolean)
    return durations.length ? durations.reduce((a, b) => a + b, 0) / durations.length : 0
  }

  private calculateAverageItemsProcessed(metrics: any[]): number {
    const items = metrics.map((m) => m.items_processed).filter(Boolean)
    return items.length ? items.reduce((a, b) => a + b, 0) / items.length : 0
  }

  private calculateErrorRate(metrics: any[]): number {
    const errors = metrics.filter((m) => m.status === 'failed').length
    return (errors / metrics.length) * 100
  }

  private calculateTimeoutRate(metrics: any[]): number {
    const timeouts = metrics.filter(
      (m) => m.error_message?.includes('timeout') || m.error_message?.includes('timed out'),
    ).length
    return (timeouts / metrics.length) * 100
  }

  private aggregatePerformanceMetrics(metrics: any[]) {
    const performanceData = metrics.map((m) => m.metadata?.performance).filter(Boolean)

    if (!performanceData.length) return null

    return {
      avgItemsPerSecond: this.average(performanceData.map((p) => p.itemsPerSecond)),
      avgProcessingTime: this.average(performanceData.map((p) => p.avgProcessingTime)),
      avgPeakMemoryUsage: this.average(performanceData.map((p) => p.peakMemoryUsage)),
    }
  }

  private average(values: number[]): number {
    return values.reduce((a, b) => a + b, 0) / values.length
  }

  private async calculateJobDuration(jobId: string): Promise<number> {
    const metric = await this.prisma.job_metrics.findFirst({
      where: { job_id: jobId, status: 'active' },
    })
    return metric ? Date.now() - metric.started_at.getTime() : 0
  }

  async trackJobStart(jobName: string, jobId: string, metadata?: Record<string, any>) {
    this.logger.debug(`Tracking job start: ${jobName}`, { jobId, metadata })
    try {
      await this.prisma.job_metrics.create({
        data: {
          job_id: jobId,
          job_name: jobName,
          status: 'active',
          started_at: new Date(),
          metadata: metadata || {},
        },
      })
    } catch (error: any) {
      await this.logError('Failed to track job start', error, {
        jobName,
        jobId,
        metadata,
      })
    }
  }

  async trackJobSuccess(
    jobName: string,
    jobId: string,
    duration: number,
    result: {
      itemsProcessed?: number
      metadata?: Record<string, any>
      performance?: {
        itemsPerSecond: number
        avgProcessingTime: number
        peakMemoryUsage: number
      }
    },
  ) {
    this.logger.debug(`Tracking job success: ${jobName}`, { jobId, duration, result })
    try {
      await this.prisma.job_metrics.updateMany({
        where: {
          job_name: jobName,
          job_id: jobId,
          status: 'active',
        },
        data: {
          status: 'completed',
          duration_ms: duration,
          items_processed: result?.itemsProcessed,
          metadata: {
            ...result?.metadata,
            performance: result?.performance,
          },
          completed_at: new Date(),
        },
      })
    } catch (error: any) {
      await this.logError('Failed to track job success', error, {
        jobName,
        jobId,
        duration,
        result,
      })
    }
  }

  async trackJobFailure(
    jobName: string,
    jobId: string,
    duration: number,
    error: Error,
    metadata?: Record<string, any>,
  ) {
    this.logger.debug(`Tracking job failure: ${jobName}`, { jobId, duration, error, metadata })
    try {
      await this.prisma.job_metrics.updateMany({
        where: {
          job_name: jobName,
          job_id: jobId,
          status: 'active',
        },
        data: {
          status: 'failed',
          duration_ms: duration,
          error_message: error.message,
          error_stack: error.stack,
          metadata: metadata || {},
          failed_at: new Date(),
        },
      })

      await this.logError('Job failed', error, {
        jobName,
        jobId,
        duration,
        metadata,
      })
    } catch (logError: any) {
      await this.logError('Failed to track job failure', logError, {
        jobName,
        jobId,
        duration,
        originalError: error.message,
        metadata,
      })
    }
  }

  async trackJobCompletion(
    jobName: string,
    jobId: string,
    metrics: JobMetrics & { metadata?: Record<string, any> },
  ) {
    this.logger.debug(`Tracking job completion: ${jobName}`, { jobId, metrics })
    if (metrics.success) {
      await this.trackJobSuccess(jobName, jobId, metrics.duration, {
        itemsProcessed: metrics.itemsProcessed,
        metadata: metrics.metadata,
        performance: metrics.performance,
      })
    } else {
      await this.trackJobFailure(jobName, jobId, metrics.duration, metrics.error!, metrics.metadata)
    }
  }

  private async logError(message: string, error: Error, context: Record<string, any> = {}) {
    try {
      await this.prisma.error_logs.create({
        data: {
          service_name: 'jobs',
          error_type: error_type.DATABASE_ERROR,
          severity: 'high',
          message: `${message}: ${error.message}`,
          stack_trace: error.stack,
          metadata: context,
          environment: process.env.NODE_ENV || 'development',
          domain: context.jobName || 'metrics',
        },
      })
    } catch (logError: any) {
      this.logger.error('Failed to log to database:', logError)
    }
  }
}

================
File: src/core/services/prisma.service.ts
================
// src/core/services/prisma.service.ts
import { PrismaClient, Prisma } from '@prisma/client'
import { config } from '../../config'
import { CustomLogger } from './logger.service'

export class PrismaService extends PrismaClient {
  private isConnected = false

  constructor(private readonly logger: CustomLogger) {
    console.log('config.database.url', config.database.directUrl)
    super({
      datasources: {
        db: {
          url: config.database.directUrl,
        },
      },
      log: [
        { emit: 'event', level: 'query' },
        { emit: 'event', level: 'error' },
        { emit: 'event', level: 'warn' },
      ],
    })

    this.setupLogging()
  }

  private setupLogging() {
    // Log queries in development
    if (config.app.environment === 'development') {
      this.$on('query' as any, (e: any) => {
        this.logger.debug('Query executed', {
          query: e.query,
          duration: e.duration,
          timestamp: e.timestamp,
        })
      })
    }

    // Always log errors
    this.$on('error' as any, (e: any) => {
      this.logger.error('Prisma error occurred', {
        ...e,
      })
    })

    this.$on('warn' as any, (e: any) => {
      this.logger.warn('Prisma warning', {
        message: e.message,
        target: e.target,
      })
    })
  }

  async connect() {
    try {
      this.logger.info('Connecting to database', {
        // Log redacted URLs for debugging
        url: config.database.url?.replace(/:[^:\/]+@/, ':****@'),
        directUrl: config.database.directUrl?.replace(/:[^:\/]+@/, ':****@'),
      })

      await this.$connect() // Use $connect from super
      this.isConnected = true
      this.logger.info('Database connected successfully')
    } catch (error: any) {
      this.logger.error('Database connection failed', {
        error,
        context: {
          stack: error.stack,
        },
      })
      throw error
    }
  }

  async disconnect(): Promise<void> {
    if (!this.isConnected) {
      return
    }

    try {
      await this.$disconnect() // Use $disconnect from super
      this.isConnected = false
      this.logger.info('Database disconnected successfully')
    } catch (error: any) {
      this.logger.error('Error disconnecting from database', error)
      throw error
    }
  }

  /**
   * Execute a database transaction with automatic retries
   */
  async transaction<T>(
    fn: (tx: Prisma.TransactionClient) => Promise<T>,
    options?: {
      maxRetries?: number
      timeout?: number
    },
  ): Promise<T> {
    const maxRetries = options?.maxRetries ?? 3
    const timeout = options?.timeout ?? 5000

    let attempt = 0
    while (attempt < maxRetries) {
      try {
        return await this.$transaction(fn, {
          timeout,
          maxWait: timeout * 2,
        })
      } catch (error: any) {
        attempt++
        const shouldRetry = attempt < maxRetries && this.isPrismaError(error)

        if (!shouldRetry) {
          throw error
        }

        this.logger.warn('Transaction failed, retrying...', {
          attempt,
          maxRetries,
          error,
        })

        // Wait before retrying with exponential backoff
        await new Promise((resolve) => setTimeout(resolve, 1000 * Math.pow(2, attempt - 1)))
      }
    }

    throw new Error('Transaction failed after max retries')
  }

  private isPrismaError(error: unknown): boolean {
    return error instanceof Error && error.constructor.name.startsWith('PrismaClient')
  }
}

================
File: src/core/services/queue.service.ts
================
// src/core/services/queue.service.ts
import PgBoss from 'pg-boss'
import pool from 'pg-pool'
import type { Pool } from 'pg'

import { CustomLogger } from './logger.service'
import { MetricsService } from './metrics.service'
import { PrismaService } from './prisma.service'
import type { WorkflowJob, WorkflowJobData } from '@types'

export interface QueueOptions {
  timeout?: number
  startAfter?: string
  singletonKey?: string
  priority?: number | 'low' | 'normal' | 'high' | 'critical'
  retryLimit?: number
  retryDelay?: number
  retryBackoff?: boolean
  expireInSeconds?: number
  expireInMinutes?: number
  expireInHours?: number
  retentionSeconds?: number
  retentionMinutes?: number
  retentionHours?: number
  retentionDays?: number
  deadLetter?: string
}

export type QueuePolicy = 'standard' | 'short' | 'singleton' | 'stately'

export interface Queue extends QueueOptions {
  name: string
  policy?: QueuePolicy
}

export interface StopOptions {
  wait?: boolean
  graceful?: boolean
  close?: boolean
  timeout?: number
}

export class QueueService {
  private boss: PgBoss | null = null
  private pgPool: Pool | null = null
  private isStarted = false
  private prisma: PrismaService

  constructor(
    private readonly connectionString: string,
    private readonly logger: CustomLogger,
    private readonly metrics: MetricsService,
  ) {
    this.logger.setDomain('jobs')
    this.prisma = new PrismaService(logger)
  }

  async start(): Promise<void> {
    if (!this.boss) {
      throw new Error('Queue service not initialized. Call init() first')
    }

    try {
      await this.boss?.start()
      this.isStarted = true
      this.logger.info('Queue service started successfully')
    } catch (error: any) {
      this.logger.error('Failed to start queue service', {
        error,
      })
      throw error
    }
  }

  // Modify your init() method to include monitoring
  async init(): Promise<void> {
    try {
      const isSupabase = this.connectionString.includes('supabase')
      // Initialize pg Pool
      this.pgPool = new pool({
        connectionString: this.connectionString,
        ssl: isSupabase
          ? {
              rejectUnauthorized: false,
            }
          : false, // Disable SSL for local development
        max: 10,
        idleTimeoutMillis: 30000,
        connectionTimeoutMillis: 5000,
      })

      // Test the connection
      await this.pgPool.query('SELECT 1')
      this.logger.info('Database pool initialized successfully')

      // Drop the pgboss schema if it exists
      await this.pgPool.query('DROP SCHEMA IF EXISTS pgboss CASCADE')
      this.logger.info('Cleaned up existing pgboss schema')

      this.boss = new PgBoss({
        db: {
          executeSql: async (text: string, values?: any[]) => {
            if (!this.pgPool) throw new Error('Database pool not initialized')
            return await this.pgPool.query(text, values)
          },
        },
        schema: 'pgboss',
        application_name: 'jobs_service',
        monitorStateIntervalMinutes: 1, // Check every minute
        archiveCompletedAfterSeconds: 43200,
        deleteAfterDays: 7,
        maintenanceIntervalMinutes: 5,
      })

      // Set up event handlers for monitoring
      this.boss.on('monitor-states', async (states) => {
        this.logger.info('Queue health check', {
          states,
          timestamp: new Date().toISOString(),
        })

        try {
          const stats = {
            created: Number(states.created) || 0,
            retry: Number(states.retry) || 0,
            active: Number(states.active) || 0,
            completed: Number(states.completed) || 0,
            cancelled: Number(states.cancelled) || 0,
            failed: Number(states.failed) || 0,
            all: Object.values(states).reduce(
              (sum: number, val: any) => sum + (Number(val) || 0),
              0,
            ),
          }

          await this.metrics.trackQueueStats('overall', stats)
        } catch (error: any) {
          this.logger.error('Failed to track queue stats', {
            error,
          })
        }
      })

      // Monitor active workers
      this.boss.on('wip', (workers) => {
        this.logger.debug('Active workers', {
          count: workers.length,
          workers: workers.map((w) => ({
            id: w.id,
            name: w.name,
            state: w.state,
            count: w.count,
            lastError: w.lastError,
          })),
          timestamp: new Date().toISOString(),
        })
      })

      await this.boss.start()
      this.logger.info('Queue service started successfully')
    } catch (error: any) {
      this.logger.error('Failed to initialize queue service', {
        error,
      })
      throw error
    }
  }

  // Add a method to get queue statistics on demand
  async getQueueStats(): Promise<Record<string, number>> {
    if (!this.boss) {
      throw new Error('Queue service not initialized')
    }

    // Use raw query to get stats since pg-boss doesn't expose direct API
    const query = `
      SELECT 
        count(*) FILTER (WHERE state = 'created') as created,
        count(*) FILTER (WHERE state = 'retry') as retry,
        count(*) FILTER (WHERE state = 'active') as active,
        count(*) FILTER (WHERE state = 'completed') as completed,
        count(*) FILTER (WHERE state = 'failed') as failed,
        count(*) FILTER (WHERE state = 'cancelled') as cancelled,
        count(*) as total
      FROM ${this.boss.constructor.name}.job
    `

    try {
      const result = await this.pgPool?.query(query)
      return (
        result?.rows[0] || {
          created: 0,
          retry: 0,
          active: 0,
          completed: 0,
          failed: 0,
          cancelled: 0,
          total: 0,
        }
      )
    } catch (error: any) {
      this.logger.error('Failed to get queue stats', { error })
      throw error
    }
  }

  private async trackJobMetrics(jobName: string, jobId: string, data: JobMetricsData) {
    try {
      await this.prisma.job_metrics.upsert({
        where: {
          job_id: jobId,
        },
        create: {
          job_id: jobId,
          job_name: jobName,
          started_at: new Date(),
          status: data.status,
          metadata: data.metadata || {},
          error_message: data.error_message,
          error_stack: data.error_stack,
          duration_ms: data.duration_ms,
          items_processed: data.items_processed,
        },
        update: {
          status: data.status,
          metadata: data.metadata,
          error_message: data.error_message,
          error_stack: data.error_stack,
          duration_ms: data.duration_ms,
          items_processed: data.items_processed,
          updated_at: new Date(),
          ...(data.status === 'completed' ? { completed_at: new Date() } : {}),
          ...(data.status === 'failed' ? { failed_at: new Date() } : {}),
        },
      })
    } catch (error) {
      this.logger.error('Failed to track job metrics', { error, jobName, jobId })
    }
  }

  async processJob(name: string, handler: (job: PgBoss.Job) => Promise<any>): Promise<void> {
    if (!this.boss) throw new Error('Queue service not started')

    try {
      await this.boss.work(name, async (job) => {
        const startTime = Date.now()

        try {
          // Track job start
          await this.trackJobMetrics(name, job.id, {
            status: 'active',
            started_at: new Date(),
          })

          // Process the job
          const result = await handler(job)

          // Track successful completion
          await this.trackJobMetrics(name, job.id, {
            status: 'completed',
            duration_ms: Date.now() - startTime,
            items_processed: Array.isArray(result) ? result.length : undefined,
            metadata: { result },
          })

          return { state: 'completed', result }
        } catch (error: any) {
          // Track failure
          await this.trackJobMetrics(name, job.id, {
            status: 'failed',
            duration_ms: Date.now() - startTime,
            error_message: error.message,
            error_stack: error.stack,
            metadata: { error: { message: error.message, stack: error.stack } },
          })

          throw error
        }
      })
    } catch (error: any) {
      this.logger.error(`Failed to register worker: ${name}`, { error })
      throw error
    }
  }

  async getJobStats(
    options: {
      jobName?: string
      status?: string
      startDate?: Date
      endDate?: Date
    } = {},
  ) {
    const { jobName, status, startDate, endDate } = options

    return this.prisma.job_metrics.groupBy({
      by: ['job_name', 'status'],
      where: {
        ...(jobName && { job_name: jobName }),
        ...(status && { status }),
        ...(startDate && { created_at: { gte: startDate } }),
        ...(endDate && { created_at: { lte: endDate } }),
      },
      _count: {
        _all: true,
      },
      _avg: {
        duration_ms: true,
        items_processed: true,
      },
    })
  }

  async getJobConfigs() {
    return this.prisma.job_configs.findMany({
      where: {
        enabled: true,
      },
    })
  }

  async updateQueueStats(queueName: string, stats: Record<string, number>) {
    await this.prisma.job_queue_stats.upsert({
      where: { queue_name: queueName },
      create: {
        queue_name: queueName,
        active_count: stats.active || 0,
        created_count: stats.created || 0,
        completed_count: stats.completed || 0,
        failed_count: stats.failed || 0,
        cancelled_count: stats.cancelled || 0,
        retry_count: stats.retry || 0,
        total_count: stats.total || 0,
      },
      update: {
        active_count: stats.active || 0,
        created_count: stats.created || 0,
        completed_count: stats.completed || 0,
        failed_count: stats.failed || 0,
        cancelled_count: stats.cancelled || 0,
        retry_count: stats.retry || 0,
        total_count: stats.total || 0,
        updated_at: new Date(),
      },
    })
  }

  // Add method to get job performance metrics
  async getJobPerformanceMetrics(jobName: string, period: 'hour' | 'day' | 'week' = 'day') {
    const startDate = new Date()
    switch (period) {
      case 'hour':
        startDate.setHours(startDate.getHours() - 1)
        break
      case 'week':
        startDate.setDate(startDate.getDate() - 7)
        break
      default:
        startDate.setDate(startDate.getDate() - 1)
    }

    return this.prisma.job_metrics.findMany({
      where: {
        job_name: jobName,
        created_at: { gte: startDate },
      },
      orderBy: { created_at: 'desc' },
      select: {
        status: true,
        duration_ms: true,
        items_processed: true,
        created_at: true,
        error_message: true,
      },
    })
  }

  // Add method to get stats for a specific queue
  async getQueueSize(name: string): Promise<number | null> {
    try {
      return (await this.boss?.getQueueSize(name)) || 0
    } catch (error: any) {
      this.logger.error(`Failed to get queue size for ${name}`, { error })
      return null
    }
  }

  async stop(options: StopOptions = {}): Promise<void> {
    try {
      if (this.boss) {
        await this.boss?.stop(options)
      }
      if (this.pgPool) {
        await this.pgPool.end()
      }
      this.isStarted = false
      this.logger.info('Queue service stopped')
    } catch (error: any) {
      this.logger.error('Failed to stop queue service', error)
      throw error
    }
  }

  // Update all other methods to check for this.boss existence
  private checkInitialized(): void {
    if (!this.boss) {
      throw new Error('Queue service not initialized. Call init() first')
    }
  }

  async clearStorage(): Promise<void> {
    try {
      this.checkInitialized()

      await this.boss?.clearStorage()
      this.logger.info('Successfully cleared queue storage')
    } catch (error: any) {
      this.logger.error('Failed to clear storage', error)
      throw error
    }
  }

  async createQueue(name: string, options: Queue): Promise<void> {
    try {
      this.checkInitialized()

      await this.boss?.createQueue(name, options)
      this.logger.info(`Queue created: ${name}`, { options })
    } catch (error: any) {
      this.logger.error(`Failed to create queue: ${name}`, {
        ...error,
        options,
      })
      throw error
    }
  }

  async updateQueue(name: string, options: PgBoss.Queue): Promise<void> {
    try {
      this.checkInitialized()

      await this.boss?.updateQueue(name, options)
      this.logger.info(`Queue updated: ${name}`, { options })
    } catch (error: any) {
      this.logger.error(`Failed to update queue: ${name}`, {
        ...error,
        options,
      })
      throw error
    }
  }

  async addJob(name: string, data: any, options: QueueOptions = {}) {
    if (!this.isStarted) throw new Error('Queue service not started')

    try {
      const jobId = await this.boss?.send(name, data, {
        ...options,
        priority: this.convertPriorityToInteger(options.priority),
        retryLimit: options.retryLimit ?? 3,
        retryDelay: options.retryDelay ?? 60,
        expireInSeconds: options.expireInSeconds ?? 3600,
      })

      if (!jobId) {
        this.logger.warn(`Job throttled: ${name}`)
        return null
      }

      this.logger.info(`Job added: ${name}`, { jobId, options })
      return jobId
    } catch (error: any) {
      this.logger.error(`Failed to add job: ${name}`, {
        ...error,
        options,
      })
      throw error
    }
  }

  async getSchedules() {
    if (!this.isStarted) {
      throw new Error('Queue service not started')
    }

    try {
      return await this.boss?.getSchedules()
    } catch (error: any) {
      this.logger.error('Failed to get schedules', {
        ...error,
      })
      throw error
    }
  }

  async unscheduleJob(name: string) {
    if (!this.isStarted) {
      throw new Error('Queue service not started')
    }

    try {
      await this.boss?.unschedule(name)
      this.logger.info(`Unscheduled job: ${name}`)
    } catch (error: any) {
      this.logger.error(`Failed to unschedule job: ${name}`, {
        ...error,
      })
      throw error
    }
  }

  async getWorkflowJob(name: string, jobId: string): Promise<WorkflowJob | null> {
    if (!this.isStarted) {
      throw new Error('Queue service not started')
    }

    try {
      const job = await this.boss?.getJobById(name, jobId)

      if (!job) {
        this.logger.warn('Job not found', {
          name,
          jobId,
        })
        return null
      }

      return {
        id: job.id,
        name: job.name,
        data: job.data as WorkflowJobData,
        priority: job.priority,
        state: job.state as WorkflowJob['state'],
        startedAt: job.startedOn,
        completedAt: job.completedOn ?? undefined,
        createdAt: job.createdOn,
      }
    } catch (error: any) {
      this.logger.error('Failed to get workflow job', {
        ...error,
        name,
        jobId,
      })
      throw error
    }
  }

  private convertPriorityToInteger(
    priority?: number | 'low' | 'normal' | 'high' | 'critical',
  ): number {
    if (typeof priority === 'number') {
      return priority
    }

    // Default priority mapping
    const priorityMap: Record<string, number> = {
      low: 0,
      normal: 100,
      high: 200,
      critical: 300,
    }

    return priorityMap[priority || 'normal'] || 100
  }

  async scheduleJob(name: string, cronSchedule: string, data: any, options: QueueOptions = {}) {
    if (!this.isStarted) throw new Error('Queue service not started')

    try {
      // First create the queue
      await this.boss?.createQueue(name)
      this.logger.info(`Created queue: ${name}`)

      // Then remove any existing schedules
      try {
        await this.boss?.unschedule(name)
        this.logger.info(`Unscheduled existing job: ${name}`)
      } catch (error) {
        // Ignore errors if schedule doesn't exist
        this.logger.debug(`No existing schedule found for: ${name}`)
      }

      // Now schedule the new job
      const jobId = await this.boss?.schedule(name, cronSchedule, data, {
        retryLimit: options.retryLimit ?? 3,
        retryDelay: 60,
        priority: this.convertPriorityToInteger(options.priority),
      })

      this.logger.info(`Scheduled job ${name}`, {
        jobId,
        cronSchedule,
        scheduleTime: new Date().toISOString(),
      })

      return jobId
    } catch (error: any) {
      this.logger.error(`Failed to schedule job ${name}`, {
        error,
      })
      throw error
    }
  }

  private async createQueueIfNotExists(name: string) {
    try {
      // First check if queue exists
      const queue = await this.boss?.getQueue(name)

      if (!queue) {
        this.logger.info(`Creating queue: ${name}`)
        try {
          await this.boss?.createQueue(name)
        } catch (error: any) {
          // If the error is "relation already exists" (code 42P07), we can ignore it
          // This happens in race conditions when multiple workers try to create the same queue
          if (error.code === '42P07') {
            this.logger.info(`Queue ${name} already exists, continuing...`)
            return
          }
          throw error
        }
      } else {
        this.logger.debug(`Queue ${name} already exists`)
      }
    } catch (error: any) {
      this.logger.error(`Failed to create queue: ${name}`, {
        error_code: error.code,
        error_message: error.message,
        error_detail: error.detail,
        error_hint: error.hint,
        ...error,
      })
      throw error
    }
  }

  async unschedule(name: string): Promise<void> {
    try {
      await this.boss?.unschedule(name)
      this.logger.info(`Unscheduled jobs for: ${name}`)
    } catch (error: any) {
      this.logger.error(`Failed to unschedule jobs: ${name}`, {
        ...error,
      })
      throw error
    }
  }

  async purgeQueue(name: string) {
    await this.boss?.purgeQueue(name)
    this.logger.info(`Queue purged: ${name}`)
  }

  async deleteQueue(name: string) {
    await this.boss?.deleteQueue(name)
    this.logger.info(`Queue deleted: ${name}`)
  }

  async getQueues() {
    return this.boss?.getQueues()
  }

  async getQueue(name: string) {
    return this.boss?.getQueue(name)
  }

  // Job management
  async completeJob(name: string, jobId: string, data?: any) {
    await this.boss?.complete(name, jobId, data)
    this.logger.info(`Job completed: ${name}`, { jobId })
  }

  async failJob(name: string, jobId: string, error: Error) {
    await this.boss?.fail(name, jobId, error)
    this.logger.info(`Job failed: ${name}`, { jobId, error })
  }

  async cancelJob(name: string, jobId: string) {
    await this.boss?.cancel(name, jobId)
    this.logger.info(`Job cancelled: ${name}`, { jobId })
  }

  async resumeJob(name: string, jobId: string) {
    await this.boss?.resume(name, jobId)
    this.logger.info(`Job resumed: ${name}`, { jobId })
  }

  async getJobById(name: string, id: string) {
    return this.boss?.getJobById(name, id)
  }

  async deleteJob(name: string, jobId: string) {
    await this.boss?.deleteJob(name, jobId)
    this.logger.info(`Job deleted: ${name}`, { jobId })
  }

  // Utility methods
  async isInstalled(): Promise<Boolean> {
    return this.boss?.isInstalled() ?? false
  }

  async getSchemaVersion(): Promise<Number> {
    return this.boss?.schemaVersion() ?? 0
  }

  private serializeData(data: any): any {
    try {
      this.logger.debug('Attempting to serialize data', {
        dataType: typeof data,
        isArray: Array.isArray(data),
        keys: data ? Object.keys(data) : null,
        sample: data ? JSON.stringify(data).slice(0, 100) + '...' : null,
      })

      // Wrap the data in an object
      const wrappedData = { data }
      const jsonString = JSON.stringify(wrappedData)
      const parsed = JSON.parse(jsonString)

      this.logger.debug('Data serialized successfully', {
        resultType: typeof parsed,
        isArray: Array.isArray(parsed),
        sample: JSON.stringify(parsed).slice(0, 100) + '...',
      })

      return parsed
    } catch (error: any) {
      this.logger.error('Failed to serialize job data', {
        ...error,
        dataType: typeof data,
        isArray: Array.isArray(data),
        keys: data ? Object.keys(data) : null,
      })
      return { data: {} }
    }
  }

  async testJob(name: string, data: any = {}, options: QueueOptions = {}) {
    if (!this.isStarted) throw new Error('Queue service not started')

    try {
      await this.createQueueIfNotExists(name)

      this.logger.info(`Testing job immediately: ${name}`, {
        options,
        dataSize: Array.isArray(data) ? data.length : 'N/A',
      })

      // Serialize the data to ensure it can be stored in PostgreSQL
      const serializedData = this.serializeData(data)

      const jobId = await this.boss?.send(name, serializedData, {
        retryLimit: options.retryLimit ?? 3,
        retryDelay: 60,
        priority: this.convertPriorityToInteger('critical'), // Use critical priority for test runs
        startAfter: new Date(), // Start immediately
      })

      this.logger.info(`Test job queued: ${name}`, {
        jobId,
        options,
      })

      return jobId
    } catch (error: any) {
      this.logger.error(`Failed to test job ${name}`, {
        ...error,
        options,
      })
      throw error
    }
  }
}

================
File: src/core/services/scraper.service.ts
================
// src/core/services/scraper.service.ts
import { chromium } from 'playwright'
import type { Browser, Page, LaunchOptions, BrowserContext } from 'playwright'
import { CustomLogger } from '@core'

const defaultLaunchOptions: LaunchOptions = {
  headless: true,
  args: [
    '--headless=new',
    '--no-sandbox',
    '--disable-setuid-sandbox',
    '--disable-extensions',
    '--disable-background-networking',
    '--disable-background-timer-throttling',
    '--disable-backgrounding-occluded-windows',
    '--disable-breakpad',
    '--disable-client-side-phishing-detection',
    '--disable-default-apps',
    '--disable-features=site-per-process',
    '--disable-hang-monitor',
    '--disable-ipc-flooding-protection',
    '--disable-prompt-on-repost',
    '--disable-renderer-backgrounding',
    '--disable-sync',
    '--metrics-recording-only',
    '--no-default-browser-check',
    '--password-store=basic',
    '--use-mock-keychain',
    '--js-flags=--max-old-space-size=4096',
    '--max-old-space-size=4096',
  ],
  timeout: 60000, // Increased timeout
}

export class ScraperService {
  private browser: Browser | null = null
  private contexts: Map<string, BrowserContext> = new Map()
  private pages: Map<string, Page> = new Map()
  private readonly DEFAULT_TIMEOUT = 60000
  private isInitializing = false
  private initPromise: Promise<void> | null = null

  constructor(
    private readonly logger: CustomLogger,
    private readonly options: LaunchOptions = {},
  ) {
    this.logger.setDomain('scraper')
  }

  async init(): Promise<void> {
    // Prevent multiple simultaneous initialization attempts
    if (this.initPromise) {
      return this.initPromise
    }

    if (this.browser) {
      this.logger.debug('Browser already initialized')
      return
    }

    this.isInitializing = true
    this.initPromise = this._init()

    try {
      await this.initPromise
    } finally {
      this.isInitializing = false
      this.initPromise = null
    }
  }

  private async _init(): Promise<void> {
    try {
      this.logger.info('Starting browser initialization')

      const mergedOptions = {
        ...defaultLaunchOptions,
        ...this.options,
      }

      this.browser = await chromium.launch(mergedOptions)

      // Set up error handlers
      this.browser.on('disconnected', () => {
        this.logger.error('Browser disconnected unexpectedly', {})
        this.browser = null
      })

      this.logger.info('Browser initialized successfully', {
        options: mergedOptions,
      })
    } catch (error: any) {
      this.browser = null
      this.logger.error('Failed to initialize browser', {
        error: error.message,
        context: this.options,
      })
      throw error
    }
  }

  async getContext(id: string): Promise<BrowserContext> {
    if (!this.contexts.has(id)) {
      try {
        if (!this.browser) {
          await this.init()
        }

        const context = await this.browser!.newContext({
          locale: 'en-US',
          extraHTTPHeaders: {
            'Accept-Language': 'en-US,en;q=0.9',
          },
          userAgent:
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
          javaScriptEnabled: true,
          bypassCSP: true,
          viewport: { width: 1920, height: 1080 },
          deviceScaleFactor: 1,
        })

        // Only load necessary resources
        await context.route('**/*', (route) => {
          const resourceType = route.request().resourceType()
          if (resourceType === 'document' || resourceType === 'script') {
            return route.continue()
          }
          return route.abort()
        })

        this.contexts.set(id, context)
        this.logger.debug(`Created new context: ${id}`)
      } catch (error: any) {
        this.logger.error(`Failed to create context: ${id}`, { error })
        throw error
      }
    }
    return this.contexts.get(id)!
  }

  async newPage(contextId = 'default'): Promise<Page> {
    try {
      const context = await this.getContext(contextId)
      const page = await context.newPage()

      // Set up page configurations
      await this.configureNewPage(page)

      this.pages.set(`${contextId}:${page.url()}`, page)
      return page
    } catch (error: any) {
      this.logger.error('Failed to create new page', { error })
      throw error
    }
  }

  private async configureNewPage(page: Page): Promise<void> {
    // Set default timeout
    page.setDefaultTimeout(this.DEFAULT_TIMEOUT)

    // Error handling
    page.on('pageerror', (error) => {
      this.logger.error('Page error occurred', { error })
    })

    page.on('console', (msg) => {
      if (msg.type() === 'error') {
        this.logger.error('Console error from page', { context: { message: msg.text() } })
      }
    })

    page.on('crash', () => {
      this.logger.error('Page crashed', { context: { url: page.url() } })
    })

    // Set viewport and headers
    await page.setViewportSize({ width: 1920, height: 1080 })
    await page.setExtraHTTPHeaders({
      'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0)',
      'Accept-Language': 'en-US,en;q=0.9',
    })
  }

  async navigateWithRetry(page: Page, url: string, maxRetries = 3): Promise<void> {
    let lastError

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        const response = await page.goto(url, {
          waitUntil: 'domcontentloaded',
          timeout: this.DEFAULT_TIMEOUT,
        })

        if (!response) {
          throw new Error('Navigation resulted in null response')
        }

        if (!response.ok()) {
          throw new Error(`Navigation failed with status ${response.status()}`)
        }

        return
      } catch (error: any) {
        lastError = error
        this.logger.warn(`Navigation attempt ${attempt} failed`, {
          url,
          error: error.message,
          attempt,
          maxRetries,
        })

        if (attempt === maxRetries) break

        // Exponential backoff
        await new Promise((resolve) =>
          setTimeout(resolve, Math.min(1000 * Math.pow(2, attempt - 1), 10000)),
        )
      }
    }

    throw lastError
  }

  async cleanup(): Promise<void> {
    try {
      // Close all contexts first
      for (const [id, context] of this.contexts) {
        try {
          await context.close()
          this.contexts.delete(id)
        } catch (error: any) {
          this.logger.warn(`Failed to close context ${id}`, { error })
        }
      }

      // Then close the browser
      if (this.browser) {
        await this.browser.close()
        this.browser = null
      }

      this.pages.clear()
      this.logger.info('Cleanup completed successfully')
    } catch (error: any) {
      this.logger.error('Error during cleanup', { error })
      throw error
    }
  }

  async isHealthy(): Promise<boolean> {
    if (!this.browser || this.isInitializing) {
      return false
    }

    try {
      const context = await this.getContext('health-check')
      const page = await context.newPage()
      await page.close()
      await context.close()
      return true
    } catch (error: any) {
      this.logger.error('Health check failed', { error })
      return false
    }
  }

  getStats(): { contexts: number; pages: number } {
    return {
      contexts: this.contexts.size,
      pages: this.pages.size,
    }
  }
}

================
File: src/core/services/workflow.service.ts
================
// src/infrastructure/workflow/workflow.service.ts
import { QueueService, CustomLogger } from '@core'
import { JobConfig, WorkflowJob } from '@types'

export class WorkflowService {
  constructor(
    private readonly queueService: QueueService,
    private readonly logger: CustomLogger,
  ) {}

  async createWorkflow(options: {
    name: string
    jobs: JobConfig<any, any, any>[]
    onComplete?: (result: any) => Promise<void>
    onFail?: (error: Error) => Promise<void>
  }) {
    const { name, jobs, onComplete, onFail } = options

    try {
      // Create parent job
      const workflowId = await this.queueService.addJob(name, {
        jobs: jobs.map((j) => j.name),
      })

      // Schedule child jobs with dependencies
      for (const job of jobs) {
        await this.queueService.addJob(
          job.name,
          {
            ...job,
            workflowId,
          },
          {
            startAfter: `$completed:${workflowId}`, // pg-boss dependency syntax
          },
        )
      }

      // Handle completion/failure
      if (onComplete) {
        await this.queueService.processJob(`${name}:complete`, async (job: any) => {
          await onComplete(job.data)
        })
      }

      if (onFail) {
        await this.queueService.processJob(`${name}:fail`, async (job: any) => {
          await onFail(job.data.error)
        })
      }

      return workflowId
    } catch (error: any) {
      this.logger.error(`Failed to create workflow ${name}`, error)
      throw error
    }
  }

  async getWorkflowStatus(workflowId: string, name: string) {
    try {
      const job = (await this.queueService.getWorkflowJob(name, workflowId)) as WorkflowJob

      return {
        id: workflowId,
        status: job.state ?? 'pending',
        progress: job.data?.progress ?? 0,
        childJobs: job.data?.jobs ?? [],
      }
    } catch (error: any) {
      this.logger.error(`Failed to get workflow status`, {
        ...error,
        workflowId,
        name,
      })
      throw error
    }
  }
}

================
File: src/core/index.ts
================
export { CustomLogger } from './services/logger.service'
export { MetricsService } from './services/metrics.service'
export { QueueService } from './services/queue.service'
export { ScraperService } from './services/scraper.service'
export { CircuitBreakerService } from './services/circuit-breaker.service'
export { WorkflowService } from './services/workflow.service'
export { EventService } from './services/event.service'
export { PrismaService } from './services/prisma.service'
export * from './extractors'

================
File: src/core/shutdown.service.ts
================
// src/core/services/shutdown.service.ts
import { JobServices } from '@types'

export class ShutdownService {
  private isShuttingDown = false

  constructor(private readonly services: JobServices) {
    this.services.logger.setDomain('job_shutdown')
    this.setupShutdownHandlers()
  }

  private setupShutdownHandlers() {
    process.on('SIGTERM', () => this.handleShutdown('SIGTERM'))
    process.on('SIGINT', () => this.handleShutdown('SIGINT'))
    process.on('unhandledRejection', (reason) => {
      this.services.logger.error('Unhandled Promise rejection', { context: { message: String(reason) } })
    })
    process.on('uncaughtException', (error: any) => {
      this.services.logger.error('Uncaught exception', error)
      void this.handleShutdown('UNCAUGHT_EXCEPTION')
    })
  }

  private async handleShutdown(signal: string) {
    if (this.isShuttingDown) {
      this.services.logger.info('Shutdown already in progress')
      return
    }

    this.isShuttingDown = true
    this.services.logger.info(`Received ${signal}. Starting graceful shutdown...`)

    try {
      // Stop accepting new jobs
      this.services.logger.info('Stopping job queue...')
      await this.services.queue.stop()

      // Wait for active jobs to complete (with timeout)
      this.services.logger.info('Waiting for active jobs to complete...')
      await this.waitForActiveJobs()

      // Cleanup other services
      this.services.logger.info('Cleaning up services...')
      await Promise.allSettled([
        this.services.scraper.cleanup(),
        this.services.prisma.$disconnect(),
        this.services.metrics.flush(),
      ])

      this.services.logger.info('Graceful shutdown completed')
      process.exit(0)
    } catch (error: any) {
      this.services.logger.error('Error during shutdown', error)
      process.exit(1)
    }
  }

  private async waitForActiveJobs(timeout: number = 30000): Promise<void> {
    const startTime = Date.now()

    // while (Date.now() - startTime < timeout) {
    //   const activeJobs = await this.services.queue.getActiveJobs()

    //   if (activeJobs.length === 0) {
    //     return
    //   }

    //   this.services.logger.info(`Waiting for ${activeJobs.length} active jobs...`)
    //   await new Promise((resolve) => setTimeout(resolve, 1000))
    // }

    this.services.logger.warn('Timeout waiting for active jobs')
  }
}

================
File: src/helpers/files.helpers.ts
================
import path from 'path'
import fs from 'fs'
import { constants } from 'fs'
import fsPromise from 'fs/promises'

interface FilePath {
  baseDir?: string
  dir: string
  file: string
  extension?: string
}

export function getFilePath({
  baseDir = './logs',
  dir,
  file,
  extension = '.json',
}: FilePath): string {
  return path.resolve(process.cwd(), baseDir, dir, `${file}${extension}`)
}

export function readJSONFile(filePath: string): any {
  if (fs.existsSync(filePath)) {
    const rawJSON = fs.readFileSync(filePath, 'utf-8')
    return JSON.parse(rawJSON).flat()
  } else {
    console.warn(`File not found: ${filePath}`)
    return null
  }
}

export async function ensureDirectoryExists(dirPath: string): Promise<void> {
  try {
    await fsPromise.access(dirPath, constants.F_OK)
  } catch (error: any) {
    if (error.code === 'ENOENT') {
      try {
        await fsPromise.mkdir(dirPath, { recursive: true })
        console.log(`Created directory: ${dirPath}`)
      } catch (mkdirError) {
        console.error(`Error creating directory ${dirPath}:`, mkdirError)
        throw mkdirError
      }
    } else {
      console.error(`Error checking directory ${dirPath}:`, error)
      throw error
    }
  }
}

================
File: src/helpers/hashing.helpers.ts
================
import murmurhash from 'murmurhash'

// Example function to hash content
export function hashContent(content: string): number {
  return murmurhash.v3(content)
}

================
File: src/helpers/index.ts
================
export * from './logging.helpers'
export * from './files.helpers'
export * from './hashing.helpers'

================
File: src/helpers/logging.helpers.ts
================
import fs from 'fs'
import path from 'path'
import { config } from '../config'

export function logFile(fileName: string, newValues: any, isReplaced: boolean = false) {
  if (config.environment !== 'development') {
    return
  }

  let data: any[] = []
  const filePath = path.join('logs', `${fileName}.json`)

  if (!newValues) {
    return
  }

  if (Array.isArray(newValues) && !newValues.length) {
    return
  }

  if (!Array.isArray(newValues)) {
    newValues = [newValues]
  }

  // Extract directory from the file path and ensure it exists
  const dir = path.dirname(filePath)
  if (!fs.existsSync(dir)) {
    fs.mkdirSync(dir, { recursive: true })
  }

  // Read existing data if file exists
  if (fs.existsSync(filePath)) {
    try {
      data = JSON.parse(fs.readFileSync(filePath, 'utf8'))
    } catch (error: any) {
      console.error('Error parsing JSON file:', error)
    }

    if (isReplaced) {
      data = newValues
    } else {
      data.push(...newValues)
    }
  } else {
    data = newValues
  }

  // Write the updated data to the file
  fs.writeFileSync(filePath, JSON.stringify(data, null, 2))
}

================
File: src/jobs/config/news/news-links.config.ts
================
// src/jobs/news/news-links.job.ts
import { DomainsForService, Service } from '@ib/logger'
import { NewsLinkExtractor } from '../../utils/link-extractor'
import type { JobServices, ContentSource, JobConfig } from '@types'
import { DatabaseUtils } from '../../utils/database.utils'
import { JobFactory } from '../../job.factory'

interface ProcessedContent {
  contents: {
    url: string
    title: string
    description: string
    content: string
    published_at: Date
  }
  news: {
    url: string
    title: string
  }
}

interface OutputContent {
  contents: {
    url: string
    title: string
    description: string
    content: string
    published_at: Date
  }
}

export const createNewsLinksJob = (services: JobServices) => {
  return JobFactory.createJob<ContentSource, ProcessedContent, OutputContent>({
    name: 'news_links',
    domain: 'news_links',
    version: '1.2.0',
    changes: ['Added support for new sources'],
    schedule: {
      customCron: '*/15 * * * *', // Every 15 minutes
      type: 'cron',
      enabled: true,
    },
    priority: 'high',
    timeout: 600000,
    handlers: {
      beforeProcess: async () => {
        const { prisma, logger } = services
        try {
          const sources = await prisma.content_sources.findMany({
            where: {
              content_type: 'news',
              OR: [
                { refreshed_at: null },
                {
                  refreshed_at: {
                    lt: DatabaseUtils.timestampOperations.subtractHours(new Date(), 1),
                  },
                },
              ],
              has_failed: false,
              failed_count: { lt: 3 },
            },
            take: 50,
          })
          logger.info(`Fetched ${sources.length} sources`)
          return DatabaseUtils.convertBigIntToNumber(sources)
        } catch (error: any) {
          logger.error('Failed to fetch sources', { error })
          throw error
        }
      },
      processFunction: async (sources: ContentSource[], job): Promise<ProcessedContent[]> => {
        const { scraper, logger, prisma, metrics } = services
        const results: ProcessedContent[] = []

        logger.info(`Processing ${sources.length} sources`)

        // Use batch processing
        await DatabaseUtils.batchProcess({
          items: sources,
          batchSize: 10, // Process 10 sources at a time
          processor: async (batch) => {
            const startTime = Date.now()
            const batchResults = [] as string[]
            let browserInitialized = false

            for (const source of batch) {
              try {
                if (!source.rss_urls?.length && !browserInitialized) {
                  await scraper.init()
                  browserInitialized = true
                }

                const extractor = source.rss_urls?.length
                  ? new NewsLinkExtractor(null, source, { logger, prisma })
                  : new NewsLinkExtractor(await scraper.newPage(), source, { logger, prisma })

                const links = await extractor.extractBlogLinks()
                batchResults.push(...links)

                await metrics.trackJobCompletion('news_links', job.id, {
                  itemsProcessed: links.length,
                  success: true,
                  duration: Date.now() - startTime,
                })
              } catch (error) {
                await DatabaseUtils.handleEntityFailure(
                  prisma,
                  logger,
                  'content_sources',
                  { id: source.id.toString() },
                  error as Error,
                  { maxFailures: 3, disableAfterFailure: true },
                )
              }
            }
            return batchResults
          },
          logger,
        })

        return results
      },
      afterProcess: async (results: ProcessedContent[]): Promise<OutputContent[]> => {
        const { prisma, logger } = services

        console.log(`Processed ${results.length} results`)
        if (!results.length) return []

        try {
          return await DatabaseUtils.executeTransaction({
            prisma,
            logger,
            operation: async (tx) => {
              // Create contents with safe upsert
              const contents = await DatabaseUtils.safeUpsert({
                prisma: tx,
                logger,
                table: 'contents',
                where: { urls: results.map((r) => r.contents.url) },
                create: results.map((r) => ({
                  ...r.contents,
                  created_at: DatabaseUtils.timestampOperations.nowUtc(),
                })),
                update: results.map((r) => ({
                  ...r.contents,
                  updated_at: DatabaseUtils.timestampOperations.nowUtc(),
                })),
              })

              // Rest of transaction logic...
              return results
            },
            maxRetries: 3,
            retryDelay: 1000,
          })
        } catch (error: any) {
          logger.error('Failed to process results', {
            error,
            context: { resultsCount: results.length },
          })
          throw error
        }
      },
      onError: async (error: Error): Promise<void> => {
        const { logger } = services
        logger.error(`News links job v1.2.0 failed`, {
          error,
        })
      },
    },
  })
}

================
File: src/jobs/config/news/news-page.scraper.ts
================
// NewsContentScraper.ts
import type { Page } from 'playwright'
import { CustomLogger, ExtractorModule } from '@core'
import TurndownService from 'turndown'
import { hashContent } from '@helpers'

interface NewsPageData {
  id: string
  url: string
  failed_count: number
  published_at: string | null
  description: string
  author: string | null
  title: string
  keywords: string[]
  featured_image: string | null
  body?: string
  hash?: number
  scraped_at?: string
  company_id?: string
}

export class NewsContentScraper {
  private readonly turndownService: TurndownService
  private readonly dataExtractor: ExtractorModule
  private readonly selectors = [
    'article',
    '[role="article"]',
    '.post-content',
    '.article-body',
    '#main-content',
    '.main-content',
    'main',
    '.entry-content',
    '#content',
    '.content',
    '.post',
    '.article',
  ]

  constructor(private readonly logger: CustomLogger) {
    this.turndownService = new TurndownService()
    this.dataExtractor = new ExtractorModule(logger)
  }

  async extractContent(page: Page, source: NewsPageData): Promise<NewsPageData> {
    try {
      const html = await page.content()
      const articleHTML = await this.extractArticleContent(page)

      if (!articleHTML || (await this.isDifferentDomain(page, source.url))) {
        this.logger.warn(`No content extracted from ${source.url}`)
        return {
          ...source,
          failed_count: source.failed_count + 1,
        }
      }

      const body = this.turndownService.turndown(articleHTML)
      const publishedAt = source.published_at || this.dataExtractor.extractPublishedDate(html)
      const published_at = publishedAt?.toISOString() ?? publishedAt ?? null
      const title = source.title || this.dataExtractor.extractTitle(html) || ''
      const author = source.author || this.dataExtractor.extractAuthor(html)
      const featured_image =
        source.featured_image || this.dataExtractor.extractFeaturedImage(html, page.url())
      const keywords = source.keywords || this.dataExtractor.extractKeywords(html)

      return {
        ...source,
        body,
        ...(source.title ? {} : { title }),
        ...(source.description ? {} : { description: '' }),
        ...(source.featured_image ? {} : { featured_image }),
        ...(source.author ? {} : { author }),
        ...(source.keywords ? {} : { keywords }),
        ...(source.published_at ? {} : { published_at }),
        hash: hashContent(body),
        scraped_at: new Date().toISOString(),
        failed_count: 0,
      }
    } catch (error: any) {
      this.logger.error('Failed to extract content', { error, context: { url: source.url } })
      return {
        ...source,
        failed_count: source.failed_count + 1,
      }
    }
  }

  private async extractArticleContent(page: Page): Promise<string> {
    for (const selector of this.selectors) {
      try {
        const locator = page.locator(selector)
        const count = await locator.count()

        if (count > 0) {
          return await locator.first().innerHTML()
        }
      } catch (error) {
        this.logger.warn(`Error with selector ${selector}`, { error })
      }
    }

    // Fallback to body content analysis
    try {
      const content = await page.evaluate(() => {
        const possibleContentNodes = Array.from(document.body.children).filter((el) => {
          const text = el.textContent || ''
          return text.length > 500 && text.split(' ').length > 100
        })
        return possibleContentNodes.length > 0
          ? possibleContentNodes.map((node) => node.innerHTML).join('')
          : document.body.innerHTML
      })
      return content
    } catch (error: any) {
      this.logger.error('Error extracting content', { error })
      return ''
    }
  }

  private async isDifferentDomain(page: Page, sourceUrl: string): Promise<boolean> {
    return new URL(page.url()).host !== new URL(sourceUrl).host
  }
}

================
File: src/jobs/config/news/news-pages.config.ts
================
import TurndownService from 'turndown'
import { JobFactory } from '../../job.factory'
import { logFile } from '@helpers'
import { DatabaseUtils } from '../../utils/database.utils'
import { NewsContentScraper } from './news-page.scraper'
import { CustomLogger } from '@core'
import type { JobServices } from '@types'
import type { Page } from 'playwright'

interface NewsPageData {
  id: string
  url: string
  failed_count: number
  published_at: string | null
  description: string
  author: string | null
  title: string
  keywords: string[]
  featured_image: string | null
  body?: string
  hash?: number
  scraped_at?: string
  company_id?: string
}

export const createNewsPagesJob = (services: JobServices) => {
  const { logger, prisma } = services
  const newsScraper = new NewsContentScraper(logger)

  return JobFactory.createJob<NewsPageData, NewsPageData, NewsPageData>({
    name: 'news_pages',
    domain: 'news',
    version: '1.0.0',
    changes: ['Initial version'],
    schedule: {
      customCron: '*/20 * * * *',
      type: 'cron',
      enabled: true,
    },
    priority: 'high',
    timeout: 600000,
    handlers: {
      beforeProcess: async () => {
        try {
          const pages = await prisma.news.findMany({
            where: {
              body: null,
              failed_count: {
                lt: 3,
              },
            },
            select: {
              id: true,
              url: true,
              failed_count: true,
              published_at: true,
              description: true,
              author: true,
              title: true,
              keywords: true,
              featured_image: true,
              company_id: true,
            },
            take: 100,
          })

          return DatabaseUtils.convertBigIntToNumber(pages)
        } catch (error: any) {
          logger.error('Failed to fetch news pages', { error })
          throw error
        }
      },

      processFunction: async (pages: NewsPageData[]): Promise<NewsPageData[]> => {
        const { scraper, logger } = services
        const results: NewsPageData[] = []

        await DatabaseUtils.batchProcess({
          items: pages,
          batchSize: 10,
          processor: async (batch) => {
            try {
              const page = await scraper.newPage()
              page.setDefaultTimeout(60000)

              const batchResults = []
              for (const source of batch) {
                try {
                  await page.goto(source.url, { waitUntil: 'networkidle' })

                  const result = await newsScraper.extractContent(page, source)
                  batchResults.push(result)
                } catch (error: any) {
                  logger.error('Failed to scrape page', { error, url: source.url })
                  batchResults.push({
                    ...source,
                    failed_count: source.failed_count + 1,
                  })
                }
              }

              await page.close()
              results.push(...batchResults)
              return batchResults
            } catch (error: any) {
              logger.error('Batch processing failed', { error })
              return batch.map((source) => ({
                ...source,
                failed_count: source.failed_count + 1,
              }))
            }
          },
          logger,
        })

        return results
      },

      afterProcess: async (processedData: NewsPageData[]): Promise<NewsPageData[]> => {
        const { prisma, logger } = services

        logFile(
          'news_pages',
          {
            data: processedData,
          },
          true,
        )

        // Filter out failed scrapes
        processedData = processedData.filter((data) => data.body && data.failed_count === 0)

        if (processedData.length === 0) {
          logger.warn('No news pages to store')
          return []
        }

        try {
          const results = await DatabaseUtils.safeUpsert({
            prisma,
            logger,
            table: 'news',
            where: {
              urls: processedData.map((p) => p.url),
            },
            create: processedData.map((page) => ({
              ...page,
              created_at: DatabaseUtils.timestampOperations.nowUtc(),
            })),
            update: processedData.map((page) => ({
              ...page,
              updated_at: DatabaseUtils.timestampOperations.nowUtc(),
            })),
          })

          logger.info(`Stored ${results.length} news pages`)
          return results
        } catch (error: any) {
          logger.error('Failed to store news pages', {
            error,
            context: { pagesCount: processedData.length },
          })
          throw error
        }
      },

      onError: async (error: Error): Promise<void> => {
        const { logger } = services
        logger.error(`News pages job failed`, {
          error,
        })
      },
    },
  })
}

================
File: src/jobs/config/news/news-summary.config.ts
================
import { JobServices, NewsArticle } from '@types'
import { DatabaseUtils } from '../../utils/database.utils'
import { JobFactory } from '../../job.factory'
import { createClient, Agent, summarizerConfig } from '@agents'

interface ProcessedContent {
  news_id: string
  summary: string
}

interface OutputContent {
  news_id: string
  summary: string
  has_summary: boolean
}

const summarizerAgent = new Agent(
  {
    openAI: createClient.openAI(),
    groq: createClient.groq(),
    db: createClient.database(),
  },
  summarizerConfig,
)

export const createNewsSummarizerJob = (services: JobServices) => {
  return JobFactory.createJob<NewsArticle, ProcessedContent, OutputContent>({
    name: 'news_summary',
    domain: 'news',
    version: '1.0.0',
    changes: ['Initial version'],
    schedule: {
      customCron: '0 */1 * * *', // Every hour
      type: 'cron',
      enabled: true,
    },
    priority: 'low',
    timeout: 3600000, // 1 hour
    handlers: {
      beforeProcess: async () => {
        const { prisma, logger } = services
        try {
          const articles = await prisma.news.findMany({
            where: {
              has_summary: false,
              body: { not: null },
            },
            select: {
              id: true,
              title: true,
              body: true,
              author: true,
              description: true,
              url: true,
              published_at: true,
              content_status: true,
            },
            orderBy: {
              published_at: 'desc',
            },
            take: 10,
          })
          return DatabaseUtils.convertBigIntToNumber(articles)
        } catch (error: any) {
          logger.error('Failed to fetch articles', { error })
          throw error
        }
      },
      processFunction: async (articles: NewsArticle[], job): Promise<ProcessedContent[]> => {
        const { logger, metrics } = services
        const results: ProcessedContent[] = []

        await DatabaseUtils.batchProcess({
          items: articles,
          batchSize: 5,
          processor: async (batch) => {
            const startTime = Date.now()

            for (const article of batch) {
              try {
                logger.info(`Processing article: ${article.id}`)

                const result = await summarizerAgent.execute({
                  data: {
                    title: article.title || '',
                    author: article.author || 'Unknown',
                    published_at: article.published_at
                      ? new Date(article.published_at).toISOString()
                      : undefined,
                    body: article.body || '',
                  },
                })

                results.push({
                  news_id: article.id.toString(),
                  summary: result.result,
                })

                await metrics.trackJobCompletion('news_summary', job.id, {
                  itemsProcessed: 1,
                  success: true,
                  duration: Date.now() - startTime,
                })
              } catch (error) {
                await DatabaseUtils.handleEntityFailure(
                  services.prisma,
                  logger,
                  'news',
                  { id: article.id.toString() },
                  error as Error,
                  { maxFailures: 3, disableAfterFailure: true },
                )
              }
            }
            return results
          },
          logger,
        })

        return results
      },
      afterProcess: async (results: ProcessedContent[]): Promise<OutputContent[]> => {
        const { prisma, logger } = services

        if (!results.length) return []

        try {
          return await DatabaseUtils.executeTransaction({
            prisma,
            logger,
            operation: async (tx) => {
              // Create summaries with safe upsert
              const summaries = await DatabaseUtils.safeUpsert({
                prisma: tx,
                logger,
                table: 'news_summaries',
                where: { news_ids: results.map((r) => r.news_id) },
                create: results.map((r) => ({
                  news_id: r.news_id,
                  summary: r.summary,
                  created_at: DatabaseUtils.timestampOperations.nowUtc(),
                })),
                update: results.map((r) => ({
                  news_id: r.news_id,
                  summary: r.summary,
                  updated_at: DatabaseUtils.timestampOperations.nowUtc(),
                })),
              })

              // Update news articles to mark as summarized
              await tx.news.updateMany({
                where: { id: { in: results.map((r) => r.news_id) } },
                data: { has_summary: true },
              })

              return results.map((r) => ({
                ...r,
                has_summary: true,
              }))
            },
            maxRetries: 3,
            retryDelay: 1000,
          })
        } catch (error: any) {
          logger.error('Failed to process results', {
            error,
            context: { resultsCount: results.length },
          })
          throw error
        }
      },
      onError: async (error: Error): Promise<void> => {
        const { logger } = services
        logger.error(`News summarizer job v1.0.0 failed`, {
          error,
        })
      },
    },
  })
}

================
File: src/jobs/config/news/news.module.ts
================
// src/jobs/modules/news/news-links.module.ts
import { JobModule } from '@types'
import { createNewsLinksJob } from './news-links.config'
import { createNewsPagesJob } from './news-pages.config'

export const newsJobModules: JobModule[] = [
  {
    name: 'news_links',
    createJob: createNewsLinksJob,
  },
  {
    name: 'news_pages',
    createJob: createNewsPagesJob,
  },
]

================
File: src/jobs/config/test/test.config.ts
================
import { JobConfig, JobServices } from '@types'
import { faker } from '@faker-js/faker'
// src/jobs/modules/test/types.ts
export interface TestJobInput {
  batchId: string
  items: number[]
}

export interface TestJobProcessed {
  processedItems: number[]
  processingTime: number
  status: 'success' | 'partial' | 'failed'
}

export interface TestJobOutput {
  totalProcessed: number
  averageProcessingTime: number
  successRate: number
}

export interface TestJobOptions {
  name: string
  scheduleEnabled?: boolean
  scheduleType?: 'interval' | 'daily' | 'weekly' | 'monthly' | 'custom'
  customCron?: string
  simulateErrors?: boolean
}

export const createTestJob = (
  services: JobServices,
  options: TestJobOptions,
): JobConfig<TestJobInput, TestJobProcessed, TestJobOutput> => {
  const { logger } = services

  return {
    name: options.name,
    domain: 'test',
    version: '1.0.0',
    changes: ['Initial test job implementation'],
    priority: 'normal',
    batchSize: 10,
    processSize: 2,
    timeout: 5000,
    retryLimit: 2,
    schedule: {
      type: options.scheduleType || 'interval',
      customCron: options.customCron || '*/15 * * * *',
      enabled: options.scheduleEnabled ?? false,
    },
    circuitBreaker: {
      enabled: true,
      failureThreshold: 5,
      resetTimeout: 60000,
    },
    handlers: {
      beforeProcess: async (): Promise<TestJobInput[]> => {
        logger.info(`[${options.name}] Preparing test data`)
        return [
          {
            batchId: faker.string.uuid(),
            items: Array.from({ length: 5 }, () => faker.number.int({ min: 1, max: 100 })),
          },
        ]
      },
      processFunction: async (input: any[], job: any) => {
        logger.info(`[${options.name}] Processing items`, { jobId: job.id })

        const startTime = Date.now()
        await new Promise((resolve) =>
          setTimeout(resolve, faker.number.int({ min: 100, max: 1000 })),
        )

        if (options.simulateErrors && faker.number.int({ min: 1, max: 10 }) === 1) {
          throw new Error('Simulated random processing error')
        }

        const processedItems = input.map((item) => {
          if (faker.number.int({ min: 1, max: 10 }) > 8) {
            return 0 // Simulate failed item
          }
          return item * 2
        })

        const status = processedItems.every((item) => item > 0)
          ? 'success'
          : processedItems.some((item) => item > 0)
            ? 'partial'
            : 'failed'

        return {
          processedItems,
          processingTime: Date.now() - startTime,
          status,
        }
      },
      afterProcess: async (processed) => {
        logger.info(`[${options.name}] Finalizing processing`, {
          status: processed.status,
          processingTime: processed.processingTime,
        })

        const successfulItems = processed.processedItems.filter((item) => item > 0)

        return {
          totalProcessed: processed.processedItems.length,
          averageProcessingTime: processed.processingTime / processed.processedItems.length,
          successRate: (successfulItems.length / processed.processedItems.length) * 100,
        }
      },
      onError: async (error) => {
        logger.error(`[${options.name}] Error during execution`, { error })
      },
    },
  }
}

export const testModules: JobModule[] = [
  {
    name: 'test_frequent',
    createJob: (services) =>
      createTestJob(services, {
        name: 'test_frequent_job',
        scheduleEnabled: true,
        scheduleType: 'interval',
        customCron: '*/5 * * * *',
        simulateErrors: true,
      }),
  },
  {
    name: 'test_daily',
    createJob: (services) =>
      createTestJob(services, {
        name: 'test_daily_job',
        scheduleEnabled: true,
        scheduleType: 'daily',
        customCron: '0 9 * * *',
      }),
  },
  {
    name: 'test_weekly',
    createJob: (services) =>
      createTestJob(services, {
        name: 'test_weekly_job',
        scheduleEnabled: true,
        scheduleType: 'weekly',
        customCron: '0 12 * * MON',
      }),
  },
]

export const monitoringTestModule: JobModule = {
  name: 'monitoring_test',
  createJob: (services) =>
    createTestJob(services, {
      name: 'monitoring_test_job',
      scheduleEnabled: true,
      scheduleType: 'interval',
      customCron: '*/30 * * * *',
      simulateErrors: false,
    }),
}

================
File: src/jobs/config/test/test.module.ts
================
// src/jobs/modules/test/test-job.module.ts
import { JobModule } from '../../registry/types'
import { createTestJob } from './test-job.config'

export const testJobModule: JobModule = {
  name: 'test_job',
  createJob: (services) =>
    createTestJob(services, {
      name: 'test_job',
      scheduleEnabled: true,
      scheduleType: 'interval',
    }),
}

================
File: src/jobs/utils/data/models/urlClassifier/v1/model.json
================
"{\"class_name\":\"Sequential\",\"config\":{\"name\":\"sequential_10\",\"layers\":[{\"class_name\":\"Dense\",\"config\":{\"units\":64,\"activation\":\"relu\",\"use_bias\":true,\"kernel_initializer\":{\"class_name\":\"VarianceScaling\",\"config\":{\"scale\":1,\"mode\":\"fan_avg\",\"distribution\":\"normal\",\"seed\":null}},\"bias_initializer\":{\"class_name\":\"Zeros\",\"config\":{}},\"kernel_regularizer\":null,\"bias_regularizer\":null,\"activity_regularizer\":null,\"kernel_constraint\":null,\"bias_constraint\":null,\"name\":\"dense_Dense19\",\"trainable\":true,\"batch_input_shape\":[null,10],\"dtype\":\"float32\"}},{\"class_name\":\"Dense\",\"config\":{\"units\":3,\"activation\":\"softmax\",\"use_bias\":true,\"kernel_initializer\":{\"class_name\":\"VarianceScaling\",\"config\":{\"scale\":1,\"mode\":\"fan_avg\",\"distribution\":\"normal\",\"seed\":null}},\"bias_initializer\":{\"class_name\":\"Zeros\",\"config\":{}},\"kernel_regularizer\":null,\"bias_regularizer\":null,\"activity_regularizer\":null,\"kernel_constraint\":null,\"bias_constraint\":null,\"name\":\"dense_Dense20\",\"trainable\":true}}]},\"keras_version\":\"tfjs-layers 4.22.0\",\"backend\":\"tensor_flow.js\"}"

================
File: src/jobs/utils/data/models/urlClassifier/v1/training_history.json
================
{"validationData":null,"params":{"epochs":50,"initialEpoch":0,"samples":39,"steps":null,"batchSize":32,"verbose":0,"doValidation":true,"metrics":["loss","acc","val_loss","val_acc"]},"epoch":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],"history":{"val_loss":[11.034064292907715,10.411727905273438,9.794331550598145,9.202860832214355,8.330820083618164,7.312217712402344,6.30165958404541,5.100286483764648,3.566236972808838,2.7788894176483154,1.8173062801361084,0.8371250033378601,0.7401024103164673,0.977665901184082,1.2056407928466797,1.3753517866134644,1.4759997129440308,1.5028138160705566,1.469077706336975,1.3956968784332275,1.2846680879592896,1.140233039855957,1.002740740776062,0.8783232569694519,0.7810198068618774,0.7468193769454956,0.7531147599220276,0.7430559992790222,0.7232837677001953,0.7297134399414062,0.7612743973731995,0.7885775566101074,0.7954137921333313,0.780433177947998,0.7433080673217773,0.7010256052017212,0.6681287288665771,0.6425289511680603,0.6250119805335999,0.6129297018051147,0.6075425148010254,0.6097908616065979,0.6104400157928467,0.6063761115074158,0.5962927937507629,0.5770865678787231,0.552054762840271,0.5334329605102539,0.5205374360084534,0.5131162405014038],"val_acc":[0.10000000149011612,0.10000000149011612,0.10000000149011612,0.10000000149011612,0.10000000149011612,0.10000000149011612,0.10000000149011612,0.20000000298023224,0.5,0.5,0.4000000059604645,0.5,0.5,0.5,0.4000000059604645,0.4000000059604645,0.4000000059604645,0.4000000059604645,0.4000000059604645,0.4000000059604645,0.4000000059604645,0.4000000059604645,0.5,0.5,0.6000000238418579,0.8999999761581421,0.8999999761581421,0.8999999761581421,0.8999999761581421,0.800000011920929,0.6000000238418579,0.6000000238418579,0.6000000238418579,0.6000000238418579,0.6000000238418579,0.699999988079071,0.800000011920929,0.8999999761581421,0.8999999761581421,0.8999999761581421,0.8999999761581421,0.800000011920929,0.800000011920929,0.800000011920929,0.800000011920929,0.800000011920929,0.8999999761581421,0.8999999761581421,0.8999999761581421,0.8999999761581421],"loss":[11.6749849319458,11.240277290344238,10.795098304748535,10.371268272399902,9.8113374710083,9.197123527526855,8.20435619354248,6.927697658538818,5.375192642211914,4.131179332733154,3.079396963119507,1.995697259902954,0.976349949836731,0.7299487590789795,0.8721734881401062,1.0380666255950928,1.162807583808899,1.234425663948059,1.2415666580200195,1.2113896608352661,1.1401302814483643,1.0564011335372925,0.950808584690094,0.8206111192703247,0.7414582371711731,0.6985064744949341,0.7183488607406616,0.7590596079826355,0.7376965880393982,0.6770844459533691,0.6585312485694885,0.6729787588119507,0.6871654987335205,0.6913189888000488,0.6793088912963867,0.6570583581924438,0.6277555823326111,0.6110382676124573,0.6071707010269165,0.6010602116584778,0.5896164774894714,0.5789355039596558,0.5720043778419495,0.5735223293304443,0.5695644617080688,0.5648413300514221,0.5530920624732971,0.542087733745575,0.5351809859275818,0.5405049324035645],"acc":[0.05128205195069313,0.05128205195069313,0.05128205195069313,0.05128205195069313,0.05128205195069313,0.05128205195069313,0.05128205195069313,0.07692307978868484,0.1794871836900711,0.23076923191547394,0.3589743673801422,0.43589743971824646,0.5384615659713745,0.6666666865348816,0.5128205418586731,0.4871794879436493,0.4871794879436493,0.4871794879436493,0.4871794879436493,0.4871794879436493,0.4871794879436493,0.4871794879436493,0.5641025900840759,0.6666666865348816,0.7692307829856873,0.8461538553237915,0.8461538553237915,0.8205128312110901,0.8205128312110901,0.8717948794364929,0.8717948794364929,0.8205128312110901,0.8205128312110901,0.8205128312110901,0.8205128312110901,0.8461538553237915,0.8717948794364929,0.8717948794364929,0.8717948794364929,0.8974359035491943,0.8974359035491943,0.8717948794364929,0.8717948794364929,0.8717948794364929,0.8717948794364929,0.8717948794364929,0.8717948794364929,0.8717948794364929,0.8974359035491943,0.8974359035491943]}}

================
File: src/jobs/utils/data/featuresConfig.json
================
[
  {
    "type": "PathLengthFeature",
    "featureName": "path_length",
    "weight": 1.0
  },
  {
    "type": "NumberOfSlashesFeature",
    "featureName": "num_slashes",
    "weight": 1.0
  },
  {
    "type": "ContainsNumbersFeature",
    "featureName": "contains_numbers",
    "weight": 1.0
  },
  {
    "type": "ContainsKeywordFeature",
    "featureName": "contains_article_keywords",
    "keywords": ["blog", "news", "article", "post", "story"],
    "weight": 1.0
  },
  {
    "type": "AverageSegmentLengthFeature",
    "featureName": "avg_segment_length",
    "weight": 1.0
  },
  {
    "type": "EndsWithExtensionFeature",
    "featureName": "ends_with_extension",
    "weight": 1.0
  },
  {
    "type": "ContainsKeywordFeature",
    "featureName": "contains_non_article_indicators",
    "keywords": [
      "page",
      "archive",
      "feed",
      "event",
      "contact",
      "about",
      "issues",
      "picture-of-the-day",
      "what-we-do",
      "resources",
      "all-places",
      "products-companies",
      "education-and-outreach",
      "our-positions",
      "causes",
      "guides-and-how-tos"
    ],
    "weight": 1.0
  },
  {
    "type": "EndsWithDatePatternFeature",
    "featureName": "ends_with_date_pattern",
    "weight": 1.0
  },
  {
    "type": "HyphensInLastSegmentFeature",
    "featureName": "hyphens_in_last_segment",
    "weight": 1.0
  },
  {
    "type": "LastSegmentLengthFeature",
    "featureName": "last_segment_length",
    "weight": 1.0
  }
]

================
File: src/jobs/utils/data/trainingData.json
================
[
  {
    "url": "https://spacenews.com/polaris-dawn-astronauts-perform-spacewalk",
    "label": "news"
  },
  {
    "url": "https://www.space.com/event-horizon-black-hole-image-tests",
    "label": "news"
  },
  {
    "url": "https://www.nasa.gov/news-release/nasa-ames-selects-aeronautics-and-exploration-support-contractor",
    "label": "news"
  },
  {
    "url": "https://www.space.com/spacex-ast-spacemobile-bluebird-launch-september-2024",
    "label": "news"
  },
  {
    "url": "https://www.company.com/careers/software-engineer-position",
    "label": "jobs"
  },
  {
    "url": "https://www.company.com/jobs/data-scientist",
    "label": "jobs"
  },
  {
    "url": "https://www.example.com/about-us",
    "label": "unknown"
  },
  {
    "url": "https://www.example.com/contact",
    "label": "unknown"
  },
  {
    "url": "https://www.example.com/blog/welcome-to-our-blog",
    "label": "news"
  },
  {
    "url": "https://www.example.com/events/upcoming-conference-2024",
    "label": "unknown"
  },
  {
    "url": "https://www.company.com/careers/apply-now",
    "label": "jobs"
  },
  {
    "url": "https://www.nasa.gov/image-article/waxing-gibbous-moon-over-minnesota",
    "label": "news"
  },
  {
    "url": "https://www.seti.org/press-release/seti-institute-starts-first-low-frequency-search-alien-technology-distant-galaxies",
    "label": "news"
  },
  {
    "url": "https://earthsky.org/space/does-the-supermoon-have-a-super-effect-on-us",
    "label": "news"
  },
  {
    "url": "https://earthsky.org/human-world/chinese-rocket-accidentally-blasts-off-then-crashes",
    "label": "news"
  },
  {
    "url": "https://www.astronomy.com/science/could-we-experience-the-first-ever-human-made-meteor-shower",
    "label": "news"
  },
  {
    "url": "https://www.astronomy.com/observing/annular-eclipse-shot-wins-image-of-the-year",
    "label": "news"
  },
  {
    "url": "https://www.nasa.gov/technology/tech-transfer-spinoffs/printed-engines-propel-the-next-industrial-revolution",
    "label": "news"
  },
  {
    "url": "https://www.nasa.gov/directorates/stmd/prizes-challenges-crowdsourcing-program/center-of-excellence-for-collaborative-innovation-coeci/nasa-moon-to-mars-architecture-art-challenge",
    "label": "news"
  },
  {
    "url": "https://science.nasa.gov/missions/webb/nasas-webb-peers-into-the-extreme-outer-galaxy",
    "label": "news"
  },
  {
    "url": "https://www.space.com/26021-best-binoculars.html",
    "label": "news"
  },
  {
    "url": "https://www.space.com/entertainment/space-movies-shows/this-week-in-space-podcast-this-week-in-space-podcast-episode-127-space-stations-inc",
    "label": "news"
  },
  {
    "url": "https://www.space.com/space-exploration/asteroid-comet-missions/meet-phaethon-a-weird-asteroid-that-thinks-its-a-comet-our-new-research-may-explain-whats-going-on",
    "label": "news"
  },
  {
    "url": "https://www.planetary.org/articles/why-nasa-does-space-science-and-not-the-private-sector",
    "label": "news"
  },
  {
    "url": "https://skyandtelescope.org/astronomy-news/black-holes/scientists-unveil-first-black-hole-image",
    "label": "news"
  },
  {
    "url": "https://skyandtelescope.org/astronomy-news/observing-news/this-weeks-sky-at-a-glance-september-13-22",
    "label": "news"
  },
  {
    "url": "https://www.seti.org/press-release/seti-institute-starts-first-low-frequency-search-alien-technology-distant-galaxies",
    "label": "news"
  },
  {
    "url": "https://skyandtelescope.org/astronomy-blogs/black-hole-files/event-horizon-telescope-pushes-toward-sharper-images",
    "label": "news"
  },
  {
    "url": "https://earthsky.org/space/black-hole-duo-will-merge-in-a-distant-galaxy",
    "label": "news"
  },
  {
    "url": "https://www.astronomy.com/science/stars",
    "label": "unknown"
  },
  {
    "url": "https://skyandtelescope.org/astronomy-news/galaxies",
    "label": "unknown"
  },
  {
    "url": "https://skyandtelescope.org/author/kgilchrist",
    "label": "people"
  },
  {
    "url": "https://darksky.org/who-we-are/staff",
    "label": "people"
  },
  {
    "url": "https://darksky.org/who-we-are/board-of-directors",
    "label": "people"
  },
  {
    "url": "https://earthsky.org/author/paul-scott-anderson",
    "label": "people"
  },
  {
    "url": "https://earthsky.org/author/kellywhitt",
    "label": "people"
  },
  {
    "url": "https://darksky.org/what-we-do/research",
    "label": "research"
  },
  {
    "url": "https://spacenews.com/event/space-apps-seattle",
    "label": "events"
  },
  {
    "url": "https://www.space.com/archive/2024/05",
    "label": "unknown"
  },
  {
    "url": "https://www.astronomy.com/tags/news/page/3",
    "label": "unknown"
  },
  {
    "url": "http://www.collectspace.com/news/news-2021archive.html",
    "label": "unknown"
  },
  {
    "url": "https://spacenews.com/event/world-space-business-week-2024-bringing-together-the-worlds-industry-leaders",
    "label": "events"
  },
  {
    "url": "https://darksky.org/what-we-do/events/darksky-awards",
    "label": "events"
  },
  {
    "url": "https://www.nasa.gov/events",
    "label": "events_source"
  },
  {
    "url": "https://darksky.org/what-we-do/events",
    "label": "events_source"
  },
  {
    "url": "https://www.seti.org/events",
    "label": "events_source"
  },
  {
    "url": "https://darksky.org/news",
    "label": "news_source"
  },
  {
    "url": "https://darksky.org/about/careers",
    "label": "jobs_source"
  },
  {
    "url": "https://www.seti.org/jobs",
    "label": "jobs_source"
  }
]

================
File: src/jobs/utils/features/feature.factory.ts
================
import * as feat from './features'

export class FeatureFactory {
  static createFeature(config: any): feat.FeatureExtractor {
    switch (config.type) {
      case 'PathLengthFeature':
        return new feat.PathLengthFeature(config.weight)
      case 'NumberOfSlashesFeature':
        return new feat.NumberOfSlashesFeature(config.weight)
      case 'ContainsNumbersFeature':
        return new feat.ContainsNumbersFeature(config.weight)
      case 'ContainsKeywordFeature':
        return new feat.ContainsKeywordFeature(config.featureName, config.keywords, config.weight)
      case 'AverageSegmentLengthFeature':
        return new feat.AverageSegmentLengthFeature(config.weight)
      case 'EndsWithExtensionFeature':
        return new feat.EndsWithExtensionFeature(config.weight)
      case 'EndsWithDatePatternFeature':
        return new feat.EndsWithDatePatternFeature(config.weight)
      case 'HyphensInLastSegmentFeature':
        return new feat.HyphensInLastSegmentFeature(config.weight)
      case 'LastSegmentLengthFeature':
        return new feat.LastSegmentLengthFeature(config.weight)
      default:
        throw new Error(`Unknown feature type: ${config.type}`)
    }
  }
}

================
File: src/jobs/utils/features/feature.pipeline.ts
================
import { FeatureExtractor } from './features'
import { FeatureFactory } from './feature.factory'
import * as fs from 'fs'

export class FeaturePipeline {
  private extractors: FeatureExtractor[] = []

  constructor(configFilePath: string) {
    this.loadFeaturesFromConfig(configFilePath)
  }

  private loadFeaturesFromConfig(configFilePath: string): void {
    const configData = fs.readFileSync(configFilePath, 'utf-8')
    const featureConfigs = JSON.parse(configData)

    featureConfigs.forEach((config: any) => {
      const extractor = FeatureFactory.createFeature(config)
      this.extractors.push(extractor)
    })
  }

  extractFeatures(url: string): { [key: string]: number } {
    const features: { [key: string]: number } = {}
    this.extractors.forEach((extractor) => {
      const value = extractor.extract(url)
      features[extractor.featureName] = Number(value) * extractor.weight
    })
    return features
  }

  getFeatureNames(): string[] {
    return this.extractors.map((extractor) => extractor.featureName)
  }
}

================
File: src/jobs/utils/features/features.ts
================
export interface FeatureExtractor {
  extract(url: string): number
  featureName: string
  weight: number
}

export class PathLengthFeature implements FeatureExtractor {
  featureName = 'path_length'
  weight: number

  constructor(weight: number) {
    this.weight = weight
  }

  extract(url: string): number {
    const pathname = new URL(url).pathname
    return pathname.length
  }
}

export class NumberOfSlashesFeature implements FeatureExtractor {
  featureName = 'num_slashes'
  weight: number

  constructor(weight: number) {
    this.weight = weight
  }

  extract(url: string): number {
    const pathname = new URL(url).pathname
    return (pathname.match(/\//g) || []).length
  }
}

export class ContainsNumbersFeature implements FeatureExtractor {
  featureName = 'contains_numbers'
  weight: number

  constructor(weight: number) {
    this.weight = weight
  }

  extract(url: string): number {
    const pathname = new URL(url).pathname
    return /\d/.test(pathname) ? 1 : 0
  }
}

export class ContainsKeywordFeature implements FeatureExtractor {
  featureName: string
  weight: number
  private keywords: string[]

  constructor(featureName: string, keywords: string[], weight: number) {
    this.featureName = featureName
    this.keywords = keywords
    this.weight = weight
  }

  extract(url: string): number {
    const pathname = new URL(url).pathname.toLowerCase()
    return this.keywords.some((keyword) => pathname.includes(keyword)) ? 1 : 0
  }
}

export class AverageSegmentLengthFeature implements FeatureExtractor {
  featureName = 'avg_segment_length'
  weight: number

  constructor(weight: number) {
    this.weight = weight
  }

  extract(url: string): number {
    const pathname = new URL(url).pathname
    const segments = pathname.split('/').filter((segment) => segment.length > 0)
    return segments.length > 0
      ? segments.reduce((sum, segment) => sum + segment.length, 0) / segments.length
      : 0
  }
}

export class EndsWithExtensionFeature implements FeatureExtractor {
  featureName = 'ends_with_extension'
  weight: number

  constructor(weight: number) {
    this.weight = weight
  }

  extract(url: string): number {
    const pathname = new URL(url).pathname
    return /\.\w{2,4}$/.test(pathname) ? 1 : 0
  }
}

export class EndsWithDatePatternFeature implements FeatureExtractor {
  featureName = 'ends_with_date_pattern'
  weight: number

  constructor(weight: number) {
    this.weight = weight
  }

  extract(url: string): number {
    const pathname = new URL(url).pathname
    return /\/\d{4}(\/\d{2})?(\/)?$/.test(pathname) ? 1 : 0
  }
}

export class HyphensInLastSegmentFeature implements FeatureExtractor {
  featureName = 'hyphens_in_last_segment'
  weight: number

  constructor(weight: number) {
    this.weight = weight
  }

  extract(url: string): number {
    const pathname = new URL(url).pathname
    const segments = pathname.split('/').filter((segment) => segment.length > 0)
    const lastSegment = segments[segments.length - 1] || ''
    return (lastSegment.match(/-/g) || []).length
  }
}

export class LastSegmentLengthFeature implements FeatureExtractor {
  featureName = 'last_segment_length'
  weight: number

  constructor(weight: number) {
    this.weight = weight
  }

  extract(url: string): number {
    const pathname = new URL(url).pathname
    const segments = pathname.split('/').filter((segment) => segment.length > 0)
    const lastSegment = segments[segments.length - 1] || ''
    return lastSegment.length
  }
}

================
File: src/jobs/utils/content-stash.ts
================
export class ContentStash {
  private validBlogUrls: Map<string, Set<string>> = new Map()
  private allInternalUrls: Map<string, Set<string>> = new Map()
  private selectedUrls: Map<string, Set<string>> = new Map()
  private paginationState: Map<string, number> = new Map()

  private debugMode: boolean = false

  constructor(debugMode: boolean = false) {
    this.debugMode = debugMode
  }

  private log(message: string): void {
    if (this.debugMode) {
      console.log(`[ContentStash] ${message}`)
    }
  }

  setDebugMode(enabled: boolean): void {
    this.debugMode = enabled
    this.log(`Debug mode ${enabled ? 'enabled' : 'disabled'}`)
  }

  addValidBlogUrl(domain: string, url: string): boolean {
    if (!this.validBlogUrls.has(domain)) {
      this.validBlogUrls.set(domain, new Set())
    }
    const urlSet = this.validBlogUrls.get(domain)!
    if (!urlSet.has(url)) {
      urlSet.add(url)
      this.log(`Added valid blog URL for ${domain}: ${url}`)
      return true
    }
    this.log(`URL already exists as valid blog URL for ${domain}: ${url}`)
    return false
  }

  addInternalUrl(domain: string, url: string): void {
    if (!this.allInternalUrls.has(domain)) {
      this.allInternalUrls.set(domain, new Set())
    }
    this.allInternalUrls.get(domain)!.add(url)
    this.log(`Added internal URL for ${domain}: ${url}`)
  }

  isKnownInternalUrl(domain: string, url: string): boolean {
    const isKnown = this.allInternalUrls.get(domain)?.has(url) || false
    this.log(`Checking if ${url} is known for ${domain}: ${isKnown}`)
    return isKnown
  }

  addSelectedUrl(domain: string, url: string): void {
    if (!this.selectedUrls.has(domain)) {
      this.selectedUrls.set(domain, new Set())
    }
    this.selectedUrls.get(domain)!.add(url)
    this.log(`Added selected URL for ${domain}: ${url}`)
  }

  isSelectedUrl(domain: string, url: string): boolean {
    const isSelected = this.selectedUrls.get(domain)?.has(url) || false
    this.log(`Checking if ${url} is selected for ${domain}: ${isSelected}`)
    return isSelected
  }

  incrementPage(domain: string): void {
    const currentPage = this.paginationState.get(domain) || 1
    this.paginationState.set(domain, currentPage + 1)
    this.log(`Incremented page for ${domain} to ${currentPage + 1}`)
  }

  getCurrentPage(domain: string): number {
    const currentPage = this.paginationState.get(domain) || 1
    this.log(`Current page for ${domain}: ${currentPage}`)
    return currentPage
  }

  clearDomainData(domain: string): void {
    this.validBlogUrls.delete(domain)
    this.allInternalUrls.delete(domain)
    this.selectedUrls.delete(domain)
    this.paginationState.delete(domain)
    this.log(`Cleared all data for domain: ${domain}`)
  }

  flushAll(): void {
    this.validBlogUrls.clear()
    this.allInternalUrls.clear()
    this.selectedUrls.clear()
    this.paginationState.clear()
    this.log('Flushed all content stash data')
  }

  getDomainStats(domain: string): {
    validBlogUrls: number
    allInternalUrls: number
    selectedUrls: number
    currentPage: number
  } {
    const stats = {
      validBlogUrls: this.validBlogUrls.get(domain)?.size || 0,
      allInternalUrls: this.allInternalUrls.get(domain)?.size || 0,
      selectedUrls: this.selectedUrls.get(domain)?.size || 0,
      currentPage: this.paginationState.get(domain) || 1,
    }
    this.log(`Domain stats for ${domain}: ${JSON.stringify(stats)}`)
    return stats
  }

  getAllStats(): {
    domains: string[]
    totalValidBlogUrls: number
    totalInternalUrls: number
    totalSelectedUrls: number
  } {
    const domains = Array.from(
      new Set([
        ...this.validBlogUrls.keys(),
        ...this.allInternalUrls.keys(),
        ...this.selectedUrls.keys(),
      ]),
    )

    const stats = {
      domains,
      totalValidBlogUrls: Array.from(this.validBlogUrls.values()).reduce(
        (sum, set) => sum + set.size,
        0,
      ),
      totalInternalUrls: Array.from(this.allInternalUrls.values()).reduce(
        (sum, set) => sum + set.size,
        0,
      ),
      totalSelectedUrls: Array.from(this.selectedUrls.values()).reduce(
        (sum, set) => sum + set.size,
        0,
      ),
    }
    this.log(`Overall stats: ${JSON.stringify(stats)}`)
    return stats
  }
}

export const globalContentStash = new ContentStash()

================
File: src/jobs/utils/database.utils.ts
================
// src/utils/db-utils.ts
import { PrismaClient } from '@prisma/client'
import type { CustomLogger } from '@core'

const prismaTables = {
  ad_daily_metrics: PrismaClient.prototype.ad_daily_metrics,
  ad_packages: PrismaClient.prototype.ad_packages,
  ad_variants: PrismaClient.prototype.ad_variants,
  addresses: PrismaClient.prototype.addresses,
  ads: PrismaClient.prototype.ads,
  blacklisted_domains: PrismaClient.prototype.blacklisted_domains,
  blacklisted_urls: PrismaClient.prototype.blacklisted_urls,
  blocked_ips: PrismaClient.prototype.blocked_ips,
  bookmark_folders: PrismaClient.prototype.bookmark_folders,
  bookmarks: PrismaClient.prototype.bookmarks,
  categories: PrismaClient.prototype.categories,
  cities: PrismaClient.prototype.cities,
  classified_urls: PrismaClient.prototype.classified_urls,
  comments: PrismaClient.prototype.comments,
  companies: PrismaClient.prototype.companies,
  company_contacts: PrismaClient.prototype.company_contacts,
  company_employees: PrismaClient.prototype.company_employees,
  company_extras: PrismaClient.prototype.company_extras,
  company_metrics: PrismaClient.prototype.company_metrics,
  company_urls: PrismaClient.prototype.company_urls,
  contacts: PrismaClient.prototype.contacts,
  content_categories: PrismaClient.prototype.content_categories,
  content_source_visits: PrismaClient.prototype.content_source_visits,
  content_sources: PrismaClient.prototype.content_sources,
  content_statuses: PrismaClient.prototype.content_statuses,
  content_tags: PrismaClient.prototype.content_tags,
  contents: PrismaClient.prototype.contents,
  countries: PrismaClient.prototype.countries,
  customer_payments: PrismaClient.prototype.customer_payments,
  customer_processed_webhooks: PrismaClient.prototype.customer_processed_webhooks,
  customer_refunds: PrismaClient.prototype.customer_refunds,
  customer_subscription_plans: PrismaClient.prototype.customer_subscription_plans,
  customer_subscriptions: PrismaClient.prototype.customer_subscriptions,
  embedding_reviews: PrismaClient.prototype.embedding_reviews,
  error_logs: PrismaClient.prototype.error_logs,
  feature_requests: PrismaClient.prototype.feature_requests,
  feature_votes: PrismaClient.prototype.feature_votes,
  feed_categories: PrismaClient.prototype.feed_categories,
  feed_sources: PrismaClient.prototype.feed_sources,
  feedbacks: PrismaClient.prototype.feedbacks,
  feeds: PrismaClient.prototype.feeds,
  follows: PrismaClient.prototype.follows,
  metric_definitions: PrismaClient.prototype.metric_definitions,
  news: PrismaClient.prototype.news,
  news_summaries: PrismaClient.prototype.news_summaries,
  news_tags: PrismaClient.prototype.news_tags,
  newsletters: PrismaClient.prototype.newsletters,
  payment_providers: PrismaClient.prototype.payment_providers,
  plan_permissions: PrismaClient.prototype.plan_permissions,
  referrals: PrismaClient.prototype.referrals,
  referrer_blocks: PrismaClient.prototype.referrer_blocks,
  research: PrismaClient.prototype.research,
  research_embeddings: PrismaClient.prototype.research_embeddings,
  responses: PrismaClient.prototype.responses,
  role_hierarchy: PrismaClient.prototype.role_hierarchy,
  role_permissions: PrismaClient.prototype.role_permissions,
  role_permissions_materialized: PrismaClient.prototype.role_permissions_materialized,
  scoring_weights: PrismaClient.prototype.scoring_weights,
  searches: PrismaClient.prototype.searches,
  social_media: PrismaClient.prototype.social_media,
  spider_metrics: PrismaClient.prototype.spider_metrics,
  strapi_migrations: PrismaClient.prototype.strapi_migrations,
  strapi_migrations_internal: PrismaClient.prototype.strapi_migrations_internal,
  table_maintenance_log: PrismaClient.prototype.table_maintenance_log,
  // security_metrics: PrismaClient.prototype.security_metrics,
  // table_query_performance: PrismaClient.prototype.table_query_performance,
  // error_metrics: PrismaClient.prototype.error_metrics,
  // table_sequence_usage: PrismaClient.prototype.table_sequence_usage,
  table_statistics: PrismaClient.prototype.table_statistics,
  tags: PrismaClient.prototype.tags,
  user_metrics: PrismaClient.prototype.user_metrics,
  user_profiles: PrismaClient.prototype.user_profiles,
  votes: PrismaClient.prototype.votes,
} as const

export type PrismaTables = typeof prismaTables
export type PrismaTableNames = keyof typeof prismaTables

type PrismaDelegate = {
  update: (args: { where: { id: number | string }; data: any }) => Promise<any>
  upsert: (args: { where: { id: number | string }; create: any; update: any }) => Promise<any>
  findUnique: (args: { where: { id: number | string }; select?: any }) => Promise<any>
}

export function getPrismaDelegate<T extends PrismaTableNames>(
  prisma: PrismaClient,
  table: T,
): PrismaDelegate {
  return prisma[table] as unknown as PrismaDelegate
}

export class DatabaseUtils {
  /**
   * Converts BigInt values to numbers recursively throughout an object/array
   * Useful when working with ID fields from PostgreSQL
   */
  static convertBigIntToNumber(obj: any): any {
    if (obj === null || obj === undefined) return obj
    if (typeof obj === 'bigint') return Number(obj)
    if (Array.isArray(obj)) {
      return obj.map((item) => DatabaseUtils.convertBigIntToNumber(item))
    }
    if (typeof obj === 'object') {
      const newObj: any = {}
      for (const key in obj) {
        newObj[key] = DatabaseUtils.convertBigIntToNumber(obj[key])
      }
      return newObj
    }
    return obj
  }

  /**
   * Handles incrementing failure count for any source/entity
   */
  static async handleEntityFailure<T extends { id: number | string }>(
    prisma: PrismaClient,
    logger: CustomLogger,
    table: PrismaTableNames,
    entity: T,
    error: Error,
    options?: {
      maxFailures?: number
      disableAfterFailure?: boolean
    },
  ) {
    try {
      const delegate = getPrismaDelegate(prisma, table)

      await delegate.update({
        where: { id: entity.id },
        data: {
          has_failed: true,
          failed_count: { increment: 1 },
        },
      })

      // Check max failures if specified
      if (options?.maxFailures) {
        const updatedEntity = await delegate.findUnique({
          where: { id: entity.id },
          select: { failed_count: true },
        })

        if (updatedEntity?.failed_count >= options.maxFailures) {
          // Instead of using is_active, we'll just keep has_failed as true
          // and use a high failed_count to indicate disabled state
          await delegate.update({
            where: { id: entity.id },
            data: {
              has_failed: true,
              updated_at: new Date(),
            },
          })
        }
      }
    } catch (updateError: any) {
      logger.error(`Failed to update ${String(table)} failure count`, {
        error: updateError,
        context: { entityId: entity.id },
      })
    }
  }

  /**
   * Safely handles PostgreSQL-specific JSON operations
   */
  static jsonOperations = {
    arrayToJsonb: (arr: any[]) => JSON.stringify(arr),
    objectToJsonb: (obj: object) => JSON.stringify(obj),
    mergePgJsonb: (existing: any, update: any) => ({
      ...existing,
      ...update,
    }),
  }

  /**
   * Handles PostgreSQL-specific timestamp operations
   */
  static timestampOperations = {
    nowUtc: () => new Date().toISOString(),
    addHours: (date: Date, hours: number) => new Date(date.getTime() + hours * 60 * 60 * 1000),
    subtractHours: (date: Date, hours: number) => new Date(date.getTime() - hours * 60 * 60 * 1000),
    startOfDay: (date: Date) => new Date(date.setHours(0, 0, 0, 0)),
  }

  /**
   * Handles batch operations with proper chunking
   */
  static async batchProcess<T, R>({
    items,
    batchSize = 20,
    processor,
    logger,
  }: {
    items: T[]
    batchSize?: number
    processor: (batch: T[]) => Promise<R[]>
    logger: CustomLogger
  }): Promise<R[]> {
    const results: R[] = []
    for (let i = 0; i < items.length; i += batchSize) {
      const batch = items.slice(i, i + batchSize)
      try {
        const batchResults = await processor(batch)
        results.push(...batchResults)
      } catch (error: any) {
        logger.error('Batch processing failed', {
          error,
          context: { batchIndex: i, batchSize },
        })
      }
    }
    return results
  }

  /**
   * Handles safe transaction execution with retries
   */
  static async executeTransaction<T>({
    prisma,
    logger,
    operation,
    maxRetries = 3,
    retryDelay = 1000,
  }: {
    prisma: PrismaClient
    logger: CustomLogger
    operation: (tx: PrismaClient) => Promise<T>
    maxRetries?: number
    retryDelay?: number
  }): Promise<T> {
    let attempts = 0
    while (attempts < maxRetries) {
      try {
        return await prisma.$transaction(async (tx: any) => {
          return await operation(tx)
        })
      } catch (error: any) {
        attempts++
        logger.error('Transaction failed', {
          error,
          context: { attempt: attempts, maxRetries },
        })

        if (attempts === maxRetries) throw error
        await new Promise((resolve) => setTimeout(resolve, retryDelay))
      }
    }
    throw new Error('Transaction failed after max retries')
  }

  /**
   * Upsert helper with proper type handling
   */
  static async safeUpsert<T>({
    prisma,
    logger,
    table,
    where,
    create,
    update,
  }: {
    prisma: PrismaClient
    logger: CustomLogger
    table: PrismaTableNames
    where: any
    create: any
    update: any
  }): Promise<T> {
    try {
      const delegate = getPrismaDelegate(prisma, table)

      return delegate.upsert({
        where,
        create: {
          ...create,
          created_at: new Date(),
          updated_at: new Date(),
        },
        update: {
          ...update,
          updated_at: new Date(),
        },
      })
    } catch (error: any) {
      logger.error('Upsert operation failed', {
        error,
        context: { table, where },
      })
      throw error
    }
  }
}

================
File: src/jobs/utils/image-processor.ts
================
import axios from 'axios'
import sharp from 'sharp'
import { config as env } from '../../config'
import { CustomLogger } from '@core'
import { createClient } from '@supabase/supabase-js'

const config = {
  dimensions: {
    original: { width: 1920, height: 1080 }, // Full HD
    mobile: { width: 768, height: 1024 }, // Mobile optimized
    thumbnail: { width: 300, height: 200 }, // Thumbnail size
  },
  formats: ['webp', 'jpg'], // jpg as fallback
}

const supabase = createClient(env.supabase.url, env.supabase.serviceKey)

const MAX_CONCURRENT_TASKS = 5 // Adjust based on your system capabilities

const log = new CustomLogger()

export async function bulkImageProcessing() {
  try {
    log.info('Starting bulk image processing')

    // Fetch news items that need image processing
    const newsItems = await fetchNewsItemsToProcess()

    if (newsItems.length === 0) {
      log.info('No news items to process')
      return
    }

    // Process images with limited concurrency
    await processImagesInBatches(newsItems, MAX_CONCURRENT_TASKS)

    log.info('Bulk image processing completed')
  } catch (error: any) {
    log.error('Error in bulkImageProcessing', { error })
  }
}

async function fetchNewsItemsToProcess(): Promise<any[]> {
  try {
    const { data, error } = await supabase
      .from('news')
      .select('*')
      .like('featured_image', 'http%')
      .limit(100)

    if (error) {
      log.error('Error fetching news items', { error })
      throw error
    }

    log.info('Fetched news items to process', { count: data?.length || 0 })
    return data || []
  } catch (error: any) {
    log.error('Error in fetchNewsItemsToProcess', { error })
    throw error
  }
}

async function processImagesInBatches(newsItems: any[], maxConcurrent: number) {
  const queue: Promise<void>[] = []

  for (const item of newsItems) {
    if (queue.length >= maxConcurrent) {
      await Promise.race(queue)
    }

    const task = processSingleImage(item)
      .catch((error: any) => {
        log.error('Error processing image', { error })
      })
      .finally(() => {
        queue.splice(queue.indexOf(task), 1)
      })

    queue.push(task)
  }

  // Wait for all remaining tasks to complete
  await Promise.all(queue)
}

async function processSingleImage(newsItem: any): Promise<void> {
  if (!newsItem) {
    log.warn('Invalid news item', { newsItem })
    return
  }

  const imageUrl = newsItem.featured_image
  const imageName = `news_${newsItem.id}`

  // Process the image
  const defaultImagePath = await handleImageProcessing(imageUrl!, imageName)

  // Update the news item with the new image path
  await updateNewsItem(newsItem.id!, { featured_image: defaultImagePath })

  log.info('Processed image for news item', { itemId: newsItem.id })
}

async function handleImageProcessing(imageUrl: string, imageName: string): Promise<string> {
  try {
    log.info('Starting image processing', { imageUrl, imageName })

    // Fetch the image
    const imageBuffer = await fetchImage(imageUrl)

    // Process and store the image, returning the default image path
    const defaultImagePath = await processAndStoreImage(imageBuffer, imageName)

    log.info('Image processing completed', { imageName })

    return defaultImagePath
  } catch (error: any) {
    log.error('Error in handleImageProcessing', { error })
    throw error
  }
}

async function fetchImage(url: string): Promise<Buffer> {
  try {
    const response = await axios.get(url, { responseType: 'arraybuffer' })
    log.info('Image fetched successfully', { url })
    return Buffer.from(response.data, 'binary')
  } catch (error: any) {
    log.error('Error fetching image', { error })
    throw error
  }
}

async function processAndStoreImage(imageBuffer: Buffer, imageName: string): Promise<string> {
  let defaultImagePath = ''

  for (const [sizeName, size] of Object.entries(config.dimensions)) {
    for (const format of config.formats) {
      try {
        // Resize and convert the image
        const processedImageBuffer = await sharp(imageBuffer)
          .resize(size.width, size.height, { fit: 'inside' })
          .toFormat(format as keyof sharp.FormatEnum)
          .toBuffer()

        // Define the storage path
        const filePath = `${sizeName}/${imageName}.${format}`

        // Upload the image to Supabase storage
        const { error } = await supabase.storage
          .from('news')
          .upload(filePath, processedImageBuffer, {
            contentType: `image/${format}`,
            upsert: true,
          })

        if (error) {
          log.error('Error uploading image', { error })
        } else {
          log.info('Image uploaded successfully', { filePath })

          // Set default image path to 'original' size and 'jpg' format
          if (sizeName === 'original' && format === 'jpg') {
            defaultImagePath = `news/${filePath}` // e.g., 'news/original/news_123.jpg'
          }
        }
      } catch (error: any) {
        log.error('Error processing image', { error })
      }
    }
  }

  return defaultImagePath
}

async function updateNewsItem(id: string, updates: Partial<any>): Promise<void> {
  const { error } = await supabase.from('news').update(updates).eq('id', id)

  if (error) {
    log.error('Error updating news item', { error })
    throw error
  }
}

================
File: src/jobs/utils/link-extractor.ts
================
// newsLinkExtractor.ts
import { Page } from 'playwright'
import robotsParser from 'robots-parser'
import Parser from 'rss-parser'
import pLimit from 'p-limit'
import { CustomLogger, PrismaService } from '@core'
import { ContentSource } from '@types'
import { logFile } from '@helpers'
import { URLFormatter } from './url-formatter'
import { UrlClassifier } from './url-classifier'
import { ContentStash, globalContentStash } from './content-stash'
import fetch from 'node-fetch'

interface ClassifiedUrl {
  url: string
  predicted_category: string
  created_at: string
}

export class NewsLinkExtractor {
  private readonly REQUEST_TIMEOUT = 10000
  private readonly MAX_CONCURRENT = 5
  private readonly parser: Parser

  private contentStash: ContentStash
  private domain: string
  private classifier: UrlClassifier
  private processedUrls = new Set<string>()
  private classifiedUrls: ClassifiedUrl[] = []

  constructor(
    private readonly page: Page | null,
    private readonly source: ContentSource,
    private readonly services: {
      logger: CustomLogger
      prisma: PrismaService
    },
  ) {
    this.domain = new URL(source.url).hostname
    this.contentStash = globalContentStash
    this.classifier = new UrlClassifier()
    this.parser = new Parser({
      timeout: this.REQUEST_TIMEOUT,
      headers: { 'User-Agent': 'NewsBot/1.0 (Astrotribe RSS Fetcher)' },
    })

    this.services.logger.info(`Initialized NewsLinkExtractor for source: ${source.url}`)
  }

  public async extractLinks(): Promise<any[]> {
    const { logger } = this.services

    // First try RSS feeds if available
    if (this.source.rss_urls?.length) {
      logger.info('RSS feeds found, attempting RSS extraction first', {
        feedCount: this.source.rss_urls.length,
        feeds: this.source.rss_urls,
      })

      try {
        const rssItems = await this.extractFromRSSFeeds()
        if (rssItems.length > 0) {
          logger.info('Successfully extracted items from RSS feeds', {
            itemCount: rssItems.length,
            samples: rssItems.slice(0, 3).map((item) => ({
              title: item.title,
              link: item.link,
            })),
          })
          return this.createBlogPostObjects(rssItems)
        }
        logger.warn('No items found in RSS feeds, falling back to HTML scraping')
      } catch (error: any) {
        logger.error('RSS extraction failed, falling back to HTML scraping', { error })
      }
    }

    // Fallback to HTML scraping if we have a page
    if (!this.page) {
      logger.warn('No page provided for HTML scraping')
      return []
    }

    logger.info('Starting HTML scraping')
    const urls = await this.extractAllLinks()
    const validUrls = await this.filterAndClassifyUrls(urls)
    return this.createBlogPostObjects(validUrls)
  }

  private async filterAndClassifyUrls(urls: string[]): Promise<string[]> {
    const { logger } = this.services
    const extractedUrls: string[] = []
    const limit = pLimit(this.MAX_CONCURRENT)

    await Promise.all(
      urls.map((url) =>
        limit(async () => {
          if (this.processedUrls.has(url)) return
          this.processedUrls.add(url)

          try {
            if (!this.isSameDomain(url) || !(await this.isScrapingAllowed(url))) {
              return
            }

            if (await this.isBlogPost(url)) {
              extractedUrls.push(url)
              this.contentStash.addValidBlogUrl(new URL(url).hostname, url)
            }
          } catch (error: any) {
            logger.error(`Error processing URL: ${url}`, { error })
          }
        }),
      ),
    )

    await this.storeClassificationsInBulk()
    return extractedUrls
  }

  private async extractFromRSSFeeds(): Promise<any[]> {
    const { logger } = this.services
    const limit = pLimit(this.MAX_CONCURRENT)

    const feedPromises = (this.source.rss_urls || []).filter(Boolean).map((rssUrl) =>
      limit(async () => {
        try {
          logger.debug(`Fetching RSS feed: ${rssUrl}`)
          const feed = await this.fetchRSSWithTimeout(rssUrl)

          if (feed?.items?.length) {
            logger.info(`Successfully fetched RSS feed: ${rssUrl}`, {
              itemCount: feed.items.length,
            })
            return feed.items
          }

          logger.warn(`No items found in RSS feed: ${rssUrl}`)
          return []
        } catch (error: any) {
          logger.error(`Failed to fetch RSS feed: ${rssUrl}`, { error })
          return []
        }
      }),
    )

    const feeds = await Promise.all(feedPromises)
    return feeds
      .flat()
      .filter(
        (item, index, self) => item.link && index === self.findIndex((i) => i.link === item.link),
      )
  }

  private async fetchRSSWithTimeout(url: string): Promise<any> {
    try {
      return await Promise.race([
        this.parser.parseURL(url),
        new Promise((_, reject) =>
          setTimeout(
            () => reject(new Error(`RSS fetch timeout after ${this.REQUEST_TIMEOUT}ms`)),
            this.REQUEST_TIMEOUT,
          ),
        ),
      ])
    } catch (error: any) {
      this.services.logger.error(`RSS fetch failed for ${url}`, { error })
      throw error
    }
  }

  private async storeClassificationsInBulk(): Promise<void> {
    try {
      if (this.classifiedUrls.length === 0) {
        return
      }

      await this.services.prisma.$transaction(async (tx) => {
        // Process in chunks to avoid overwhelming the database
        const chunkSize = 100
        for (let i = 0; i < this.classifiedUrls.length; i += chunkSize) {
          const chunk = this.classifiedUrls.slice(i, i + chunkSize)

          await tx.classified_urls.createMany({
            data: chunk.map((url) => ({
              url: url.url,
              predicted_category: url.predicted_category as any,
              created_at: url.created_at || new Date(),
            })),
            skipDuplicates: true,
          })

          // Update existing records that might have been skipped
          await Promise.all(
            chunk.map((url) =>
              tx.classified_urls
                .update({
                  where: { url: url.url },
                  data: {
                    predicted_category: url.predicted_category as any,
                    updated_at: new Date(),
                  },
                })
                .catch((error: any) => {
                  this.services.logger.warn(`Failed to update URL: ${url.url}`, { error })
                }),
            ),
          )
        }
      })

      this.services.logger.info(`Processed ${this.classifiedUrls.length} classifications in bulk`)
      this.classifiedUrls = [] // Clear the array after processing
    } catch (error: any) {
      this.services.logger.error('Error processing classifications in bulk', {
        ...error,
        urlCount: this.classifiedUrls.length,
      })
      throw error
    }
  }

  public async extractBlogLinks(): Promise<string[]> {
    let rss
    if (this.source.rss_urls && this.source.rss_urls.length) {
      this.services.logger.info('RSS URLs found, attempting to extract from RSS feed', {
        rssUrls: this.source.rss_urls,
      })
      rss = await this.extractRSSFeed()
    }

    if (rss) {
      this.services.logger.info('Successfully extracted from RSS feed', {
        itemCount: rss.length,
        sampleItems: rss.slice(0, 3).map((item) => ({
          title: item.title,
          link: item.link,
          date: item.isoDate,
        })),
      })
      return this.createBlogPostObjects(rss)
    }

    this.services.logger.info('No RSS feed or RSS extraction failed, falling back to HTML scraping')
    const urls = await this.extractAllLinks()
    this.services.logger.info(`Extracted ${urls.length} links from HTML`, {
      sampleUrls: urls.slice(0, 5),
    })

    const startTime = performance.now()
    const extractedUrls: string[] = []
    const limit = pLimit(5) // Limit to 5 concurrent operations

    await Promise.all(
      urls.map((url) =>
        limit(async () => {
          if (this.processedUrls.has(url)) {
            this.services.logger.debug(`Skipping already processed URL: ${url}`)
            return
          }
          this.processedUrls.add(url)

          const extractionStartTime = performance.now()

          if (!this.isSameDomain(url)) {
            this.services.logger.debug(`URL is on a different domain: ${url}`, {
              sourceDomain: this.domain,
              urlDomain: new URL(url).hostname,
            })
            logFile('excluded_urls', { url, reason: 'Different domain' })
            return
          }

          const isAllowed = await this.isScrapingAllowed(url)
          if (!isAllowed) {
            this.services.logger.warn(`Scraping disallowed for URL: ${url}`)
            logFile('news/excluded_urls', {
              url,
              reason: 'Scraping disallowed',
            })
            return
          }

          const isBlogPost = await this.isBlogPost(url)
          if (isBlogPost) {
            extractedUrls.push(url)
            this.contentStash.addValidBlogUrl(new URL(url).hostname, url)
            logFile('news/extracted_urls', url)
            this.services.logger.info(`URL identified as blog post: ${url}`, {
              extractionTime: performance.now() - extractionStartTime,
              totalExtracted: extractedUrls.length,
            })
          } else {
            this.services.logger.debug(`URL is not an article: ${url}`, {
              extractionTime: performance.now() - extractionStartTime,
            })
            logFile('news/excluded_urls', { url, reason: 'Not an article' })
          }

          const extractionTime = performance.now() - extractionStartTime
          this.services.logger.debug(`URL processing complete: ${url}`, {
            extractionTime: extractionTime.toFixed(2),
            isArticle: isBlogPost,
          })
        }),
      ),
    )

    const endTime = performance.now()
    const totalTime = endTime - startTime

    this.services.logger.info('Link extraction completed', {
      totalTime: totalTime.toFixed(2),
      totalUrls: urls.length,
      extractedUrls: extractedUrls.length,
      sampleExtractedUrls: extractedUrls.slice(0, 5),
    })

    await this.storeClassificationsInBulk()

    const blogPosts = this.createBlogPostObjects(extractedUrls)
    this.services.logger.info('Blog post objects created', {
      count: blogPosts.length,
      samplePosts: blogPosts.slice(0, 3).map((post) => ({
        url: post.contents.url,
        title: post.news.title,
        publishedAt: post.news.published_at,
      })),
    })

    return blogPosts
  }

  private async isBlogPost(url: string): Promise<boolean> {
    try {
      const parsedUrl = new URL(url)
      const pathname = parsedUrl.pathname.toLowerCase()

      if (this.isExcludedLink(pathname)) {
        this.services.logger.debug(`URL excluded by pattern: ${url}`)
        logFile('news/excluded_urls', { url, reason: 'Excluded pattern' })
        return false
      }

      if (this.containsArticleIndicators(pathname)) {
        this.services.logger.debug(`URL contains article indicators: ${url}`)
        return true
      }

      // Use ML classifier
      const category = await this.classifier.predict(url)
      const isArticle = category === 'news'

      this.classifiedUrls.push({
        url,
        predicted_category: category,
        created_at: new Date().toISOString(),
      })

      if (isArticle) {
        this.services.logger.debug(`Classifier accepted URL: ${url}, category: ${category}`)
        logFile('news/extracted_urls', {
          url,
          reason: 'Classifier Accepted',
          category,
        })
      } else {
        this.services.logger.verbose(`Classifier rejected URL: ${url}, category: ${category}`)
        logFile('news/excluded_urls', {
          url,
          reason: 'Classifier rejected',
          category,
        })
      }
      return isArticle
    } catch (error: any) {
      this.services.logger.error(`Error in isBlogPost for URL: ${url}`, error)
      logFile('news/excluded_urls', { url, reason: 'Error in isBlogPost' })
      return false
    }
  }

  private createBlogPostObjects(urls: string[] | any[]): any[] {
    const blogPosts = urls.map((url) => {
      const featured_image =
        url?.['content:encoded']?.match(/<img[^>]+src="([^">]+)"/)?.[1] || url?.enclosure?.url

      return {
        contents: {
          url: (url.hasOwnProperty('link') && url.link) || url,
          content_type: 'news',
        },
        news: {
          url: (url.hasOwnProperty('link') && url.link) || url,
          author: url?.creator || url?.['dc:creator'],
          title: url?.title,
          featured_image,
          body: url?.['content:encodedSnippet'],
          description: url?.contentSnippet,
          published_at: url?.isoDate,
          keywords: url ? { values: url?.categories } : null,
          company_id: this.source.company_id,
          scraped_at: new Date().toISOString(),
          failed_count: 0,
          scrape_frequency: 'weekly',
        },
      }
    })

    return blogPosts
  }

  private isExcludedLink(pathname: string): boolean {
    const exclusions = [
      '/category/',
      '/tag/',
      '/tags/',
      '/author/',
      '/page/',
      '/search/',
      '/feed',
      '/archive',
      '/archives',
      '/archive.html',
      '/section/',
      '/event',
      '/events',
      '/contact',
      '/about',
      '/media-contacts',
      '/all-news',
      '/news/all-news',
      '/news/media-contacts',
      '/news/press-releases',
      '/newsletter',
      '/subscribe',
      '/signup',
      '/login',
      '/register',
      '/sitemap',
      '/issues/',
      '/picture-of-the-day/',
      '/what-we-do/',
      '/resources/',
    ]

    // Exclude URLs that end with a date pattern like /YYYY, /YYYY/, /YYYY/MM, or /MM
    const datePattern = /\/(\d{4}(\/\d{2})?\/?)$|\/\d+$/

    // Exclude base URL (root path)
    const isRootPath = pathname === '/' || pathname === ''

    // Get the last segment of the path
    const pathSegments = pathname.split('/').filter(Boolean)
    const lastSegment = pathSegments[pathSegments.length - 1] || ''

    // Exclude URLs with less than 3 hyphens in the last segment
    const hyphenCount = (lastSegment.match(/-/g) || []).length
    const hasFewHyphensInLastSegment = hyphenCount < 3

    const isExcluded =
      isRootPath ||
      hasFewHyphensInLastSegment ||
      exclusions.some((exclusion) => pathname.includes(exclusion)) ||
      datePattern.test(pathname)

    this.services.logger.debug(
      `isExcludedLink check for pathname: ${pathname}, result: ${isExcluded}`,
    )

    return isExcluded
  }

  private containsArticleIndicators(pathname: string): boolean {
    const articleIndicators = [
      /\/blogs?\//, // Matches /blog/ or /blogs/
      /\/news\//, // Matches /news/ or /newses/
      /\/articles?\//, // Matches /article/ or /articles/
      /\/posts?\//, // Matches /post/ or /posts/
      /\/stor(y|ies)\//, // Matches /story/ or /stories/
      /\/20\d{2}/, // Matches years from 2000 to 2099
    ]

    const hasIndicators = articleIndicators.some((indicator) =>
      indicator.test(pathname.toLowerCase()),
    )

    this.services.logger.debug(
      `containsArticleIndicators check for pathname: ${pathname}, result: ${hasIndicators}`,
    )

    return hasIndicators
  }

  private isSameDomain(url: string): boolean {
    const baseDomain = this.getBaseDomain(this.source.url)
    const urlDomain = this.getBaseDomain(url)

    // Allow subdomains of the base domain
    const isSame = urlDomain.endsWith(baseDomain)
    this.services.logger.debug(
      `isSameDomain check: baseDomain=${baseDomain}, urlDomain=${urlDomain}, result=${isSame}`,
    )

    return isSame
  }

  private getBaseDomain(url: string): string {
    const hostname = new URL(url).hostname
    const parts = hostname.split('.').slice(-2)
    const baseDomain = parts.join('.')

    this.services.logger.debug(`getBaseDomain for URL: ${url}, baseDomain: ${baseDomain}`)

    return baseDomain
  }

  private async extractRSSFeed() {
    const parser = new Parser()
    const maxRetries = 3
    const failedCounts: Record<string, number> = {}

    // Initialize pLimit with maximum concurrent tasks
    const limit = pLimit(this.MAX_CONCURRENT)

    // Create an array of limited promises

    const promises = (this.source.rss_urls || [])
      .filter(Boolean)
      .map((rss_url) =>
        limit(() => this.parseRSSWithRetries(rss_url, maxRetries, failedCounts, parser)),
      )

    const feeds = await Promise.all(promises)

    // Flatten and filter the results
    return feeds
      .flat()
      .filter(
        (item, index, self) => item.title && index === self.findIndex((o) => o.link === item.link),
      )
  }

  private async parseRSSWithRetries(
    rss_url: string,
    maxRetries: number,
    failedCounts: Record<string, number>,
    parser: Parser,
  ): Promise<any[]> {
    let retries = 0
    const items: any[] = []
    const startTime = performance.now()

    while (retries < maxRetries) {
      try {
        const feed = await this.parseURLWithTimeout(parser, rss_url)

        if (feed?.items?.length > 0) {
          items.push(...feed.items)
          const duration = performance.now() - startTime

          this.services.logger.info(
            `Successfully processed feed from ${rss_url} in ${duration.toFixed(2)}ms`,
          )
          return items
        } else {
          this.services.logger.warn(`No items found in feed from ${rss_url}`)
          const duration = performance.now() - startTime
          this.services.logger.info(
            `Processed feed from ${rss_url} (no items) in ${duration.toFixed(2)}ms`,
          )
          return []
        }
      } catch (error: any) {
        retries++
        failedCounts[rss_url] = (failedCounts[rss_url] || 0) + 1

        const errorMessage = error instanceof Error ? error.message : 'Unknown error'
        this.services.logger.error(`RSS Parse Error for ${rss_url}`, {
          ...error,
          message: `Error occurred on ${retries} attempt while parsing RSS: ${errorMessage}`,
        })

        if (retries === maxRetries) {
          const duration = performance.now() - startTime
          this.services.logger.error('Failed to fetch', {
            context: {
              message:
                `Max retries reached for ${rss_url}. Failed ${failedCounts[rss_url]} times. ` +
                `Total processing time: ${duration.toFixed(2)}ms`,
            },
          })
        }
      }
    }

    const duration = performance.now() - startTime
    this.services.logger.info(`Processed feed from ${rss_url} in ${duration.toFixed(2)}ms`)

    return items
  }

  private async parseURLWithTimeout(parser: Parser, url: string): Promise<any> {
    return Promise.race([
      parser.parseURL(url),
      new Promise((_, reject) =>
        setTimeout(
          () => reject(new Error(`Timeout after ${this.REQUEST_TIMEOUT}ms`)),
          this.REQUEST_TIMEOUT,
        ),
      ),
    ])
  }

  private async extractAllLinks(): Promise<string[]> {
    this.services.logger.debug('Starting link extraction from page')

    try {
      // Wait for content to load
      await this.page?.waitForLoadState('domcontentloaded')

      // First try specific content areas
      const contentSelectors = [
        'main',
        '[role="main"]',
        '[class*="content"]',
        'article',
        '.news-content',
        '.article-content',
        '.post-content',
        '#content',
      ].join(', ')

      // Check if main content exists
      const mainContent = this.page?.locator(contentSelectors)
      const mainContentCount = await mainContent?.count()

      this.services.logger.debug(`Found ${mainContentCount} main content areas`)

      let links: string[] = []

      if (!!mainContentCount && mainContentCount > 0) {
        // Extract links within main content
        links = (await mainContent?.evaluateAll((nodes) => {
          const hrefs = new Set<string>()
          nodes.forEach((node) => {
            const anchorTags = node.querySelectorAll('a[href]')
            anchorTags.forEach((a) => {
              const href = (a as HTMLAnchorElement).href
              if (href && !href.startsWith('javascript:') && !href.startsWith('#')) {
                hrefs.add(href)
              }
            })
          })
          return Array.from(hrefs)
        })) as string[]

        this.services.logger.info(`Extracted ${links.length} links from main content areas`, {
          sampleLinks: links.slice(0, 5),
        })
      } else {
        // Fallback: Extract all links
        this.services.logger.warn('No main content found, falling back to whole page extraction')

        // Wait for any dynamic content
        await this.page?.waitForLoadState('networkidle')

        links = (await this.page?.evaluate(() => {
          const hrefs = new Set<string>()
          document.querySelectorAll('a[href]').forEach((a) => {
            const href = (a as HTMLAnchorElement).href
            if (href && !href.startsWith('javascript:') && !href.startsWith('#')) {
              hrefs.add(href)
            }
          })
          return Array.from(hrefs)
        })) as string[]

        this.services.logger.info(`Extracted ${links.length} links from entire page`, {
          sampleLinks: links.slice(0, 5),
        })
      }

      // Format and validate URLs
      const { validUrls, contacts } = URLFormatter.formatURLs(
        URLFormatter.getBaseUrl(this.source.url),
        links,
      )

      if (contacts.length > 0) {
        this.services.logger.info(`Found ${contacts.length} contacts during extraction`)
      }

      const uniqueLinks = Array.from(new Set(validUrls))

      this.services.logger.info('Link extraction completed', {
        totalRawLinks: links.length,
        validLinks: validUrls.length,
        contacts: contacts.length,
        uniqueLinks: uniqueLinks.length,
        sampleUniqueLinks: uniqueLinks.slice(0, 5),
      })

      return uniqueLinks
    } catch (error: any) {
      this.services.logger.error('Error extracting links from page', {
        error,
        context: {
          url: this.source.url,
          pageTitle: await this.page?.title(),
        },
      })
      return []
    }
  }

  private async isScrapingAllowed(url: string): Promise<boolean> {
    const baseUrl = new URL(url).origin
    const robotsUrl = `${baseUrl}/robots.txt`
    try {
      const response = await fetch(robotsUrl)
      const robotsTxt = await response.text()
      const robots = robotsParser(robotsUrl, robotsTxt)
      const isAllowed = robots.isAllowed(url, 'YourScraperUserAgent') || false
      this.services.logger.debug(`Robots.txt check for URL: ${url}, isAllowed: ${isAllowed}`)
      return isAllowed
    } catch (error: any) {
      this.services.logger.error(`Error fetching robots.txt from ${robotsUrl}`, error)
      // Default to not scraping if unable to fetch robots.txt
      return false
    }
  }

  public getContentStashStats(): any {
    this.services.logger.debug('Retrieving content stash stats')
    return this.contentStash.getAllStats()
  }
}

export async function scrapeNewsLinks(
  page: Page,
  source: ContentSource,
  services: { logger: CustomLogger; prisma: PrismaService },
): Promise<string[]> {
  console.info('Creating NewsLinkExtractor instance')
  const scraper = new NewsLinkExtractor(page, source, services)
  return scraper.extractBlogLinks()
}

================
File: src/jobs/utils/schedule.utils.ts
================
import { DayOfWeek, ScheduleConfig, ScheduleInterval, TimeUnit } from '../../types'

// utils/schedule.utils.ts
export class ScheduleParser {
  private static readonly timeUnitMap: Record<TimeUnit, number> = {
    minute: 1,
    hour: 60,
    day: 60 * 24,
    week: 60 * 24 * 7,
    month: 60 * 24 * 30,
  }

  private static readonly daysOfWeek: Record<DayOfWeek, number> = {
    Sunday: 0,
    Monday: 1,
    Tuesday: 2,
    Wednesday: 3,
    Thursday: 4,
    Friday: 5,
    Saturday: 6,
  }

  static toCron(schedule: ScheduleConfig): string | false {
    if (!schedule.enabled) return false

    switch (schedule.type) {
      case 'interval':
        return this.intervalToCron(schedule.interval!)
      case 'daily':
        return this.dailyToCron(schedule.time!)
      case 'weekly':
        return this.weeklyToCron(schedule.dayOfWeek!, schedule.time!)
      case 'monthly':
        return this.monthlyToCron(schedule.dayOfMonth!, schedule.time!)
      case 'custom':
        return schedule.customCron!
      default:
        throw new Error(`Unsupported schedule type: ${schedule.type}`)
    }
  }

  private static intervalToCron({ value, unit }: ScheduleInterval): string {
    switch (unit) {
      case 'minute':
        return `*/${value} * * * *`
      case 'hour':
        return `0 */${value} * * *`
      case 'day':
        return `0 0 */${value} * *`
      case 'week':
        return `0 0 * * */${value}`
      case 'month':
        return `0 0 1 */${value} *`
      default:
        throw new Error(`Invalid interval unit: ${unit}`)
    }
  }

  private static dailyToCron(time: string): string {
    const [hours, minutes] = time.split(':')
    return `${minutes} ${hours} * * *`
  }

  private static weeklyToCron(day: DayOfWeek, time: string): string {
    const [hours, minutes] = time.split(':')
    return `${minutes} ${hours} * * ${this.daysOfWeek[day]}`
  }

  private static monthlyToCron(day: number | 'first' | 'last', time: string): string {
    const [hours, minutes] = time.split(':')
    const dayStr = day === 'last' ? 'L' : day === 'first' ? '1' : day
    return `${minutes} ${hours} ${dayStr} * *`
  }
}

================
File: src/jobs/utils/tlds.ts
================
export default [
  'aaa',
  'aarp',
  'abb',
  'abbott',
  'abbvie',
  'abc',
  'able',
  'abogado',
  'abudhabi',
  'ac',
  'academy',
  'accenture',
  'accountant',
  'accountants',
  'aco',
  'actor',
  'ad',
  'ads',
  'adult',
  'ae',
  'aeg',
  'aero',
  'aetna',
  'af',
  'afl',
  'africa',
  'ag',
  'agakhan',
  'agency',
  'ai',
  'aig',
  'airbus',
  'airforce',
  'airtel',
  'akdn',
  'al',
  'alibaba',
  'alipay',
  'allfinanz',
  'allstate',
  'ally',
  'alsace',
  'alstom',
  'am',
  'amazon',
  'americanexpress',
  'americanfamily',
  'amex',
  'amfam',
  'amica',
  'amsterdam',
  'analytics',
  'android',
  'anquan',
  'anz',
  'ao',
  'aol',
  'apartments',
  'app',
  'apple',
  'aq',
  'aquarelle',
  'ar',
  'arab',
  'aramco',
  'archi',
  'army',
  'arpa',
  'art',
  'arte',
  'as',
  'asda',
  'asia',
  'associates',
  'at',
  'athleta',
  'attorney',
  'au',
  'auction',
  'audi',
  'audible',
  'audio',
  'auspost',
  'author',
  'auto',
  'autos',
  'aw',
  'aws',
  'ax',
  'axa',
  'az',
  'azure',
  'ba',
  'baby',
  'baidu',
  'banamex',
  'band',
  'bank',
  'bar',
  'barcelona',
  'barclaycard',
  'barclays',
  'barefoot',
  'bargains',
  'baseball',
  'basketball',
  'bauhaus',
  'bayern',
  'bb',
  'bbc',
  'bbt',
  'bbva',
  'bcg',
  'bcn',
  'bd',
  'be',
  'beats',
  'beauty',
  'beer',
  'bentley',
  'berlin',
  'best',
  'bestbuy',
  'bet',
  'bf',
  'bg',
  'bh',
  'bharti',
  'bi',
  'bible',
  'bid',
  'bike',
  'bing',
  'bingo',
  'bio',
  'biz',
  'bj',
  'black',
  'blackfriday',
  'blockbuster',
  'blog',
  'bloomberg',
  'blue',
  'bm',
  'bms',
  'bmw',
  'bn',
  'bnpparibas',
  'bo',
  'boats',
  'boehringer',
  'bofa',
  'bom',
  'bond',
  'boo',
  'book',
  'booking',
  'bosch',
  'bostik',
  'boston',
  'bot',
  'boutique',
  'box',
  'br',
  'bradesco',
  'bridgestone',
  'broadway',
  'broker',
  'brother',
  'brussels',
  'bs',
  'bt',
  'build',
  'builders',
  'business',
  'buy',
  'buzz',
  'bv',
  'bw',
  'by',
  'bz',
  'bzh',
  'ca',
  'cab',
  'cafe',
  'cal',
  'call',
  'calvinklein',
  'cam',
  'camera',
  'camp',
  'canon',
  'capetown',
  'capital',
  'capitalone',
  'car',
  'caravan',
  'cards',
  'care',
  'career',
  'careers',
  'cars',
  'casa',
  'case',
  'cash',
  'casino',
  'cat',
  'catering',
  'catholic',
  'cba',
  'cbn',
  'cbre',
  'cc',
  'cd',
  'center',
  'ceo',
  'cern',
  'cf',
  'cfa',
  'cfd',
  'cg',
  'ch',
  'chanel',
  'channel',
  'charity',
  'chase',
  'chat',
  'cheap',
  'chintai',
  'christmas',
  'chrome',
  'church',
  'ci',
  'cipriani',
  'circle',
  'cisco',
  'citadel',
  'citi',
  'citic',
  'city',
  'ck',
  'cl',
  'claims',
  'cleaning',
  'click',
  'clinic',
  'clinique',
  'clothing',
  'cloud',
  'club',
  'clubmed',
  'cm',
  'cn',
  'co',
  'coach',
  'codes',
  'coffee',
  'college',
  'cologne',
  'com',
  'commbank',
  'community',
  'company',
  'compare',
  'computer',
  'comsec',
  'condos',
  'construction',
  'consulting',
  'contact',
  'contractors',
  'cooking',
  'cool',
  'coop',
  'corsica',
  'country',
  'coupon',
  'coupons',
  'courses',
  'cpa',
  'cr',
  'credit',
  'creditcard',
  'creditunion',
  'cricket',
  'crown',
  'crs',
  'cruise',
  'cruises',
  'cu',
  'cuisinella',
  'cv',
  'cw',
  'cx',
  'cy',
  'cymru',
  'cyou',
  'cz',
  'dad',
  'dance',
  'data',
  'date',
  'dating',
  'datsun',
  'day',
  'dclk',
  'dds',
  'de',
  'deal',
  'dealer',
  'deals',
  'degree',
  'delivery',
  'dell',
  'deloitte',
  'delta',
  'democrat',
  'dental',
  'dentist',
  'desi',
  'design',
  'dev',
  'dhl',
  'diamonds',
  'diet',
  'digital',
  'direct',
  'directory',
  'discount',
  'discover',
  'dish',
  'diy',
  'dj',
  'dk',
  'dm',
  'dnp',
  'do',
  'docs',
  'doctor',
  'dog',
  'domains',
  'dot',
  'download',
  'drive',
  'dtv',
  'dubai',
  'dunlop',
  'dupont',
  'durban',
  'dvag',
  'dvr',
  'dz',
  'earth',
  'eat',
  'ec',
  'eco',
  'edeka',
  'edu',
  'education',
  'ee',
  'eg',
  'email',
  'emerck',
  'energy',
  'engineer',
  'engineering',
  'enterprises',
  'epson',
  'equipment',
  'er',
  'ericsson',
  'erni',
  'es',
  'esq',
  'estate',
  'et',
  'eu',
  'eurovision',
  'eus',
  'events',
  'exchange',
  'expert',
  'exposed',
  'express',
  'extraspace',
  'fage',
  'fail',
  'fairwinds',
  'faith',
  'family',
  'fan',
  'fans',
  'farm',
  'farmers',
  'fashion',
  'fast',
  'fedex',
  'feedback',
  'ferrari',
  'ferrero',
  'fi',
  'fidelity',
  'fido',
  'film',
  'final',
  'finance',
  'financial',
  'fire',
  'firestone',
  'firmdale',
  'fish',
  'fishing',
  'fit',
  'fitness',
  'fj',
  'fk',
  'flickr',
  'flights',
  'flir',
  'florist',
  'flowers',
  'fly',
  'fm',
  'fo',
  'foo',
  'food',
  'football',
  'ford',
  'forex',
  'forsale',
  'forum',
  'foundation',
  'fox',
  'fr',
  'free',
  'fresenius',
  'frl',
  'frogans',
  'frontier',
  'ftr',
  'fujitsu',
  'fun',
  'fund',
  'furniture',
  'futbol',
  'fyi',
  'ga',
  'gal',
  'gallery',
  'gallo',
  'gallup',
  'game',
  'games',
  'gap',
  'garden',
  'gay',
  'gb',
  'gbiz',
  'gd',
  'gdn',
  'ge',
  'gea',
  'gent',
  'genting',
  'george',
  'gf',
  'gg',
  'ggee',
  'gh',
  'gi',
  'gift',
  'gifts',
  'gives',
  'giving',
  'gl',
  'glass',
  'gle',
  'global',
  'globo',
  'gm',
  'gmail',
  'gmbh',
  'gmo',
  'gmx',
  'gn',
  'godaddy',
  'gold',
  'goldpoint',
  'golf',
  'goo',
  'goodyear',
  'goog',
  'google',
  'gop',
  'got',
  'gov',
  'gp',
  'gq',
  'gr',
  'grainger',
  'graphics',
  'gratis',
  'green',
  'gripe',
  'grocery',
  'group',
  'gs',
  'gt',
  'gu',
  'gucci',
  'guge',
  'guide',
  'guitars',
  'guru',
  'gw',
  'gy',
  'hair',
  'hamburg',
  'hangout',
  'haus',
  'hbo',
  'hdfc',
  'hdfcbank',
  'health',
  'healthcare',
  'help',
  'helsinki',
  'here',
  'hermes',
  'hiphop',
  'hisamitsu',
  'hitachi',
  'hiv',
  'hk',
  'hkt',
  'hm',
  'hn',
  'hockey',
  'holdings',
  'holiday',
  'homedepot',
  'homegoods',
  'homes',
  'homesense',
  'honda',
  'horse',
  'hospital',
  'host',
  'hosting',
  'hot',
  'hotels',
  'hotmail',
  'house',
  'how',
  'hr',
  'hsbc',
  'ht',
  'hu',
  'hughes',
  'hyatt',
  'hyundai',
  'ibm',
  'icbc',
  'ice',
  'icu',
  'id',
  'ie',
  'ieee',
  'ifm',
  'ikano',
  'il',
  'im',
  'imamat',
  'imdb',
  'immo',
  'immobilien',
  'in',
  'inc',
  'industries',
  'infiniti',
  'info',
  'ing',
  'ink',
  'institute',
  'insurance',
  'insure',
  'int',
  'international',
  'intuit',
  'investments',
  'io',
  'ipiranga',
  'iq',
  'ir',
  'irish',
  'is',
  'ismaili',
  'ist',
  'istanbul',
  'it',
  'itau',
  'itv',
  'jaguar',
  'java',
  'jcb',
  'je',
  'jeep',
  'jetzt',
  'jewelry',
  'jio',
  'jll',
  'jm',
  'jmp',
  'jnj',
  'jo',
  'jobs',
  'joburg',
  'jot',
  'joy',
  'jp',
  'jpmorgan',
  'jprs',
  'juegos',
  'juniper',
  'kaufen',
  'kddi',
  'ke',
  'kerryhotels',
  'kerrylogistics',
  'kerryproperties',
  'kfh',
  'kg',
  'kh',
  'ki',
  'kia',
  'kids',
  'kim',
  'kindle',
  'kitchen',
  'kiwi',
  'km',
  'kn',
  'koeln',
  'komatsu',
  'kosher',
  'kp',
  'kpmg',
  'kpn',
  'kr',
  'krd',
  'kred',
  'kuokgroup',
  'kw',
  'ky',
  'kyoto',
  'kz',
  'la',
  'lacaixa',
  'lamborghini',
  'lamer',
  'lancaster',
  'land',
  'landrover',
  'lanxess',
  'lasalle',
  'lat',
  'latino',
  'latrobe',
  'law',
  'lawyer',
  'lb',
  'lc',
  'lds',
  'lease',
  'leclerc',
  'lefrak',
  'legal',
  'lego',
  'lexus',
  'lgbt',
  'li',
  'lidl',
  'life',
  'lifeinsurance',
  'lifestyle',
  'lighting',
  'like',
  'lilly',
  'limited',
  'limo',
  'lincoln',
  'link',
  'lipsy',
  'live',
  'living',
  'lk',
  'llc',
  'llp',
  'loan',
  'loans',
  'locker',
  'locus',
  'lol',
  'london',
  'lotte',
  'lotto',
  'love',
  'lpl',
  'lplfinancial',
  'lr',
  'ls',
  'lt',
  'ltd',
  'ltda',
  'lu',
  'lundbeck',
  'luxe',
  'luxury',
  'lv',
  'ly',
  'ma',
  'madrid',
  'maif',
  'maison',
  'makeup',
  'man',
  'management',
  'mango',
  'map',
  'market',
  'marketing',
  'markets',
  'marriott',
  'marshalls',
  'mattel',
  'mba',
  'mc',
  'mckinsey',
  'md',
  'me',
  'med',
  'media',
  'meet',
  'melbourne',
  'meme',
  'memorial',
  'men',
  'menu',
  'merckmsd',
  'mg',
  'mh',
  'miami',
  'microsoft',
  'mil',
  'mini',
  'mint',
  'mit',
  'mitsubishi',
  'mk',
  'ml',
  'mlb',
  'mls',
  'mm',
  'mma',
  'mn',
  'mo',
  'mobi',
  'mobile',
  'moda',
  'moe',
  'moi',
  'mom',
  'monash',
  'money',
  'monster',
  'mormon',
  'mortgage',
  'moscow',
  'moto',
  'motorcycles',
  'mov',
  'movie',
  'mp',
  'mq',
  'mr',
  'ms',
  'msd',
  'mt',
  'mtn',
  'mtr',
  'mu',
  'museum',
  'music',
  'mv',
  'mw',
  'mx',
  'my',
  'mz',
  'na',
  'nab',
  'nagoya',
  'name',
  'navy',
  'nba',
  'nc',
  'ne',
  'nec',
  'net',
  'netbank',
  'netflix',
  'network',
  'neustar',
  'new',
  'news',
  'next',
  'nextdirect',
  'nexus',
  'nf',
  'nfl',
  'ng',
  'ngo',
  'nhk',
  'ni',
  'nico',
  'nike',
  'nikon',
  'ninja',
  'nissan',
  'nissay',
  'nl',
  'no',
  'nokia',
  'norton',
  'now',
  'nowruz',
  'nowtv',
  'np',
  'nr',
  'nra',
  'nrw',
  'ntt',
  'nu',
  'nyc',
  'nz',
  'obi',
  'observer',
  'office',
  'okinawa',
  'olayan',
  'olayangroup',
  'ollo',
  'om',
  'omega',
  'one',
  'ong',
  'onl',
  'online',
  'ooo',
  'open',
  'oracle',
  'orange',
  'org',
  'organic',
  'origins',
  'osaka',
  'otsuka',
  'ott',
  'ovh',
  'pa',
  'page',
  'panasonic',
  'paris',
  'pars',
  'partners',
  'parts',
  'party',
  'pay',
  'pccw',
  'pe',
  'pet',
  'pf',
  'pfizer',
  'pg',
  'ph',
  'pharmacy',
  'phd',
  'philips',
  'phone',
  'photo',
  'photography',
  'photos',
  'physio',
  'pics',
  'pictet',
  'pictures',
  'pid',
  'pin',
  'ping',
  'pink',
  'pioneer',
  'pizza',
  'pk',
  'pl',
  'place',
  'play',
  'playstation',
  'plumbing',
  'plus',
  'pm',
  'pn',
  'pnc',
  'pohl',
  'poker',
  'politie',
  'porn',
  'post',
  'pr',
  'pramerica',
  'praxi',
  'press',
  'prime',
  'pro',
  'prod',
  'productions',
  'prof',
  'progressive',
  'promo',
  'properties',
  'property',
  'protection',
  'pru',
  'prudential',
  'ps',
  'pt',
  'pub',
  'pw',
  'pwc',
  'py',
  'qa',
  'qpon',
  'quebec',
  'quest',
  'racing',
  'radio',
  're',
  'read',
  'realestate',
  'realtor',
  'realty',
  'recipes',
  'red',
  'redstone',
  'redumbrella',
  'rehab',
  'reise',
  'reisen',
  'reit',
  'reliance',
  'ren',
  'rent',
  'rentals',
  'repair',
  'report',
  'republican',
  'rest',
  'restaurant',
  'review',
  'reviews',
  'rexroth',
  'rich',
  'richardli',
  'ricoh',
  'ril',
  'rio',
  'rip',
  'ro',
  'rocks',
  'rodeo',
  'rogers',
  'room',
  'rs',
  'rsvp',
  'ru',
  'rugby',
  'ruhr',
  'run',
  'rw',
  'rwe',
  'ryukyu',
  'sa',
  'saarland',
  'safe',
  'safety',
  'sakura',
  'sale',
  'salon',
  'samsclub',
  'samsung',
  'sandvik',
  'sandvikcoromant',
  'sanofi',
  'sap',
  'sarl',
  'sas',
  'save',
  'saxo',
  'sb',
  'sbi',
  'sbs',
  'sc',
  'scb',
  'schaeffler',
  'schmidt',
  'scholarships',
  'school',
  'schule',
  'schwarz',
  'science',
  'scot',
  'sd',
  'se',
  'search',
  'seat',
  'secure',
  'security',
  'seek',
  'select',
  'sener',
  'services',
  'seven',
  'sew',
  'sex',
  'sexy',
  'sfr',
  'sg',
  'sh',
  'shangrila',
  'sharp',
  'shell',
  'shia',
  'shiksha',
  'shoes',
  'shop',
  'shopping',
  'shouji',
  'show',
  'si',
  'silk',
  'sina',
  'singles',
  'site',
  'sj',
  'sk',
  'ski',
  'skin',
  'sky',
  'skype',
  'sl',
  'sling',
  'sm',
  'smart',
  'smile',
  'sn',
  'sncf',
  'so',
  'soccer',
  'social',
  'softbank',
  'software',
  'sohu',
  'solar',
  'solutions',
  'song',
  'sony',
  'soy',
  'spa',
  'space',
  'sport',
  'spot',
  'sr',
  'srl',
  'ss',
  'st',
  'stada',
  'staples',
  'star',
  'statebank',
  'statefarm',
  'stc',
  'stcgroup',
  'stockholm',
  'storage',
  'store',
  'stream',
  'studio',
  'study',
  'style',
  'su',
  'sucks',
  'supplies',
  'supply',
  'support',
  'surf',
  'surgery',
  'suzuki',
  'sv',
  'swatch',
  'swiss',
  'sx',
  'sy',
  'sydney',
  'systems',
  'sz',
  'tab',
  'taipei',
  'talk',
  'taobao',
  'target',
  'tatamotors',
  'tatar',
  'tattoo',
  'tax',
  'taxi',
  'tc',
  'tci',
  'td',
  'tdk',
  'team',
  'tech',
  'technology',
  'tel',
  'temasek',
  'tennis',
  'teva',
  'tf',
  'tg',
  'th',
  'thd',
  'theater',
  'theatre',
  'tiaa',
  'tickets',
  'tienda',
  'tips',
  'tires',
  'tirol',
  'tj',
  'tjmaxx',
  'tjx',
  'tk',
  'tkmaxx',
  'tl',
  'tm',
  'tmall',
  'tn',
  'to',
  'today',
  'tokyo',
  'tools',
  'top',
  'toray',
  'toshiba',
  'total',
  'tours',
  'town',
  'toyota',
  'toys',
  'tr',
  'trade',
  'trading',
  'training',
  'travel',
  'travelers',
  'travelersinsurance',
  'trust',
  'trv',
  'tt',
  'tube',
  'tui',
  'tunes',
  'tushu',
  'tv',
  'tvs',
  'tw',
  'tz',
  'ua',
  'ubank',
  'ubs',
  'ug',
  'uk',
  'unicom',
  'university',
  'uno',
  'uol',
  'ups',
  'us',
  'uy',
  'uz',
  'va',
  'vacations',
  'vana',
  'vanguard',
  'vc',
  've',
  'vegas',
  'ventures',
  'verisign',
  'vermgensberater',
  'vermgensberatung',
  'versicherung',
  'vet',
  'vg',
  'vi',
  'viajes',
  'video',
  'vig',
  'viking',
  'villas',
  'vin',
  'vip',
  'virgin',
  'visa',
  'vision',
  'viva',
  'vivo',
  'vlaanderen',
  'vn',
  'vodka',
  'volvo',
  'vote',
  'voting',
  'voto',
  'voyage',
  'vu',
  'wales',
  'walmart',
  'walter',
  'wang',
  'wanggou',
  'watch',
  'watches',
  'weather',
  'weatherchannel',
  'webcam',
  'weber',
  'website',
  'wed',
  'wedding',
  'weibo',
  'weir',
  'wf',
  'whoswho',
  'wien',
  'wiki',
  'williamhill',
  'win',
  'windows',
  'wine',
  'winners',
  'wme',
  'wolterskluwer',
  'woodside',
  'work',
  'works',
  'world',
  'wow',
  'ws',
  'wtc',
  'wtf',
  'xbox',
  'xerox',
  'xihuan',
  'xin',
  'xxx',
  'xyz',
  'yachts',
  'yahoo',
  'yamaxun',
  'yandex',
  'ye',
  'yodobashi',
  'yoga',
  'yokohama',
  'you',
  'youtube',
  'yt',
  'yun',
  'za',
  'zappos',
  'zara',
  'zero',
  'zip',
  'zm',
  'zone',
  'zuerich',
  'zw',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
  '',
]

================
File: src/jobs/utils/url-classifier.ts
================
// urlClassifier.ts

import * as tf from '@tensorflow/tfjs'
import '@tensorflow/tfjs-backend-cpu'
import fs from 'fs'
import * as path from 'path'
import { fileURLToPath } from 'url'
import { FeaturePipeline } from './features/feature.pipeline'
import { ensureDirectoryExists } from '@helpers'
import { CustomLogger } from '@core'

tf.setBackend('cpu')

interface TrainingSample {
  features: number[]
  label: number
  url: string
}

export class UrlClassifier {
  private log = new CustomLogger()
  private model!: tf.LayersModel
  private featurePipeline: FeaturePipeline
  private categories: string[]
  private modelVersion = 'v1'
  private modelPath = path.join(__dirname, './data/models/urlClassifier', this.modelVersion)
  private isModelReady = false

  constructor() {
    this.categories = ['news', 'jobs', 'unknown']
    this.featurePipeline = new FeaturePipeline(path.join(__dirname, './data/featuresConfig.json'))
    ensureDirectoryExists(this.modelPath)
    this.log.setDomain('url_classifier')
    void this.initializeModel()
  }

  private async initializeModel(): Promise<void> {
    try {
      await this.loadModel()
      this.log.info('Model loaded successfully.')
      this.isModelReady = true
    } catch (error: any) {
      this.log.warn('Model not found or input shape mismatch, attempting to create new model.')
      try {
        const data = await this.loadTrainingDataWithFallback()
        this.model = this.createModel()

        if (data.length > 0) {
          await this.trainModel(data)
          await this.saveModel()
          this.isModelReady = true
          this.log.info('New model created and trained successfully.')
        } else {
          this.log.warn('No training data available. Model will use default classifications.')
          // Still create model but don't train it
          this.model = this.createModel()
          this.isModelReady = true
        }
      } catch (trainError: any) {
        this.log.error('Failed to create/train new model:', trainError)
        // Create untrained model for default classifications
        this.model = this.createModel()
        this.isModelReady = true
      }
    }
  }

  private async loadTrainingDataWithFallback(): Promise<TrainingSample[]> {
    try {
      return this.loadTrainingData()
    } catch (error: any) {
      this.log.warn('No training data found, using default empty dataset')
      return []
    }
  }

  private getDefaultCategory(url: string): string {
    // Simple heuristic-based classification when no model is available
    const pathname = new URL(url).pathname.toLowerCase()

    if (
      pathname.includes('news') ||
      pathname.includes('article') ||
      pathname.includes('blog') ||
      pathname.includes('post')
    ) {
      return 'news'
    }

    if (pathname.includes('jobs') || pathname.includes('career') || pathname.includes('position')) {
      return 'jobs'
    }

    return 'unknown'
  }

  public async predict(url: string): Promise<string> {
    try {
      if (!this.isModelReady || !this.model) {
        this.log.warn('Model not ready, using default classification')
        return this.getDefaultCategory(url)
      }

      this.log.info(`Predicting category for URL: ${url}`)
      const features = this.extractFeatures(url)

      if (!features || features.some((f) => f === undefined)) {
        this.log.warn('Invalid features extracted, using default classification')
        return this.getDefaultCategory(url)
      }

      const inputTensor = tf.tensor2d([features])
      const prediction = this.model.predict(inputTensor) as tf.Tensor
      const predictionArray = prediction.dataSync()

      const maxProbability = Math.max(...predictionArray)
      const predictedIndex = predictionArray.indexOf(maxProbability)

      const confidenceThreshold = 0.5
      if (maxProbability < confidenceThreshold) {
        this.log.debug(
          `Low confidence prediction (${maxProbability}), using default classification`,
        )
        return this.getDefaultCategory(url)
      }

      const predictedCategory = this.categories[predictedIndex]
      this.log.debug(`Predicted ${predictedCategory} with confidence ${maxProbability}`)
      return predictedCategory
    } catch (error: any) {
      this.log.error(`Error in classifier prediction:`, error)
      return this.getDefaultCategory(url)
    }
  }

  private createModel(): tf.LayersModel {
    const numberOfFeatures = 10 // Adjust based on your features
    const model = tf.sequential()
    model.add(
      tf.layers.dense({
        inputShape: [numberOfFeatures],
        units: 64,
        activation: 'relu',
      }),
    )
    model.add(
      tf.layers.dense({
        units: this.categories.length,
        activation: 'softmax',
      }),
    )
    model.compile({
      optimizer: 'adam',
      loss: 'categoricalCrossentropy',
      metrics: ['accuracy'],
    })
    this.log.debug('Model created and compiled')
    return model
  }

  private oneHotEncode(labelIndex: number): number[] {
    const encoding = Array(this.categories.length).fill(0)
    if (labelIndex >= 0 && labelIndex < this.categories.length) {
      encoding[labelIndex] = 1
    }
    return encoding
  }

  public async trainModel(data: TrainingSample[]): Promise<void> {
    try {
      this.log.info(`Starting model training with ${data.length} samples`)
      const { trainingData, validationData } = this.splitData(data, 0.8)

      this.log.debug(`Training data: ${trainingData.length} samples`)
      this.log.debug(`Validation data: ${validationData.length} samples`)

      const xsTrain = tf.tensor2d(trainingData.map((sample) => sample.features))
      const ysTrain = tf.tensor2d(trainingData.map((sample) => this.oneHotEncode(sample.label)))

      const xsVal = tf.tensor2d(validationData.map((sample) => sample.features))
      const ysVal = tf.tensor2d(validationData.map((sample) => this.oneHotEncode(sample.label)))

      this.log.info('Starting model fitting...')
      const history = await this.model.fit(xsTrain, ysTrain, {
        epochs: 50,
        validationData: [xsVal, ysVal],
        verbose: 0,
        callbacks: {
          onEpochEnd: (epoch, logs) => {
            this.log.verbose(
              `Epoch ${epoch + 1}: loss = ${logs?.loss?.toFixed(
                4,
              )}, accuracy = ${logs?.acc?.toFixed(4)}`,
            )
          },
        },
      })

      this.log.info('Model fitting completed')

      // Evaluate the model
      const evaluation = this.evaluateModel(validationData)
      this.log.info('Model Evaluation:', evaluation)

      // Save training history
      await this.saveTrainingHistory(history)
    } catch (error: any) {
      this.log.error('Error during model training', error)
      throw error
    }
  }

  private async saveTrainingHistory(history: tf.History): Promise<void> {
    const historyPath = `${this.modelPath}/training_history.json`
    await fs.promises.writeFile(historyPath, JSON.stringify(history))
    this.log.debug('Training history saved to:', { historyPath })
  }

  private extractFeatures(url: string): number[] {
    if (!this.featurePipeline) {
      this.log.warn('Feature pipeline is not initialized')
      return Array(10).fill(0) // Return default features
    }
    try {
      const featureDict = this.featurePipeline.extractFeatures(url)
      const featureNames = this.featurePipeline.getFeatureNames()
      this.log.debug(`Extracted features for URL ${url}:`, featureDict)
      return featureNames.map((name) => featureDict[name] ?? 0)
    } catch (error: any) {
      this.log.error(`Error extracting features for URL ${url}`, error)
      return Array(10).fill(0) // Return default features
    }
  }

  private loadTrainingData(): TrainingSample[] {
    const dataPath = path.join(__dirname, './data/trainingData.json')
    this.log.debug(`[loadTrainingData] Attempting to load training data from: ${dataPath}`)

    let rawData: string
    try {
      rawData = fs.readFileSync(dataPath, 'utf-8')
      this.log.debug(`[loadTrainingData] Successfully read raw data from file`)
    } catch (error: any) {
      this.log.error(`[loadTrainingData] Error reading training data file`, error)
      throw new Error(`Failed to read training data file: ${error.message}`)
    }

    let jsonData: any
    try {
      jsonData = JSON.parse(rawData)
      this.log.debug(`[loadTrainingData] Successfully parsed JSON data`)
    } catch (error: any) {
      this.log.error(`[loadTrainingData] Error parsing JSON data`, error)
      throw new Error(`Failed to parse training data JSON: ${error.message}`)
    }

    if (!Array.isArray(jsonData) || jsonData.length === 0) {
      this.log.warn(`[loadTrainingData] Invalid training data: empty or not an array`)
      throw new Error('Training data is empty or not an array')
    }

    this.log.info(`[loadTrainingData] Loading ${jsonData.length} training samples`)

    const samples: TrainingSample[] = []
    const unknownLabels = new Set<string>()

    jsonData.forEach((sample: any, index: number) => {
      try {
        if (!sample.url || !sample.label) {
          this.log.warn(`[loadTrainingData] Sample at index ${index} is missing url or label`)
          return // Skip this sample
        }

        const labelIndex = this.categories.indexOf(sample.label)
        if (labelIndex === -1) {
          unknownLabels.add(sample.label)
          this.log.warn(
            `[loadTrainingData] Unknown label '${sample.label}' for URL: ${sample.url}. Using 'unknown' category.`,
          )
          samples.push({
            url: sample.url,
            features: this.extractFeatures(sample.url),
            label: this.categories.indexOf('unknown'),
          })
        } else {
          samples.push({
            url: sample.url,
            features: this.extractFeatures(sample.url),
            label: labelIndex,
          })
        }
      } catch (error: any) {
        this.log.error(`[loadTrainingData] Error processing sample at index ${index}`, error)
      }
    })

    if (unknownLabels.size > 0) {
      this.log.warn(
        `[loadTrainingData] Found ${unknownLabels.size} unknown labels:`,
        Array.from(unknownLabels),
      )
    }

    if (samples.length < this.categories.length * 2) {
      this.log.warn(
        `[loadTrainingData] Warning: Only ${samples.length} valid samples for ${this.categories.length} categories. Model may not train effectively.`,
      )
    }

    this.log.info(`[loadTrainingData] Successfully loaded ${samples.length} training samples`)
    this.log.debug(
      `[loadTrainingData] Category distribution:`,
      this.getCategoryDistribution(samples),
    )

    return samples
  }

  private getCategoryDistribution(samples: TrainingSample[]): Record<string, number> {
    const distribution: Record<string, number> = {}
    this.categories.forEach((category) => (distribution[category] = 0))

    samples.forEach((sample) => {
      const category = this.categories[sample.label]
      distribution[category] = (distribution[category] || 0) + 1
    })

    return distribution
  }

  public async retrain(): Promise<void> {
    try {
      this.log.info('Starting model retraining...')
      const data = this.loadTrainingData()
      await this.trainModel(data)
      await this.saveModel()
      this.log.info('Model retrained successfully.')
    } catch (error: any) {
      this.log.error('Error during model retraining:', error)
    }
  }

  private watchTrainingDataFile(): void {
    const dataPath = './data/trainingData.json'
    fs.watch(dataPath, (eventType, filename) => {
      if (eventType === 'change') {
        this.log.info(
          `Training data file ${filename} has changed. Reloading data and retraining model.`,
        )
        this.retrain()
      }
    })
  }

  public async updateModel(newSamples: TrainingSample[]): Promise<void> {
    const data = this.loadTrainingData().concat(newSamples)
    await this.trainModel(data)
  }

  private splitData(
    data: TrainingSample[],
    trainRatio: number,
  ): { trainingData: TrainingSample[]; validationData: TrainingSample[] } {
    const shuffledData = data.sort(() => 0.5 - Math.random())
    const trainSize = Math.floor(shuffledData.length * trainRatio)
    const trainingData = shuffledData.slice(0, trainSize)
    const validationData = shuffledData.slice(trainSize)
    return { trainingData, validationData }
  }

  private evaluateModel(validationData: TrainingSample[]): any {
    const xsVal = tf.tensor2d(validationData.map((sample) => sample.features))
    const ysTrue = validationData.map((sample) => sample.label)

    const predictions = this.model.predict(xsVal) as tf.Tensor2D
    const ysPred = predictions.argMax(1).dataSync()

    let correctPredictions = 0
    for (let i = 0; i < ysPred.length; i++) {
      if (ysPred[i] === ysTrue[i]) {
        correctPredictions++
      }
    }

    const accuracy = correctPredictions / ysPred.length

    // You can compute precision, recall, and F1-score for each category if needed.

    return {
      accuracy,
      total: ysPred.length,
      correctPredictions,
    }
  }

  private async saveModel(): Promise<void> {
    try {
      const modelDir = path.resolve(this.modelPath)
      if (!fs.existsSync(modelDir)) {
        fs.mkdirSync(modelDir, { recursive: true })
      }

      const modelPath = path.join(modelDir, 'model.json')
      const weightsPath = path.join(modelDir, 'weights.bin')

      const modelJSON = this.model.toJSON()
      fs.writeFileSync(modelPath, JSON.stringify(modelJSON))

      const weightData = await this.model.getWeights()[0].data()
      const weightDataBuffer = Buffer.from(weightData.buffer)
      fs.writeFileSync(weightsPath, new Uint8Array(weightDataBuffer))

      this.log.info('Model saved successfully:', { modelPath })
    } catch (error: any) {
      this.log.error('Error saving model:', error)
      throw error
    }
  }

  private async loadModel(): Promise<void> {
    try {
      const modelPath = path.join(this.modelPath, 'model.json')
      const weightsPath = path.join(this.modelPath, 'weights.bin')

      if (!fs.existsSync(modelPath) || !fs.existsSync(weightsPath)) {
        throw new Error('Model files not found')
      }

      const modelJSON = JSON.parse(fs.readFileSync(modelPath, 'utf8'))
      const weightData = fs.readFileSync(weightsPath)

      const buffer =
        weightData instanceof SharedArrayBuffer
          ? new Uint8Array(weightData).buffer
          : weightData instanceof ArrayBuffer
            ? weightData
            : new ArrayBuffer(0)

      const modelArtifacts: tf.io.ModelArtifacts = {
        modelTopology: modelJSON,
        weightData: buffer,
      }

      this.model = await tf.loadLayersModel(tf.io.fromMemory(modelArtifacts))
      this.log.info('Model loaded successfully from:', { modelPath: this.modelPath })
      this.log.info('Model summary:', { summary: this.model.summary() })
    } catch (error: any) {
      this.log.error('Error loading model:', error)
      throw error
    }
  }
}

================
File: src/jobs/utils/url-formatter.ts
================
import { URL } from 'url'
import tlds from './tlds'
import { URLValidator } from './url-validator'

export interface FormattedURLs {
  validUrls: string[]
  contacts: string[]
}

export class URLFormatter {
  private static readonly CONTACT_LINKS = [
    'mailto:',
    'tel:',
    'sms:',
    'whatsapp:',
    'viber:',
    'skype:',
    'facetime:',
    'zoom:',
    'slack:',
    'teams:',
    'meet:',
    'hangouts:',
    'webex:',
  ]

  private static readonly TLD_SET = new Set(tlds)

  private static readonly ACCEPTABLE_TERMS = ['pdf', 'doc', 'jpg', 'png']

  private static readonly ROUTE_PATTERNS: RegExp[] = [
    /^#\//, // Common pattern for SPA routes, e.g., "#/users"
    /^#![\/]/, // Hashbang pattern, e.g., "#!/users"
  ]

  private static readonly validator = new URLValidator()

  public static formatURLs(baseUrl: string, urls: string | string[]): FormattedURLs {
    const result: FormattedURLs = { validUrls: [], contacts: [] }

    if (typeof urls === 'string') {
      urls = [urls]
    }

    urls.forEach((url) => {
      const formattedUrl = this.formatSingleURL(baseUrl, url)
      if (formattedUrl) {
        if (formattedUrl.type === 'contact') {
          result.contacts.push(formattedUrl.url)
        } else {
          result.validUrls.push(formattedUrl.url)
        }
      }
    })

    return result
  }

  public static formatSingleURL(
    baseUrl: string,
    url: string,
  ): { url: string; type: 'contact' | 'valid' | 'invalid' } | null {
    if (this.isContactLink(url)) {
      return { url, type: 'contact' }
    }

    url = this.decodeURL(url)

    if (this.isRelativeURL(url)) {
      url = this.prependBaseURL(baseUrl, url)
    }

    if (!URLFormatter.validator.isValid(url)) {
      return null
    }

    url = this.normalizeURL(url)

    // const englishUrl = this.changeUrlLanguageToEnglish(url);
    return { url, type: 'valid' }
  }

  private static isContactLink(url: string): boolean {
    return this.CONTACT_LINKS.some((prefix) => url.toLowerCase().startsWith(prefix))
  }

  private static isValidTLD(tld: string): boolean {
    return this.TLD_SET.has(tld.toLowerCase())
  }

  static getBaseUrl(url: string): string {
    const parsedUrl = new URL(url)
    return `${parsedUrl.protocol}//${parsedUrl.host}`
  }

  private static prependBaseURL(baseUrl: string, link: string): string {
    const normalizedBaseURL = this.removeTrailingSlash(baseUrl.toLowerCase())
    baseUrl = this.getBaseUrl(normalizedBaseURL)

    if (this.isFullDomain(link)) {
      return `https://${link}`
    } else if (link.startsWith('/')) {
      return `${baseUrl}${link}`
    } else if (link.startsWith('//')) {
      return `https:${link}`
    } else if (link.startsWith('../')) {
      const newLink = link.replaceAll('../', '')
      return `${normalizedBaseURL}/${newLink}`
    } else if (link.startsWith('http')) {
      return link
    }
    return `${normalizedBaseURL}/${link}`
  }

  private static removeTrailingSlash(url: string): string {
    url = url.endsWith('/.') ? url.slice(0, -2) : url
    return url.endsWith('/') ? url.slice(0, -1) : url
  }

  private static changeUrlLanguageToEnglish(baseUrl: string): string | null {
    const languageRegex = /jap/
    const match = languageRegex.exec(baseUrl)
    if (match) {
      const languageCode = match[1]
      if (this.ACCEPTABLE_TERMS.includes(languageCode)) {
        return baseUrl
      }

      if (languageCode.length === 2) {
        return baseUrl.replace(`/${languageCode}/`, '/en/')
      } else if (languageCode.length === 3) {
        return baseUrl.replace(`/${languageCode}/`, '/eng/')
      }
    }
    return null
  }

  private static isRouteFragment(hash: string): boolean {
    // Remove the leading '#' if present
    const cleanHash = hash.startsWith('#') ? hash.slice(1) : hash

    // Check against patterns
    return this.ROUTE_PATTERNS.some((pattern) => pattern.test(hash))
  }

  private static removeQueryParameters(url: string): string {
    try {
      const urlObj = new URL(url)
      const preservedParams = [] as string[]
      urlObj.searchParams.forEach((value, key) => {
        if (value.includes('/')) {
          preservedParams.push(`${key}=${value}`)
        }
      })
      const preservedQuery = preservedParams.length > 0 ? `?${preservedParams.join('&')}` : ''
      return `${urlObj.origin}${urlObj.pathname}${preservedQuery}`
    } catch (error: any) {
      console.error('Invalid URL:', error)
      return url
    }
  }

  private static decodeURL(url: string): string {
    try {
      return decodeURIComponent(url)
    } catch {
      return url
    }
  }
  private static normalizeURL(url: string): string {
    try {
      const parsedUrl = new URL(url)

      // Normalize protocol
      parsedUrl.protocol = parsedUrl.protocol.toLowerCase()

      // Normalize hostname
      parsedUrl.hostname = parsedUrl.hostname.toLowerCase()

      // Remove default ports
      if (
        (parsedUrl.protocol === 'http:' && parsedUrl.port === '80') ||
        (parsedUrl.protocol === 'https:' && parsedUrl.port === '443')
      ) {
        parsedUrl.port = ''
      }

      // Only remove the fragment if it's not a route
      if (!this.isRouteFragment(parsedUrl.hash)) {
        parsedUrl.hash = ''
      }

      // Normalize path
      parsedUrl.pathname = this.normalizePath(parsedUrl.pathname)

      // Sort query parameters
      parsedUrl.search = ''

      // Remove fragment unless it's part of a routing strategy
      if (!this.isRouteFragment(parsedUrl.hash)) {
        parsedUrl.hash = ''
      }

      return parsedUrl.toString()
    } catch (error: any) {
      console.error('Error normalizing URL:', error)
      return url // Return original URL if normalization fails
    }
  }

  private static normalizePath(path: string): string {
    // Remove duplicate slashes
    path = path.replace(/\/+/g, '/')

    // Remove trailing slash unless it's the root path
    return path === '/' ? path : path.replace(/\/$/, '')
  }

  private static sortQueryParameters(search: string): string {
    if (!search) return ''
    const searchParams = new URLSearchParams(search)
    const sortedParams = Array.from(searchParams.entries()).sort(([a], [b]) => a.localeCompare(b))
    return new URLSearchParams(sortedParams).toString()
  }

  private static isRelativeURL(url: string): boolean {
    return (
      !url.startsWith('http://') &&
      !url.startsWith('https://') &&
      !url.startsWith('//') &&
      !this.isFullDomain(url)
    )
  }

  private static isFullDomain(url: string): boolean {
    const parts = url.split('.')
    if (parts.length < 2) return false

    const potentialTLD = parts[parts.length - 1]

    // Check if the last part matches our TLD regex
    if (!this.isValidTLD(potentialTLD)) return false

    // Ensure it's not just a file extension
    const secondLastPart = parts[parts.length - 2]
    return (
      secondLastPart.length > 1 && !this.ACCEPTABLE_TERMS.includes(secondLastPart.toLowerCase())
    )
  }

  public static isSameOrigin(url1: string, url2: string): boolean {
    try {
      const parsedUrl1 = new URL(url1)
      const parsedUrl2 = new URL(url2)
      return parsedUrl1.origin === parsedUrl2.origin
    } catch {
      return false
    }
  }
}

================
File: src/jobs/utils/url-validator.ts
================
import { URL } from 'url'

export class URLValidator {
  private VALID_PROTOCOLS = ['http:', 'https:']
  private MAX_URL_LENGTH = 2083 // Maximum length for most browsers and servers
  private INVALID_CHARACTERS = /[<>{}|\\^`]/ // Characters generally not allowed in URLs
  private INVALID_LINK_PATTERNS = ['javascript:', 'data:', '#', 'void(0)', 'about:blank']

  private MIN_DOMAIN_PARTS = 2

  public isValid(url: string): boolean {
    if (!url || url.length > this.MAX_URL_LENGTH) {
      return false
    }

    if (this.INVALID_LINK_PATTERNS.some((pattern) => url.includes(pattern))) {
      return false
    }

    try {
      const parsedUrl = new URL(url)

      // Check protocol
      if (!this.VALID_PROTOCOLS.includes(parsedUrl.protocol)) {
        return false
      }

      // Check for invalid characters
      if (this.INVALID_CHARACTERS.test(url)) {
        return false
      }

      // Check hostname
      if (!parsedUrl.hostname || parsedUrl.hostname.length < 1) {
        return false
      }

      if (!this.hasMinimumDomainParts(parsedUrl.hostname)) {
        return false
      }

      // Additional checks can be added here

      return true
    } catch (error: any) {
      return false
    }
  }

  private hasMinimumDomainParts(hostname: string): boolean {
    const parts = hostname.split('.')
    return parts.length >= this.MIN_DOMAIN_PARTS
  }

  public removeInvalidURLs(urls: string[]): string[] {
    return urls.filter((url) => this.isValid(url))
  }

  public normalizeURL(url: string): string {
    try {
      const parsedUrl = new URL(url)

      // Remove default ports
      if (
        (parsedUrl.protocol === 'http:' && parsedUrl.port === '80') ||
        (parsedUrl.protocol === 'https:' && parsedUrl.port === '443')
      ) {
        parsedUrl.port = ''
      }

      // Remove trailing slash if it's the only path
      if (parsedUrl.pathname === '/') {
        parsedUrl.pathname = ''
      }

      // Convert hostname to lowercase
      parsedUrl.hostname = parsedUrl.hostname.toLowerCase()

      // Sort query parameters
      parsedUrl.searchParams.sort()

      return parsedUrl.toString()
    } catch (error: any) {
      return url // Return original URL if normalization fails
    }
  }

  public isSameOrigin(url1: string, url2: string): boolean {
    try {
      const parsedUrl1 = new URL(url1)
      const parsedUrl2 = new URL(url2)
      return parsedUrl1.origin === parsedUrl2.origin
    } catch (error: any) {
      return false
    }
  }

  public getUrlWithoutFragment(url: string): string {
    try {
      const parsedUrl = new URL(url)
      parsedUrl.hash = ''
      return parsedUrl.toString()
    } catch (error: any) {
      return url
    }
  }

  public isRelativeUrl(url: string): boolean {
    return !url.startsWith('http://') && !url.startsWith('https://') && !url.startsWith('//')
  }
}

================
File: src/jobs/job.events.ts
================
// src/jobs/job.events.ts
import { CustomLogger, EventService } from '@core'

export class JobEvents {
  constructor(
    private readonly eventService: EventService,
    private readonly logger: CustomLogger,
  ) {
    this.setupEventHandlers()
    this.logger.setDomain('job_events')
  }

  private setupEventHandlers() {
    this.eventService.on('job.started', (jobName: string, jobId: string) => {
      this.logger.info(`Job started`, { jobName, jobId })
    })

    this.eventService.on('job.completed', (jobName: string, jobId: string, result: any) => {
      this.logger.info(`Job completed`, { jobName, jobId, result })
    })

    this.eventService.on('job.failed', (jobName: string, jobId: string, error: Error) => {
      this.logger.error(`Job failed`, { error, context: { jobName, jobId } })
    })

    this.eventService.on('job.progress', (jobName: string, jobId: string, progress: number) => {
      this.logger.debug(`Job progress: ${progress}%`, { jobName, jobId, progress })
    })
  }
}

================
File: src/jobs/job.factory.ts
================
// utils/job-factory.ts
import { DomainsForService, Service } from '@ib/logger'
import { JobName, JobConfig, JobHandlers } from '../types'

// src/utils/job-factory.ts
export class JobFactory {
  static createJob<TInput, TProcessed, TOutput>(
    config: Partial<JobConfig<TInput, TProcessed, TOutput>> & {
      name: string
      domain: DomainsForService<Service.JOBS>
      version: string
      changes: string[]
      handlers: JobHandlers<TInput, TProcessed, TOutput>
    },
  ): JobConfig<TInput, TProcessed, TOutput> {
    return {
      // Default configuration
      priority: 'normal',
      batchSize: 50,
      processSize: 10,
      timeout: 30000,
      retryLimit: 3,
      schedule: {
        type: 'interval',
        customCron: '0 0 * * *',
        enabled: false,
      },
      circuitBreaker: {
        enabled: true,
        failureThreshold: 5,
        resetTimeout: 60000,
      },
      // Override with provided config
      ...config,
    }
  }
}

================
File: src/jobs/job.registry.ts
================
// src/jobs/job.registry.ts
import PgBoss from 'pg-boss'
import { JobServices, QueueJob, JobClass, JobConfig, JobModule } from '@types'
import { CustomLogger } from '../core/services/logger.service'
import { QueueService } from '../core/services/queue.service'
import { JobRunner } from './job.runner'
import { JobVersionService } from './job.versioning'
import { newsJobModules } from './config/news/news.module'

export class JobRegistry {
  private jobs = new Map<string, JobConfig<any, any, any>>()
  private versionService: JobVersionService
  private jobModules: JobModule[] = []

  constructor(private services: JobServices) {
    this.versionService = new JobVersionService(services.logger, services.prisma)
  }

  private registerJobModule(jobModule: JobModule) {
    if (this.jobModules.find((m) => m.name === jobModule.name)) {
      throw new Error(`Job module ${jobModule.name} already registered`)
    }
    this.jobModules.push(jobModule)
  }

  private registerModules(modules: JobModule[]) {
    for (const module of modules) {
      this.registerJobModule(module)
      this.services.logger.info(`Registered job module: ${module.name}`)
    }
  }

  private loadJobModules() {
    // Register core job modules
    this.registerModules(newsJobModules)

    // In the future, add other module groups here like:
    // this.registerModules(analyticsJobModules)
    // this.registerModules(maintenanceJobModules)
    // etc.
  }

  async initialize(
    options: { environment: 'development' | 'production' } = { environment: 'production' },
  ) {
    try {
      this.loadJobModules()

      this.services.logger.info('Loaded job modules', {
        modules: this.jobModules.map((m) => m.name),
      })

      if (!(await this.services.queue.isInstalled())) {
        this.services.logger.info('Installing queue schema...')
        await this.services.queue.start()
      }

      // Initialize all registered job modules
      for (const module of this.jobModules) {
        const jobConfig = module.createJob(this.services)
        await this.registerJob(jobConfig)
        this.services.logger.info(`Initialized job from module: ${module.name}`, {
          name: jobConfig.name,
          version: jobConfig.version,
          schedule: jobConfig.schedule,
        })
      }

      // Log the final state
      const registeredJobs = this.getJobs()
      this.services.logger.info('Job registry initialized successfully', {
        totalJobs: registeredJobs.length,
        jobs: registeredJobs.map((job) => ({
          name: job.name,
          version: job.version,
          schedule: job.schedule?.enabled ? job.schedule.customCron : 'disabled',
        })),
      })
    } catch (error: any) {
      this.services.logger.error('Failed to initialize job registry', { error })
      throw error
    }
  }

  async registerJob<TInput, TProcessed, TOutput>(config: JobConfig<TInput, TProcessed, TOutput>) {
    try {
      // Store job config
      const { logger } = this.services
      this.jobs.set(config.name, config)

      // Create version record
      await this.versionService.createVersion(config.name, config.version, config.changes, config)

      // Register job handler with queue
      await this.services.queue.processJob(config.name, async (job) => {
        try {
          // Execute job handlers in sequence
          logger.info(`Starting job: ${job.name}:${config.name}`)
          const input = (await config.handlers.beforeProcess?.()) || []
          logger.info(`Processing job: ${job.name}:${config.name}`)
          const processed = await config.handlers.processFunction(input, job)
          logger.info(`Storing job data: ${job.name}:${config.name}`)
          const output = (await config.handlers.afterProcess?.(processed)) || processed
          logger.info(`Job completed: ${job.name}:${config.name}`)

          return output
        } catch (error: any) {
          await config.handlers.onError?.(error as Error)
          throw error
        }
      })

      // Set up job schedule if configured
      if (config.schedule?.enabled) {
        await this.services.queue.scheduleJob(
          config.name,
          config.schedule.customCron,
          {},
          {
            timeout: config.timeout,
            retryLimit: config.retryLimit,
            priority: config.priority,
          },
        )
      }

      this.services.logger.info(`Registered job: ${config.name} v${config.version}`)
    } catch (error: any) {
      this.services.logger.error(`Failed to register job: ${config.name}`, { error })
      throw error
    }
  }

  getJob(name: string): JobConfig<any, any, any> | undefined {
    return this.jobs.get(name)
  }

  getJobs(): JobConfig<any, any, any>[] {
    return Array.from(this.jobs.values())
  }

  async testJob(name: string) {
    const job = this.getJob(name)

    if (!job) {
      throw new Error(`Job ${name} not found`)
    }

    try {
      const jobId = await this.services.queue.testJob(name)
      this.services.logger.info(`Test job created: ${name}`, { jobId })
      return jobId
    } catch (error: any) {
      this.services.logger.error(`Failed to test job: ${name}`, { error })
      throw error
    }
  }
}

================
File: src/jobs/job.runner.ts
================
import type { JobServices, JobConfig, QueueJob } from '@types'

// src/jobs/job.runner.ts
export class JobRunner {
  constructor(private services: JobServices) {}

  async registerJob<TInput, TProcessed, TOutput>(config: JobConfig<TInput, TProcessed, TOutput>) {
    const { logger, queue } = this.services

    try {
      // Register job handler
      await queue.processJob(config.name, this.createJobHandler(config) as any)

      // Set up schedule if configured
      if (config.schedule?.enabled) {
        await this.scheduleJob(config)
      }

      logger.info(`Job registered: ${config.name}`)
    } catch (error: any) {
      logger.error(`Failed to register job: ${config.name}`, { error })
      throw error
    }
  }

  private createJobHandler<TInput, TProcessed, TOutput>(
    config: JobConfig<TInput, TProcessed, TOutput>,
  ) {
    return async (job: QueueJob) => {
      const startTime = Date.now()

      try {
        const input = (await config.handlers.beforeProcess?.()) || []
        const processed = await config.handlers.processFunction(input, job)
        const output = (await config.handlers.afterProcess?.(processed)) || processed

        await config.handlers.onSuccess?.(output as TOutput)
        return output
      } catch (error) {
        await config.handlers.onError?.(error as Error)
        throw error
      }
    }
  }

  private async scheduleJob(config: JobConfig<any, any, any>) {
    await this.services.queue.scheduleJob(config.name, config.schedule?.customCron ?? '', {
      timeout: config.timeout,
      retryLimit: config.retryLimit,
      priority: config.priority,
    })
  }
}

================
File: src/jobs/job.versioning.ts
================
// src/jobs/versioning/job-version.service.ts
import { JsonValue } from '@prisma/client/runtime/library'
import { CustomLogger, PrismaService } from '@core'
import { JobConfig } from '@types'

export interface JobVersion {
  version: string
  changes: string[]
  config: JsonValue
  created_at: Date
}

export class JobVersionService {
  constructor(
    private readonly logger: CustomLogger,
    private readonly prisma: PrismaService,
  ) {
    this.logger.setDomain('job_versions')
  }

  private createSerializableConfig(config: JobConfig<any, any, any>) {
    return {
      name: config.name,
      version: config.version,
      domain: config.domain,
      changes: config.changes,
      priority: config.priority,
      batchSize: config.batchSize,
      processSize: config.processSize,
      timeout: config.timeout,
      retryLimit: config.retryLimit,
      schedule: {
        type: config.schedule?.type,
        customCron: config.schedule?.customCron,
        enabled: config.schedule?.enabled,
      },
      circuitBreaker: {
        enabled: config.circuitBreaker?.enabled,
        failureThreshold: config.circuitBreaker?.failureThreshold,
        resetTimeout: config.circuitBreaker?.resetTimeout,
      },
      handlerNames: {
        hasBeforeProcess: !!config.handlers.beforeProcess,
        hasProcessFunction: !!config.handlers.processFunction,
        hasAfterProcess: !!config.handlers.afterProcess,
        hasOnError: !!config.handlers.onError,
      },
    }
  }

  async createVersion(
    jobName: string,
    version: string,
    changes: string[],
    config: JobConfig<any, any, any>,
  ) {
    try {
      this.logger.debug('Creating/updating job version', {
        jobName,
        version,
        changes,
      })

      const serializableConfig = this.createSerializableConfig(config)

      // Use upsert to handle both creation and updates
      await this.prisma.job_versions.upsert({
        where: {
          job_name_version: {
            job_name: jobName,
            version: version,
          },
        },
        create: {
          job_name: jobName,
          version,
          changes,
          config: serializableConfig,
          created_at: new Date().toISOString(),
        },
        update: {
          changes,
          config: serializableConfig,
          updated_at: new Date().toISOString(),
        },
      })

      this.logger.info(`Created/updated version ${version} for job ${jobName}`)
    } catch (error: any) {
      // Check if it's a schema issue
      if (error.code === 'P2009') {
        this.logger.error(
          'Schema validation error - check if job_versions table has correct structure',
          error,
        )
      }
      this.logger.error(`Failed to create/update version for job ${jobName}`, error)
      throw error
    }
  }

  async getJobVersions(jobName: string): Promise<JobVersion[]> {
    try {
      return await this.prisma.job_versions.findMany({
        where: { job_name: jobName },
        orderBy: { created_at: 'desc' },
      })
    } catch (error: any) {
      this.logger.error(`Failed to get versions for job ${jobName}`, error)
      throw error
    }
  }

  async getLatestVersion(jobName: string): Promise<JobVersion | null> {
    try {
      return await this.prisma.job_versions.findFirst({
        where: { job_name: jobName },
        orderBy: { created_at: 'desc' },
      })
    } catch (error: any) {
      this.logger.error(`Failed to get latest version for job ${jobName}`, error)
      throw error
    }
  }

  async rollbackVersion(jobName: string, targetVersion: string) {
    try {
      const version = await this.prisma.job_versions.findFirst({
        where: { job_name: jobName, version: targetVersion },
      })

      if (!version) {
        throw new Error(`Version ${targetVersion} not found for job ${jobName}`)
      }

      const config = version.config as any
      await this.prisma.job_configs.update({
        where: { name: jobName },
        data: {
          priority: config.priority,
          metadata: config.metadata,
          retry_limit: config.retryLimit,
          schedule: config.schedule,
          circuit_breaker_threshold: config.circuitBreaker?.failureThreshold,
          circuit_breaker_timeout_ms: config.circuitBreaker?.resetTimeout,
          updated_at: new Date().toISOString(),
        },
      })

      this.logger.info(`Rolled back ${jobName} to version ${targetVersion}`)
    } catch (error: any) {
      this.logger.error(`Failed to rollback version for job ${jobName}`, error)
      throw error
    }
  }
}

================
File: src/types/breaker.types.ts
================
export type CircuitState = 'closed' | 'open' | 'half-open'

export interface CircuitStatus {
  state: CircuitState
  failures: number
  lastFailure: Date
}

export interface CircuitBreakerEvents {
  'circuit-breaker.state-changed': {
    jobName: string
    previousState: CircuitState
    newState: CircuitState
    failures: number
  }
  'circuit-breaker.failure': {
    jobName: string
    error: Error
    failures: number
    threshold: number
  }
  'circuit-breaker.success': {
    jobName: string
    previousState: CircuitState
  }
  'circuit-breaker.timeout': {
    jobName: string
    timeoutMs: number
  }
}

================
File: src/types/domain.types.ts
================
// Original interfaces remain the same
export enum Priority {
  Critical = 100,
  High = 75,
  Normal = 50,
  Low = 25,
}

export type PriorityLevel = keyof typeof Priority

export interface DomainConfig {
  requiresAuth: boolean
  defaultPermissions: string[]
  supportsSoftDelete?: boolean
  supportsVersioning?: boolean
  requiresCompany?: boolean
  requiresUser?: boolean
  requiresEncryption?: boolean
  supportsCaching?: boolean
  requiresAdmin?: boolean
  sensitiveFields?: string[]
  exclude?: boolean
}

export interface CrossDomainConfig {
  allowedRelations: string[]
  implicitRelations?: {
    user?: boolean
    company?: boolean
  }
}

export interface LoggerConfig {
  domainName: string
  subContexts?: string[]
}

================
File: src/types/index.ts
================
export * from './workflow.types'
export * from './metric.types'
export * from './schedule.types'
export * from './domain.types'
export * from './job.types'
export * from './queue.types'
export * from './news.types'
export * from './module.types'
export * from './breaker.types'

================
File: src/types/job.types.ts
================
// src/types/job.types.ts
import PgBoss from 'pg-boss'
import { DomainsForService, Service } from '@ib/logger'
import type {
  PrismaService,
  EventService,
  ScraperService,
  QueueService,
  CustomLogger,
  MetricsService,
} from '@core'
import type { JobVersionService } from '../jobs/job.versioning'
import type { ScheduleConfig } from './schedule.types'
import type { QueueJob } from './queue.types'

// Required services for jobs
export interface JobServices {
  queue: QueueService
  scraper: ScraperService
  logger: CustomLogger
  event: EventService
  metrics: MetricsService
  prisma: PrismaService
  version: JobVersionService
}

export type JobClass = new (services: JobServices) => PgBoss.Job

// Job status types
export type JobStatus = 'pending' | 'running' | 'completed' | 'failed' | 'retrying'

export type JobName =
  | 'news_links'
  | 'news_pages'
  | 'news_summary'
  | 'company_pages'
  | 'company_spider'
  | 'update_scrape_frequency'
  | 'daily_newsletter'

export interface JobMetadata {
  name: JobName
  domain: DomainsForService<Service.JOBS>
  description: string
  priority: 'low' | 'normal' | 'high' | 'critical'
  schedule: ScheduleConfig
  timeout?: number
  retryLimit?: number
  tags?: string[]
}

export interface BatchConfig {
  batchSize: number
  processSize: number
  maxPages?: number
  maxItems?: number
}

export interface CircuitBreakerConfig {
  enabled: boolean
  failureThreshold?: number
  resetTimeout?: number
  halfOpenRetries?: number
}

export interface JobSchedule {
  customCron: string
  type: 'cron' | 'interval'
  enabled: boolean
}

export interface JobHandlers<TInput, TProcessed, TOutput> {
  beforeProcess?: () => Promise<TInput[]>
  processFunction: (rows: TInput[], job: QueueJob) => Promise<TProcessed[]>
  afterProcess?: (processedData: TProcessed[]) => Promise<TOutput[]>
  onSuccess?: (result: TOutput) => Promise<void>
  onError?: (error: Error) => Promise<void>
}

export interface JobConfig<TInput, TProcessed, TOutput> {
  name: string
  domain: DomainsForService<Service.JOBS>
  version: string
  changes: string[]
  handlers: JobHandlers<TInput, TProcessed, TOutput>
  schedule?: {
    customCron: string
    type: 'cron' | 'interval'
    enabled: boolean
  }
  priority?: 'low' | 'normal' | 'high' | 'critical'
  batchSize?: number
  processSize?: number
  timeout?: number
  retryLimit?: number
  circuitBreaker?: {
    enabled: boolean
    failureThreshold: number
    resetTimeout: number
  }
  tags?: string[]
}

================
File: src/types/logger.types.ts
================
// src/infrastructure/logger/logger.types.ts
import { Service, DomainsForService } from '@ib/logger'

export type LogLevel = 'error' | 'warn' | 'info' | 'debug' | 'verbose' | 'silly'

export interface LogContext {
  jobId?: string
  jobName?: string
  domain?: DomainsForService<Service.JOBS>
  [key: string]: any
}

================
File: src/types/metric.types.ts
================
import { error_type, job_status } from '@prisma/client'

// types/metrics.types.ts
export interface JobMetrics {
  duration: number
  success: boolean
  error?: Error
  itemsProcessed?: number
  memoryUsage?: {
    heapUsed: number
    heapTotal: number
    external: number
  }
  performance?: {
    itemsPerSecond: number
    avgProcessingTime: number
    batchSize: number
  }
}

export interface JobExecutionMetrics {
  jobName: string
  jobId: string
  status: job_status
  startTime: Date
  duration?: number
  itemsProcessed?: number
  performance?: {
    itemsPerSecond: number
    avgProcessingTime: number
    peakMemoryUsage: number
  }
  error?: {
    message: string
    stack?: string
    code?: string
    type: error_type
  }
  metadata?: Record<string, any>
}

export interface CircuitBreakerMetrics {
  jobName: string
  state: 'closed' | 'open' | 'half-open'
  failures: number
  lastFailure: Date
  lastSuccess?: Date
  recoveryAttempts: number
  timeInCurrentState: number
}

export interface QueueMetrics {
  waiting: number
  active: number
  completed: number
  failed: number
  delayed: number
}

================
File: src/types/module.types.ts
================
import { JobConfig, JobServices } from './job.types'

export interface JobModule<TInput = any, TProcessed = any, TOutput = any> {
  name: string
  createJob: (services: JobServices) => JobConfig<TInput, TProcessed, TOutput>
}

================
File: src/types/news.types.ts
================
export interface ContentSource {
  id: bigint
  company_id: string
  url: string
  rss_urls: string[]
  hash: bigint
  has_failed: boolean
  failed_count: number
  refreshed_at: Date
  expected_count?: number
}

================
File: src/types/queue.types.ts
================
// src/types/queue.types.ts
export interface QueueOptions {
  priority?: number
  retryLimit?: number
  timeout?: number
  startAfter?: string | Date
  singletonKey?: string
  expireInSeconds?: number
}

export interface QueueJob<T = any> {
  id: string
  name: string
  data: T
  priority: number
  startedAt?: Date
  completedAt?: Date
  createdAt: Date
  state: 'created' | 'active' | 'completed' | 'failed' | 'expired'
}

export interface WorkflowJobData {
  progress: number
  jobs: string[]
  [key: string]: any
}

export interface WorkflowJob extends QueueJob<WorkflowJobData> {}

================
File: src/types/schedule.types.ts
================
// types/schedule.types.ts
export type TimeUnit = 'minute' | 'hour' | 'day' | 'week' | 'month'
export type DayOfWeek =
  | 'Sunday'
  | 'Monday'
  | 'Tuesday'
  | 'Wednesday'
  | 'Thursday'
  | 'Friday'
  | 'Saturday'

export interface ScheduleInterval {
  value: number
  unit: TimeUnit
}

export interface ScheduleConfig {
  type: 'interval' | 'daily' | 'weekly' | 'monthly' | 'custom'
  interval?: ScheduleInterval
  time?: string // HH:mm format
  dayOfWeek?: DayOfWeek
  dayOfMonth?: number | 'first' | 'last'
  customCron?: string
  enabled: boolean
}

================
File: src/types/workflow.types.ts
================
import { BatchConfig } from './job.types'

export interface WorkflowOptions {
  name: string
  jobs: BatchConfig[]
  concurrency?: number
  onComplete?: (result: any) => Promise<void>
  onFail?: (error: Error) => Promise<void>
}

export interface WorkflowStatus {
  total: number
  completed: number
  failed: number
  pending: number
  active: number
}

================
File: src/app.ts
================
// src/app.ts
import {
  QueueService,
  CustomLogger,
  MetricsService,
  ScraperService,
  EventService,
  PrismaService,
} from '@core'
import { ShutdownService } from './core/shutdown.service'
import { JobVersionService } from './jobs/job.versioning'
import { JobRegistry } from './jobs/job.registry'
import { ApplicationConfig } from './config'
import type { JobServices } from '@types'

export class Application {
  private readonly services: JobServices
  private readonly jobRegistry: JobRegistry
  private readonly shutdownService: ShutdownService

  constructor(config: ApplicationConfig) {
    // Initialize base services first
    const logger = new CustomLogger()
    logger.init()
    logger.setDomain('jobs')

    const prisma = new PrismaService(logger)
    const event = new EventService(logger)
    const metricsService = new MetricsService(logger, prisma, event)

    // Initialize queue service if needed
    const queue = new QueueService(config.database.directUrl, logger, metricsService)

    // Create services object
    this.services = {
      logger,
      prisma,
      event,
      queue,
      metrics: metricsService,
      scraper: new ScraperService(logger),
      version: new JobVersionService(logger, prisma),
    }
    // Initialize shutdown service last
    this.jobRegistry = new JobRegistry(this.services)

    this.shutdownService = new ShutdownService(this.services)
  }

  async start(jobName?: string) {
    try {
      this.services.logger.info('Starting application initialization')

      // Initialize database connection first
      this.services.logger.info('Connecting to database')
      await this.services.prisma.connect()

      // Initialize and start queue service
      this.services.logger.info('Initializing queue service')
      await this.services.queue.init()

      // Get current stats
      const stats = await this.services.queue.getQueueStats()
      this.services.logger.info('Current queue stats', { stats })
      await this.services.queue.start()

      await this.jobRegistry.initialize()

      const history = await this.services.metrics.getQueueStatsHistory('news_links', 24)
      this.services.logger.info('Queue stats history', { history })
      
      if (jobName) {
        await this.jobRegistry.testJob(jobName)
      }
    } catch (error: any) {
      this.services.logger.error('Failed to start application', error)
      throw error
    }
  }

  async stop() {
    try {
      this.services.logger.info('Stopping application...', {
        timestamp: new Date().toISOString(),
      })

      if (this.services.queue) {
        await this.services.queue.stop()
      }
      await this.services.prisma.disconnect()

      this.services.logger.info('Application stopped successfully', {
        timestamp: new Date().toISOString(),
      })
    } catch (error: any) {
      this.services.logger.error('Error stopping application', {
        ...error,
        timestamp: new Date().toISOString(),
      })
      throw error
    }
  }
}

================
File: src/config.ts
================
// src/config/config.ts
import { z } from 'zod'
import dotenv from 'dotenv'
import path from 'path'

// Load environment variables
dotenv.config({
  path: path.resolve(process.cwd(), `.env.${process.env.NODE_ENV || 'development'}`),
})

// Configuration schema
const configSchema = z.object({
  supabase: z.object({
    url: z.string(),
    serviceKey: z.string(),
  }),
  environment: z.enum(['development', 'staging', 'production']).default('development'),
  app: z.object({
    name: z.string().default('cron-jobs'),
    port: z.number().default(3000),
    environment: z.enum(['development', 'test', 'production']).default('development'),
    logLevel: z.enum(['error', 'warn', 'info', 'debug']).default('info'),
  }),
  database: z.object({
    url: z.string(),
    directUrl: z.string(),
    maxConnections: z.number().default(20),
    idleTimeout: z.number().default(60000),
  }),
  queue: z.object({
    retryLimit: z.number().default(3),
    retryDelay: z.number().default(60000),
    monitorInterval: z.number().default(30000),
  }),
  jobs: z.object({
    maxConcurrent: z.number().default(5),
    defaultTimeout: z.number().default(300000),
    healthCheckInterval: z.number().default(60000),
  }),
  metrics: z.object({
    enabled: z.boolean().default(true),
    interval: z.number().default(60000),
  }),
})

// Configuration interface
export type ApplicationConfig = z.infer<typeof configSchema>

// Load and validate configuration
function loadConfig(): ApplicationConfig {
  const config = {
    environment: process.env.NODE_ENV,
    supabase: {
      url: process.env.SUPABASE_URL,
      serviceKey: process.env.SUPABASE_SERVICE_KEY,
    },
    app: {
      name: process.env.APP_NAME,
      port: parseInt(process.env.APP_PORT || '3050', 10),
      environment: process.env.NODE_ENV,
      logLevel: process.env.LOG_LEVEL || 'info',
    },
    database: {
      url: process.env.DATABASE_URL,
      directUrl: process.env.DATABASE_DIRECT_URL,
      maxConnections: parseInt(process.env.DB_MAX_CONNECTIONS || '20', 10),
      idleTimeout: parseInt(process.env.DB_IDLE_TIMEOUT || '60000', 10),
    },
    queue: {
      retryLimit: parseInt(process.env.QUEUE_RETRY_LIMIT || '3', 10),
      retryDelay: parseInt(process.env.QUEUE_RETRY_DELAY || '60000', 10),
      monitorInterval: parseInt(process.env.QUEUE_MONITOR_INTERVAL || '30000', 10),
    },
    jobs: {
      maxConcurrent: parseInt(process.env.JOBS_MAX_CONCURRENT || '5', 10),
      defaultTimeout: parseInt(process.env.JOBS_DEFAULT_TIMEOUT || '300000', 10),
      healthCheckInterval: parseInt(process.env.JOBS_HEALTH_CHECK_INTERVAL || '60000', 10),
    },
    metrics: {
      enabled: process.env.METRICS_ENABLED !== 'false',
      interval: parseInt(process.env.METRICS_INTERVAL || '60000', 10),
    },
  }

  return configSchema.parse(config)
}

export const config = loadConfig()

================
File: src/index.ts
================
// src/index.ts
import { Application } from './app'
import { config } from './config'

async function bootstrap() {

  const app = new Application(config)

  // Get test job name from command line arguments if provided
  const testJobArg = process.argv.find((arg) => arg.startsWith('--test-job='))
  const testJobName = testJobArg ? testJobArg.split('=')[1] : undefined

  await app.start(testJobName)

  // Handle shutdown gracefully
  process.on('SIGTERM', async () => {
    await app.stop()
    process.exit(0)
  })

  process.on('SIGINT', async () => {
    await app.stop()
    process.exit(0)
  })
}

bootstrap().catch((error: any) => {
  console.error('Application failed to start:', error)
  process.exit(1)
})

================
File: .dockerignore
================
node_modules
dist
coverage
.git
.env
*.log
.nx/cache
tmp
**/*.spec.ts
**/*.test.ts
**/__tests__
.next
.nuxt
.output
.turbo

================
File: docker-compose.yml
================
services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - '3000:3000'
    environment:
      - NODE_ENV=production
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=postgres
      - DB_USER=postgres
      - DB_PASSWORD=postgres
      - DB_SCHEMA=public
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/postgres
      - DIRECT_URL=postgresql://postgres:postgres@postgres:5432/postgres
      - SUPABASE_URL=http://supabase:54321
      - API_PREFIX=/api
      - SWAGGER_PATH=/api
      - CORS_ORIGINS=http://localhost:3000,http://localhost:3001
    volumes:
      - .:/usr/src/app
      - /usr/src/app/node_modules

================
File: Dockerfile
================
FROM node:22-slim
ENV PNPM_HOME="/pnpm"
ENV PATH="$PNPM_HOME:$PATH"
ENV NX_DAEMON=false
ENV NX_VERBOSE_LOGGING=true
RUN corepack enable

WORKDIR /app

# Enable pnpm and install system dependencies
RUN apt-get update && \
    apt-get install -y \
    python3 \
    make \
    g++ \
    git \
    libnss3 \
    libatk1.0-0 \
    libatk-bridge2.0-0 \
    libcups2 \
    libdrm2 \
    libxkbcommon0 \
    libxcomposite1 \
    libxdamage1 \
    libxfixes3 \
    libxrandr2 \
    libgbm1 \
    libasound2 \
    libpangocairo-1.0-0 \
    libpango-1.0-0 \
    libcairo2 \
    libatspi2.0-0 \
    libgtk-3-0 && \
    rm -rf /var/lib/apt/lists/*

# Copy package files
COPY pnpm-*.yaml package.json ./
COPY apps/cron-jobs/package.json ./apps/cron-jobs/
COPY libs/logger/package.json ./libs/logger/

# Install dependencies and Playwright
RUN --mount=type=cache,id=s/3c9904ae-05e1-4a46-a525-fca654d7d374-/pnpm/store,target=/pnpm/store \
    pnpm install --frozen-lockfile && \
    pnpm install -g playwright && \
    pnpm exec playwright install chromium && \
    pnpm exec playwright install-deps

# Copy source files and build
COPY . .
RUN pnpx prisma generate && \
    rm -rf layers/*/dist && \
    pnpm exec nx reset && \
    pnpm exec nx build @astronera/cron-jobs --configuration=production --skip-nx-cache --verbose

WORKDIR /app/apps/cron-jobs

CMD ["node", "dist/src/index.js"]

================
File: eslint.config.js
================
import baseConfig from '../../eslint.config'

export default {
  ...baseConfig,
  rules: {
    ...baseConfig.rules,
  },
}

================
File: package.json
================
{
  "name": "@astronera/cron-jobs",
  "version": "0.0.3",
  "private": true,
  "type": "module",
  "engines": {
    "node": ">=22.0.0"
  },
  "imports": {
    "#/*": "./dist/*"
  },
  "scripts": {
    "lint": "pnpm nx lint",
    "test": "pnpm pnpm nx test",
    "dev": "NODE_OPTIONS='--experimental-loader=ts-node/esm' pnpm nx serve",
    "build": "pnpm nx build",
    "start": "node dist/src/index.js"
  },
  "dependencies": {
    "@ib/logger": "workspace:*",
    "@prisma/client": "^6.0.1",
    "@supabase/supabase-js": "^2.47.8",
    "@tensorflow/tfjs": "^4.22.0",
    "axios": "^1.7.7",
    "cheerio": "^1.0.0",
    "chrono-node": "^2.7.7",
    "dotenv": "^16.4.5",
    "natural": "^8.0.1",
    "node-fetch": "latest",
    "p-limit": "latest",
    "pg-boss": "^10.1.5",
    "playwright": "1.41.1",
    "playwright-core": "1.41.1",
    "robots-parser": "^3.0.1",
    "rss-parser": "^3.13.0",
    "sharp": "^0.33.2",
    "turndown": "^7.2.0",
    "zod": "^3.23.8"
  },
  "devDependencies": {
    "@rollup/plugin-json": "^6.0.0",
    "@supabase/auth-js": "^2.67.3",
    "@supabase/functions-js": "^2.4.4",
    "@supabase/node-fetch": "^2.6.13",
    "@supabase/postgrest-js": "^1.17.9",
    "@supabase/realtime-js": "^2.11.3",
    "@supabase/storage-js": "^2.7.1",
    "@tensorflow/tfjs-backend-cpu": "^4.22.0",
    "@tensorflow/tfjs-backend-webgl": "^4.22.0",
    "@tensorflow/tfjs-converter": "^4.22.0",
    "@tensorflow/tfjs-core": "^4.22.0",
    "@tensorflow/tfjs-data": "^4.22.0",
    "@tensorflow/tfjs-layers": "^4.22.0",
    "@types/node": "^22.10.5",
    "@types/turndown": "^5.0.5",
    "long": "^5.2.3",
    "seedrandom": "^3.0.5",
    "ts-node": "^10.9.2",
    "tslib": "^2.8.1",
    "vite": "^6.0.6",
    "vite-node": "^2.1.8"
  }
}

================
File: project.json
================
{
  "name": "@astronera/cron-jobs",
  "$schema": "../../node_modules/nx/schemas/project-schema.json",
  "projectType": "application",
  "sourceRoot": "apps/cron-jobs",
  "// targets": "to see all targets run: nx show project @astronera/cron-jobs --web",

  "targets": {
    "build": {
      "executor": "nx:run-commands",
      "options": {
        "commands": ["rm -rf dist", "vite build"],
        "cwd": "apps/cron-jobs"
      },
      "dependsOn": [
        {
          "target": "build",
          "projects": ["@ib/logger"]
        }
      ]
    },
    "serve": {
      "executor": "nx:run-commands",
      "options": {
        "command": "NODE_OPTIONS='--no-warnings' pnpx vite-node --no-deps src/index.ts",
        "cwd": "apps/cron-jobs"
      }
    },
    "start": {
      "executor": "nx:run-commands",
      "options": {
        "command": "node dist/src/index.js",
        "cwd": "apps/cron-jobs"
      }
    },

    "debug": {
      "executor": "nx:run-commands",
      "options": {
        "command": "tsx --inspect-brk src/index.ts",
        "cwd": "apps/cron-jobs",
        "env": {
          "NODE_ENV": "development",
          "DEBUG": "true"
        }
      }
    },

    "lint": {
      "executor": "nx:run-commands",
      "options": {
        "command": "eslint 'src/**/*.{js,ts}'",
        "cwd": "apps/cron-jobs"
      }
    },

    "lint:fix": {
      "executor": "nx:run-commands",
      "options": {
        "command": "eslint 'src/**/*.{js,ts}' --fix",
        "cwd": "apps/cron-jobs"
      }
    },

    "typecheck": {
      "executor": "nx:run-commands",
      "options": {
        "command": "tsc --noEmit",
        "cwd": "apps/cron-jobs"
      }
    },

    "test": {
      "executor": "nx:run-commands",
      "options": {
        "command": "vitest run --coverage",
        "cwd": "apps/cron-jobs",
        "env": {
          "NODE_ENV": "test",
          "VITEST_MODE": "true"
        }
      }
    },

    "test:watch": {
      "executor": "nx:run-commands",
      "options": {
        "command": "vitest",
        "cwd": "apps/cron-jobs",
        "env": {
          "NODE_ENV": "test",
          "VITEST_MODE": "true"
        }
      }
    },

    "clean": {
      "executor": "nx:run-commands",
      "options": {
        "commands": ["rm -rf dist", "rm -rf coverage", "rm -rf .turbo"],
        "cwd": "apps/cron-jobs",
        "parallel": true
      }
    }
  }
}

================
File: tsconfig.json
================
{
  "compilerOptions": {
    "outDir": "./dist",
    "baseUrl": ".",
    "rootDir": "../../",

    // Module settings
    "module": "ESNext",
    "moduleResolution": "Bundler",
    "target": "ES2022",

    // Type checking
    "strict": true,
    "skipLibCheck": true,

    // Path aliases
    "paths": {
      "@/*": ["./src/*"],
      "@types": ["./src/types"],
      "@helpers": ["./src/helpers"],
      "@core": ["./src/core"],
      "@jobs": ["./src/jobs/config"],
      "@agents": ["./src/agents"]
    }
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}

================
File: vite.config.ts
================
import { defineConfig } from 'vite'
import path from 'path'
import json from '@rollup/plugin-json'

export default defineConfig({
  build: {
    target: 'node22',
    outDir: 'dist/src',
    sourcemap: true,
    minify: false,
    modulePreload: false,
    ssr: true,
    rollupOptions: {
      input: {
        index: path.resolve(__dirname, 'src/index.ts'),
      },
      output: {
        dir: 'dist/src',
        format: 'esm',
        preserveModules: true,
        preserveModulesRoot: 'src',
        entryFileNames: '[name].js',
        chunkFileNames: '[name].js',
        interop: 'auto',
      },
      external: [
        // Node.js built-in modules
        'fs',
        'fs/promises',
        'path',
        'url',
        'crypto',
        'http',
        'https',
        'stream',
        'zlib',
        'buffer',
        'util',
        'os',
        'child_process',
        'node-fetch',
        // TensorFlow dependencies
        'long',
        'seedrandom',
        '@tensorflow/tfjs',
        '@tensorflow/tfjs-core',
        '@tensorflow/tfjs-layers',
        '@tensorflow/tfjs-converter',
        '@tensorflow/tfjs-data',
        '@tensorflow/tfjs-backend-webgl',
        '@tensorflow/tfjs-backend-cpu',
        '@tensorflow/tfjs-core/dist/public/chained_ops/register_all_chained_ops',
        '@tensorflow/tfjs-core/dist/register_all_gradients',
        // External dependencies
        /^node:/,
        '@tensorflow/tfjs',
        '@tensorflow/tfjs-backend-cpu',
        'sharp',
        'playwright',
        'playwright-core',
        '@playwright/test',
        /^playwright-.*/,
        /^@playwright\/.*/,
        'pg-boss',
        '@prisma/client',
        '@supabase/supabase-js',
        '@supabase/auth-js',
        '@supabase/functions-js',
        '@supabase/realtime-js',
        '@supabase/storage-js',
        '@supabase/postgrest-js',
        'robots-parser',
        'rss-parser',
        'p-limit',
      ],
    },
  },
  plugins: [
    json({
      preferConst: true,
      compact: true,
      namedExports: true,
      include: ['node_modules/**/*.json', '**/*.json'],
    }),
  ],
  resolve: {
    alias: {
      '@': path.resolve(__dirname, './src'),
      '@types': path.resolve(__dirname, './src/types'),
      '@helpers': path.resolve(__dirname, './src/helpers'),
      '@core': path.resolve(__dirname, './src/core'),
      '@jobs': path.resolve(__dirname, './src/jobs/config'),
    },
    mainFields: ['module', 'jsnext:main', 'jsnext', 'main'],
    preserveSymlinks: true,
  },
  optimizeDeps: {
    exclude: [
      'long',
      'seedrandom',
      '@tensorflow/tfjs',
      '@tensorflow/tfjs-core',
      '@tensorflow/tfjs-layers',
      '@tensorflow/tfjs-converter',
      '@tensorflow/tfjs-data',
      '@tensorflow/tfjs-backend-webgl',
      '@tensorflow/tfjs-backend-cpu',
      '@supabase/supabase-js',
      '@supabase/auth-js',
      '@supabase/functions-js',
      '@supabase/realtime-js',
      '@supabase/storage-js',
      '@supabase/postgrest-js',
    ],
  },
})
